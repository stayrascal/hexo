---
title: Machine-Learning
date: 2018-11-03 11:35:33
tags:
---

再炫目的技术归根结底都是基本模型与方法再具体领域问题上的组合，而理解这些基本模型与方法才是掌握机器学习，也是掌握任何一门学问的要义所在。

### 机器学习概观
概率在机器学习中扮演者核心角色，而频率学派和贝叶斯学派对概率迥异的解读方式也将机器学习一份为二，发展出两套完全不同的理论体系。
机器学习领域的元老Tom.Mitchell这样定义机器学习：机器学习是一门研究通过计算的手段利用经验来改善系统自身性能的学科。现在大多数经验都以数据的形式出现，所以机器学习的任务就变成了基于已知数据构造概率模型，反过来再运用概率模型对未知的数据进行预测和分析。

#### 频率视角下的机器学习
- 频率率学派口中的概率表示的是事件发生频率的极限值，只有在无限次独立重复试验之下才有绝对的精确意义。
- 在频率学派眼中，当重复试验的次数趋近于无穷大时，事件发生的频率会收敛到真实的概率之上，这种观点背后暗含了一个前提，那就是概率是一个确定的值，并不会受单次观察结果的影响。
- 频率统计理论的核心在于认定待估计的参数是固定不变的常量，讨论参数的概率分布式没有意义的；而用来估计参数的数据是随机的变量，每个数据都是参数支配下一次独立重复试验的结果，由于参数本身是确定的，那频率的波动就并非来源于参数本身的不确定性，而是由有限次观察造成的干扰而导致的。
- 统计学的核心任务之一就是根据从总体中抽取出的样本，也就是数据来估计未知的参数。参数的最优估计可以通过样本数据的分布，也就是采样分布来求解。由于频率统计将数据看做随机变量，所以计算采样分布是没有问题的。确定采样分布之后，参数估计可以等效成一个最优化的问题，而频率统计最常用的最优化方法，就是最大似然估计，它的目的是让似然概率最大化，也就是在固定参数的前提之下，数据出现的条件概率最大化。这是评率学派估计参数的基本出发点：一组数据之所以能够在单次试验中出现，是因为它出现的可能性最大。而参数估计的过程就是赋予观测数据最大似然概率的过程。(就是对全部数据采样之后得到一部分数据，然后根据这部分数据估计出一个最优的参数，就是模型训练的过程)
- 频率主义解决统计问题的基本思路就是：参数是确定，数据是随机的，利用随机的数据推断确定的参数，得到的结果也是随机的。
- 将频率主义“参数确定，数据随机”的思路应用在机器学习当中，得到的就是统计机器学习。统计机器学习的做法是通过对给定的指标(比如似然函数或者均方误差)进行最优化，来估计模型中参数的取值，估计时并不考虑参数的不确定性，也就是不考虑未知参数的先验分布。和参数相关的信息全部来源于数据，输出的则是未知参数唯一的估计结果，这就是统计机器学习的核心特征。
- 但是由于噪声和干扰的影响，我们观测到的数据并不是未知参数的准确反映，因此如何衡量估计结果的精确程度就成为统计机器学习中的一个关键问题。损失函数直接定义了模型性能的度量方式，其数学期望被称为风险，风险最小化就是参数估计得依据和准则。估计参数时需要计算风险，计算风险时需要在数据的概率分布上对损失函数进行积分，可表示数据的分布又需要依赖未知参数的精确取值，这就带来了一个难题，风险函数是没办法精确求解的。为了解决这个问题，统计机器学习引入了经验风险，用训练数据的经验分布替换原始表达式中数据的真实分布，借此将风险函数转化成了可计算的数值，比如分类问题的误分类率，回归问题的均方误差。而我们所谓的最优模型也就是使经验风险最小化的那个模型。（就是因为我们是采样的数据，并不能代表整体数据，而且可能存在一些误差或者干扰，所以并不能确定每次采样数据所估计出来的参数哪个更优，因为我们不知道真实的参数值，所以采用第二次采样估计参数时，使用第一次参数估计值作为经验值代替参数真实值进行误差计算）

#### 贝叶斯视角下的机器学习
- 频率主义把概率定义为无限次独立重复试验下事件发生频率的极限值，对于一些只会发生一次或者很少次的事件(比如飞机事故，一锤子买卖等不包含随机变量的事件)，评率主义的观点就不适用了。为了解决这个问题，贝叶斯学派给出了一种更加通用的概率定义：概率表示的是客观上事件的可信程度，也可以说成是主观上主体对事件的信任程度，它是简历在对事件的已有知识基础上的。
- 除了对概率的置信度解释之外，贝叶斯派中的另一个核心内容是贝叶斯定理，用来解决逆向概率问题。假定数据由一个生成模型给出，前向概率是在已知生成过程的前提条件下来计算数据的概率分布和数字特征，逆向概率则是已知数据的前提下反过来计算生成过程的未知特性。
- 将贝叶斯定理应用到统计推断中，就是贝叶斯主义的统计学。评率统计理论的核心在于认定待估计的参数是固定不变的常量，而用来估计得数据是随机的变量。贝叶斯统计则恰恰相反：它将待估计得参数视为随机变量，用来估计得数据是确定的常数，讨论观测数据的概率分布才是没有意义的。贝叶斯统计的任务就是根据这些确定的观测数据反过来推断未知参数的概率分布。
- 相对于频率主义的最大似然估计，贝叶斯主义在参数估计中倾向于使用后验概率最大化，使用最大后验概率估计，最大后验概率推断即使结合参数自身的分布特性，找到最可能产生观测数据的那个参数的过程。
- 贝叶斯的定理表示，后验概率正比于先验概率和似然概率的乘积，这意味着后验概率实质上就是用先验概率对似然概率做了个加权处理
- 先验信息在贝叶斯统计中占据着相当重要的地位，先验信息是在使用数据之前关于分析对象的已有知识，可当这种已有知识并不存在时，就不能对先验做出合理的建模。事实上，指定先验分布的必要性正是贝叶斯学派被频率学派诟病之处，因为先验分布不可避免地会受到主观因素的影响，这与统计学立足客观的出发点背道而驰。其实即使包含某些主观判断，先验信息也是贝叶斯主义中不可或缺的核心要素
- 当已有的知识实在不足以形成先验信息时，贝叶斯主义的处理方式是引入无信息先验，认为未知参数取均匀分布，由于这时的先验参数是个常数，这个先验概率也被称为平坦先验(flat prior)。在平坦先验之下，最大后验概率和最大似然估计等效
- 将贝叶斯定理应用到机器学习之中，完成模型预测和选择的任务，就是贝叶斯视角下的机器学习。由于贝叶斯定理大量涉及各种显式变量与隐式变量的依赖关系，通常用概率图模型来直观地描述。贝叶斯主义将未知参数视为随机变量，参数在学习之前的不确定性由先验概率描述，这中间不确定性的消除就是机器学习的作用
- 与频率主义不同的是，贝叶斯学习的输出不是简单的最优估计值，而是关于参数的概率分布，从而给出了跟加完整的信息。在预测问题时，贝叶斯学习给出的也不仅仅是一个可能性最大的结果，而是将所有结果以及概率以概率分布的形式完整地呈现出来
- 贝叶斯方法有两个缺点：
    - 对未知变量的积分运算会导致极高的计算复杂度
    - 先验分布的设定包含一定的主观性，因而一直不招老派统计学家待见


### 机器学习的使用
- 机器学习能解决的问题必然包含某些显式或者隐式的模式，没有模式的问题就不能通过机器学习解决。完全随机的问题是不可能被求解，也不可能被学习的。（问题不能是完全随机的，需要具备一定的模式）
- 一个具有解析解的问题是完全不需要机器学习的。即使问题本身没有解析解，要是能够通过数值计算的方法解决，而不实际明显的优化过程的话，也无需机器学习的使用。（问题本身不能通过纯计算的方法解决）
- 用机器学习解决问题需要一个条件，就是大量的可用数据。（有大量的数据可用）


### 频率学派的统计学习-统计机器学习
统计机器学习的核心是数据，它即从数据中来，利用不用的模型去拟合数据背后的规律；也到数据中去，用拟合出的规律去推断和预测未知的结果。几乎所有的其他模型都是从不同角度对线性回归模型做出的拓展和修正
### 贝叶斯学派的符号学习-概率图模型
和基于数据的统计学习相比，基于关系的图模型更多的代表了因果推理的发展方向。贝叶斯主义也需要计算待学习对象的概率分布，但它利用的不是海量的具体数据，而是变量之间的相关关系、每个变量的先验分布和大量复杂的积分技巧。
