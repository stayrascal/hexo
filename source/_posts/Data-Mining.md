---
title: Data-Mining
date: 2018-02-16 10:42:32
tags:
---

# 数据分析的本质
> 一般情况下，我们所说的分析是指使用大量数据、统计和定量分析、解释和预测模型以及基于事实的管理来推动决策过程与实现价值驱动。

> 根据分析的方法和目的，分析可以被划分为：

- 描述性分析： 数据收集、整理、制表以及描述正要研究的事物的特征，这类分析以往被称为报告。描述性分析可能非常有用，但它不能解释某种结果出现的原因或者未来可能发生的事情。
- 预测性分析：不仅可以对数据特征和变量之间的关系进行描述，还可以基于过去的数据预测未来。预测性分析首先会确定变量之间的关联，然后基于这种已知关联预测另一种现象出现的可能性，比如在看到某个广告后，一位消费者可能会去买产品的可能性。虽然预测性分析中的预测是基于变量之间的关联作出的，但这并不代表预测性分析都需要明确因果关系。事实上，准确的预测并不一定需要因果关系。
- 规范性分析：规范性分析是更高层次的分析，如实验设计和优化等。就像医生会在处方上建议患者采取什么行动一样，实验设计试图通过做实验给出某些事物发生的原因。为了能够在因果关系研究中信心饱满地做出判断，研究人员必须妥善处理一个或多个独立的变量，并有效控制其他无关的变量。如果处于实验环境下的测试组的表现大大优于对照组，决策制定者就应该立即推广这种实验环境。优化是规范性分析采用的一种方法，指试图识别出一个特定变量与另一变量之间理想的关系水平。例如，我们可能会对识别最有可能让产品实现高收益的价格感兴趣。同样地，优化这种方法能够识别出使零售企业最大限度避免缺货情况的库存水平。

> 根据分析采用的方法以及收集和分析的数据类型，分析可被分为：

- 定性分析：定性分析的目的是深入了解某种现象出现的根本原因和诱因。非结构化数据通常是从少数非代表性案例中收集而来，并进行了非统计性的分析。定性分析是分析的最初阶段，它通常是探索性研究的有效工具。
- 定量分析：定量分析是指通过统计、数学或计算的方法对现象进行系统的实证研究。通常情况下，结构化数据是从大量典型案例中收集而来，并进行统计分析

> 为了服务于研究者的不同研究目的，存在以下几种类型的分析：

- 统计学：收集、整理、分析、说明和呈现数据的科学
- 预测：根据已有数据，预测一些感兴趣的变量在未来某个特定时间点的情况
- 数据挖掘：通过使用算法和统计技术，自动或半自动地提取大量数据中未知的有趣模式
- 文本挖掘：用类似于数据挖掘的方式从文本中得出模式和趋势的过程
- 优化：在同事满足约束条件的情况下，按照某些标准，利用数学方法来寻找最优的解决方案
- 实验设计：给各组随机分配被试，然后使用测试组和对照组来推到出特定结果中存在的因果关系。

> 定量分析遵循3个阶段和6个步骤：

- 构建问题
	* 识别问题
		- 问题是什么
		- 这个问题为什么重要
	* 回顾之前的发现
- 解决问题
	* 建模或选择变量
	* 收集数据
	* 分析数据
- 传达结果并基于结果采取行动
	* 传达结果并采取行动



# 数据挖掘的基本任务
> 数据挖掘的基本任务包括利用分类与预测、聚类分析、关联规则、时序模式、偏差检测、智能推荐等方法，帮助企业提取数据中蕴含的商业价值，提高竞争里

# 数据挖掘建模过程

### 1.定义挖掘目标
> 针对具体的数据挖掘应用需求，首先要明确挖掘的目标是什么？系统完成后能达到什么样的效果？要想充分发挥数据挖掘的价值，必须对目标有一个清晰明确的定义，即决定到底想要做什么

- 目标定义
	* 任务理解
	* 指标确定
- 数据采集
	* 建模抽样
	* 质量把控
	* 实时采集
- 数据整理
	* 数据探索
	* 数据清洗
	* 数据变换
- 构建模型
	* 模式发现
	* 构建模型
	* 验证模型
- 模型评价
	* 设定评价标准
	* 多模型对比
	* 模型优化
- 模型发布
	* 模型部署
	* 模型重构

### 2.数据采样
> 在明确了需要进行挖掘的目标后，接下来就需要从业务系统中抽取一个与挖掘目标相关的样本数据子集。抽取数据的标准，一是相关性，二是可靠性，三是有效性。
> 衡量取样数据质量的标准如下
> 
- 资料完整无缺，各类指标齐全
- 数据准确无误，反映的都是正常状态下的水平

### 3.数据探索
> 当拿到一个样本数据集后，它是否达到我们原来设想的要求，样本中有没有什么明显的规律和趋势，有没有出现从未设想过的数据状态，属性之间有什么相关性，它们可区分成怎样的一些类别等，这都是要探索的内容
> 
> 对所抽取的样本数据进行探索、审核和必要的加工处理，是保证最终的挖掘模型的质量所必须的。数据探索和预处理的目的是为了保证样本数据的质量，从而为保证模型质量打下基础。数据探索主要包括：异常值分析、缺失值分析、相关性分析、周期性分析

### 4.数据预处理
> 当采样数据维度过大时，如何进行降维处理、缺失值处理都是数据预处理要解决的问题。由于采样数据中常常包含许多含有噪声、不完整，甚至不一致的数据，对数据挖掘所涉及的数据对象必须进行预处理。数据预处理主要包括：数据刷选、数据变量转换、缺失值处理、坏数据处理、数据标准化、主成分分析、属性选择、数据规约等

### 5.挖掘建模
> 样本抽取完成并经预处理之后，接下来需要考虑本次建模属于数据挖掘应用中的那类问题，分类、聚类、关联规则、时序模型或者是智能推荐，选用哪种算法进行模型构建

### 6.模型评价
> 模型评价的目的之一就是从这些模型中自动找出一个最好的模型，另外就是要根据业务对模型进行解释和应用


# 3.数据探索
> 根据观测、调查收集到初步的样本数据集后，接下来要考虑的问题：

- 样本数据集的数量和质量是否满足模型构建的要求？
- 是否出现从未设想过的数据状态？
- 其中有没有什么明显的规律和趋势？
- 各因素之间有什么样的关联性？

> 数据探索：通过检验数据集的数据质量、绘制图表、计算某些特征量等手段，对样本数据集的结构和规律进行分析的过程。数据探索有助于选择合适的数据预处理和建模方法，甚至可以完成一些通常由数据挖掘解决的问题。

## 3.1 数据质量分析
> 数据质量分析是数据挖掘中数据准备过程的重要一环，是数据处理的前提，也是数据挖掘分析结论有效性和准确性的基础，没有可信的数据，数据挖掘构建的模型将是空中楼阁。数据质量分析的主要任务是检查原始数据中是否存在脏数据，脏数据一般是指不符合要求，以及不能直接进行相应分析的数据。在常见的数据挖掘工作中，脏数据包括：

- 缺失值
- 异常值
- 不一致的值
- 重复数据及含有特殊符号(#,¥,*)的数据

### 3.1.1 缺失值分析
> 数据的缺失主要包括记录的缺失和记录中某个字段信息的缺失，两者都会造成分析结果的不准确。

**缺失值产生的原因**

- 信息暂时无法获取，或者获取信息的代价太大
- 信息遗漏，可能是因为输入时认为不重要、忘记填写或对数据理解错误等一些人为因素而遗漏，也可能是由于数据采集设备的故障、存储介质的故障
传输媒体的故障等分人为因素而丢失
- 属性值不存在。在某些情况下，缺失值并不意味着数据由错误，对一些对象来说某些属性值是不存在的，如一个未婚者的配偶姓名，一个儿童的固定收入

**缺失值的影响**

- 数据挖掘建模将丢失大量的有用信息
- 数据挖掘模型所表现出的不确定性更加显著，模型中蕴涵的规律更难把握
- 包含空值的数据会使建模过程陷入混乱，导致不可靠的输出

**缺失值的分析**

- 使用简单的统计分析，可以得到含有缺失值的属性的个数，以及每个属性的未却失数、缺失数与缺失率等
- 从总体上来说，缺失值的处理分为删除在缺失值的记录，对可能值进行插补和不处理3中情况

### 3.1.2 异常值分析
> 异常值分析是检验数据是否有录入错误以及含有不合常理的数据。忽视异常值的存在
是十分危险的，不加剔除地把异常值包括进数据的计算分析过程中，对结果会产生不良影响，重视异常值的出现，分析其产生的原因，常常成为发现问题进而改进决策的契机。

> 异常值是指样本中的个别值，其数值明显偏离其余的观测值。异常值也称为离群点，异常值的分析也称为离群点分析。

**简单统计计量分析**
> 先对变量做一个描述性统计，进而查看哪些数据是不合理的。最常用的统计量是最大值和最小值，用来判断这个变量的取值是否超出了合理的范围

**3∂原则**
> 如果数据服从正态分布，在3∂原则下，异常值被定义为一组测定值中与平均值的偏差超过了3倍标准差的值，在正态分布的假设下，距离平均值3∂之外的概率为P(|x-µ| > 3∂) <= 0.003，属于极个别的小概率事件。如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述

**箱型图分析**
> 箱型图根据实际数据绘制，没有对数据作任何限制性要求，它只是真实直观地表现数据分布的本来面貌；箱型图判断异常值的标准以四分位数和四分位距为基础，四分位数有一定的鲁棒性，多达25%的数据可以变得任意远而不会很大地扰动四分位数，所以异常值不能对这个标准施加影响，所以箱型图识别异常值的结果比较客观，在识别异常方面有一定的优越性。

### 3.1.3 一致性分析
> 数据不一致是指数据的矛盾性、不相容性。直接对不一致的数据进行挖掘，可能会产生与实际相违背的挖掘结果。

> 在数据挖掘过程中，不一致数据的产生主要发生在数据集成的过程中，这可能是由于被挖掘数据来自不同的数据源、对于重复存放的数据未能进行一致性更新造成的


## 3.2 数据特征分析
> 对数据进行质量分析后，接下来可通过绘制图表、计算某些特征量等手段进行数据的特征分析。

### 3.2.1 分布分析
> 分布分析能揭示数据的分布特征和分布类型。对于定量数据，如果想了解其分布形式是对称的还是非对称的，发现某些特大或特小的可疑值，可通过绘制频率分布表、频率分布直方图、茎叶图进行直观地分析；对于定性分类数据，可以用饼图和条形图直观地显示分布情况。

- **定量数据的分布分析**: 
	* 对于定量变量而言，选择“组数”和“组宽”是做频率分布分析时最重要的问题，一般按照一下步骤进行：
	    - 求极差
	    - 决定组距与组数：习惯上将各组段设为左闭右开的半开闭区间
	    - 决定分点
	    - 列出频率分布表
	    - 绘制频率分布直方图
    * 遵循的主要原则如下：
	    - 各组之间必须时相互排斥的
	    - 各组必须将所有的数据包含在内
	    - 各组的组宽最好相等
- **定性数据的分布分析**
	* 对于定性变量，常常根据变量的分类类型来分组，可以采用饼图和条形图来描述定性变量的分布

### 3.2.2 对比分析
> 对比分析是指把两个相互联系的指标进行比较，从数量上展示和说明研究对象规模的大小，水平的高低，速度的快慢，以及各种关系是否协调。特别适用于指标间的横纵向比较、时间序列的比较分析。在对比分析中，选择合适的对比标准是十分关键的步骤，只有选择合适，才能做出客观的评价，选择不合适，评价可能得出错误的结论。

> 对比分析主要有以下两种形式:

- **绝对数比较**：绝对数比较是利用绝对数进行对比，从而寻找差异的一种方法
- **相对数比较**：相对数比较是由两个有联系的指标对比计算的，用以反映客观现象之间数量联系程度的综合指标，其数值表现为相对数。由于研究目的和对比基础不同，相对数可以分为以下几种：
	* **结构相对数**：将同一总体内的部分数值与全部数值对比求得比重，用以说明事物的性质、结构或质量。如居民食品支出额占消费支出总额比重、产品合格率等。 
	* **比例相对数**：将同一总体内不同部分的数值进行对比，表明总体内各部分的比例关系。如人口性别比例、投资与消费者比例等
	* **比较相对数**：将同一时期两个性质相同的指标数值进行对比，说明同类现象在不同空间下的数量对比关系。如不同地区商品价格对比，不同行业、不同企业间某项指标对比等
	* **强度相对数**：将两个性质不同但有一定联系的总量指标进行对比，用以说明现象的强度、密度和普遍程度。如人均国内生产总值用“元/人”表示，人口密度用“人/平方公里”表示，也有用百分数或千分数表示的，如人口出生率用千分数表示
	* **计划完成度相对数**：是某一时期实际完成数与计划数的对比，用以说明计划完成程度
	* **动态相对数**：将同一现象在不同时期的指标数值进行对比，用以说明发展方向和变化的速度。如发展速度、增长速度等

### 3.2.3 统计量分析
> 用统计指标对定量数据进行统计描述，常从集中趋势和离中趋势两个方面进行分析

> 平均水平的指标是对个体集中趋势的度量，使用最广泛的是均值和中位数；反映变异程度的指标是对个体离开平均水平的度量，使用较广泛的是标准差、四分位间距

- **集中趋势度量**
	* **均值**： 均值的主要问题是对极端值很敏感。如果数据中存在极端或者数据是偏态分布的，那么均值就不能很好地度量数据的集中趋势。为了消除少数极端值的影响，可以使用截断均值或者中位数来度量数据的集中趋势。截断均值是去掉高、低端值之后的平均数。
	* **中位数**：中位数是将一组观察值按从小到大的顺序排序，位于中间的那个数。即在全部数据中，小于和大于中位数的数据个数相等。
	* **众数**：是指数据集中出现最频繁的值。众数并不经常用来度量定性变量的中心位置，更适用于定性变量。众数不具有唯一性。当然，众数一般用于离散型变量而非连续性变量。
- **离中趋势度量**
	* **极差**：极差对数据集的极端值非常敏感，并且忽略了位于最大值与最小值之间的数据的分布情况
	* **标准差**：标准差度量数据数据偏离均值的程度
	* **变异系数**： 变异系数度量标准差相对于均值的离中趋势，变异系数主要用来比较两个或多个具有不同单位或不同波动幅度的数据集的集中趋势
	* **四分位数间距**：上四分位数与下四分位数之差，其间包含了全部观察值的一半。其值越大，说明数据的变异程度越大，反之，说明离异程度越小

### 3.2.4 周期性分析
> 周期性分析是探索某个变量是否随着时间变化而呈现出某种周期变化趋势。时间尺度相对较长的周期性趋势有年度周期性趋势、季节性周期趋势，相对较短的有月度周期性趋势、周度周期性趋势，甚至更短的天、小时周期性趋势

### 3.2.5 贡献度分析
> 贡献度分析又称帕累托分析，原理是帕累托法则，又称20/80定律。同样的投入放在不同的地方会产生不同的效益

### 3.2.6 相关性分析
> 分析连续变量之间线性相关程度的强弱，并用适当的统计指标表示出来的过程称为相关分析

- **直接绘制离散点**：判断两个变量是否具有线性相关关系的最直观的方法
- **绘制散点图矩阵**：同时考察多个变量的相关关系时，一一绘制散点图比较麻烦，可以利用散点矩阵同时绘制各个变量间的关系图，从而快速的发现多个变量间的主要相关性，这在进行多元线性回归时尤其重要。
- **计算相关系数**：
	* **Pearson相关系数**： 一般用于分析两个连续性变量之间的关系，要求连续变量的取值服从正态分布
	* **Spearman秩相关系数**：不服从正态分布的变量、分类或等级变量之间的关联性可采用Spearman秩相关系数，也称等级相关系数来描述。只要两个变量具有严格单调的函数关系，那么它们就是完全Spearman相关的，这与Perason相关不同，Pearson相关只有在变量具有线性关系时才是完全相关的。实际应用计算中，上述两种相关系数都要对其进行假设检验，使用t检验方法检验其显著性水平以确定其相关程度。研究表明，在正态分布假定下，Spearman秩相关系数与Pearson相关系数在效率上时等价的，而对于连续测量数据，更适合用Pearson相关系数来进行分析。
	* **判定系数**：判定系数是相关系数的平方，用r^{2}表示，用来衡量回归方程对y的解释程度。判定系数取值范围[0,1],r^{2}越接近于1，表明x与y之间的相关性越强，r^{2}越接近于0，表明两个变量之间几乎没有直线相关关系

> 数据质量分析要求我们拿到数据后先检测是否存在缺失值和异常值
> 
> 数据特征分析要求我们在数据挖掘建模前，通过频率分布分析、对比分析、帕累托分析、周期性分析、相关性分析等方法，对采集的样本数据的特征规律进行分析，以了解数据的规律和趋势，为数据挖掘的后续环节提供支持


# 4.数据预处理
> 在数据挖掘中，海量的原始数据中存在着大量不完整(有缺失值)、不一致、有异常的数据，严重影响到数据挖掘建模的执行效率，甚至可能导致挖掘结果的偏差，所以进行数据清洗就显得尤为重要，数据清洗完成后接着进行或者同时进行数据集成、转换、规约等一系列处理，该过程就是数据预处理。
> 
> 数据预处理一方面是要提高数据的质量，另一方面是要让数据更好地适应特定的挖掘技术或工具。统计发现，在数据挖掘的过程中，数据预处理工作量占到了整个过程的60%。
> 
> 数据预处理的主要内容包括数据清洗、数据集成、数据变换和数据规约

## 4.1 数据清洗
> 数据清洗主要是删除原始数据集中的无关数据、重复数据，平滑噪声数据，筛选掉与挖掘主题无关的数据，处理缺失值、异常值等。

### 4.1.1 缺失值处理
> 处理缺失值方法可分3类：删除记录、数据插补、不处理

- **数据插补方法**
	* 均值/中位数/众数插补：根据属性值的类型，用该属性取值的平均数/中位数/众数进行插补
	* 使用固定值：将缺失的属性值用一个常量替换
	* 最近临插补：在记录中找到与缺失样本最接近的样本的该属性值插补
	* 回归方法：对带有缺失值的变量，根据已有数据和与其有关的其它变量(因变量)的数据建立拟合模型来预测缺失的属性值
	* 插值法：插值法是利用已知点建立合适的插值函数f(x)，未知值由对应点x求出的函数值f(x)近似代替
- **删除记录**：
	* 如果通过简单的删除小部分记录达到既定的目标，那么删除含有缺失值的记录的方法是最有效的
	* 这种方法的局限性在于它是以减少历史数据来换取数据的完备，会造成资源的大量浪费，将丢弃大量隐藏在这些记录中的信息。尤其在数据集本来就包含很少记录的情况下，删除少量记录可能严重影响到分析结果的客观性和正确性。一些模型可以将缺失值视作一种特殊的取值，允许直接在含有缺失值的数据上进行建模

- **插值方法**：
	* 拉格朗日插值法：根据数学知识可知，对于平面上已知的n个点可以找到一个n-1次多项式y=a_0 + a_1x + a_2x^{2}+...+a_n-1x^{n-1}，使此多项式曲线过这n个点
	* 牛顿插值法

### 4.1.2 异常值处理
> 处理异常值的方法:
> 
- **删除含有异常值的记录**：直接将含有异常值的记录删除
	* 缺点：在观测值很少的情况下，或造成样本量不足，可能会改变变量的原有分布，造成分析结果的不准确。
	* 优点：可以利用现有变量的信息，对异常值(缺失值)进行填补
- **视为缺失值**：将异常值视为缺失值，利用缺失值处理的方法进行处理
- **平均值修成**：可用前后两个观测值的平均值修正该异常值
- **不处理**：直接在具有异常值的数据集上进行挖掘建模

> 很多情况下，要先分析异常值出现的可能原因，再判断异常值是否应该舍弃，如果是正确的数据，可以直接在具有异常值的数据集上进行挖掘建模

## 4.2 数据集成
> 数据挖掘需要的数据往往分布在不同的数据源中，数据集成就是将多个数据源合并存放在一个一致的数据存储中的过程。
> 在数据集成时，来自多个数据源的现实世界实体的表达形式时不一样的，有可能不匹配，要考虑实体识别问题和属性冗余问题，从而将数据源数据在最底层上加以转换、提炼和集成。

### 4.2.1 实体识别
> 实体识别是指从不同数据源识别出现实世界的实体，它的任务是统一不同数据源的矛盾之处，常见的形式有：
> 
- **同名同义**：数据源A中的属性ID和数据源B中的属性ID分别描述的是不同编号，即描述的是不同的实体
- **异名同义**：数据源A的dt和数据源的date都是描述日期的，即A.dt = B.date
- **单位不统一**：描述同一个实体分别用的国际单位和中国传统的计量单位

> 检测和解决这些冲突就是实体识别的任务

### 4.2.2 冗余属性识别
> 数据集成往往导致数据冗余，如：
> 
- 同一属性多次出现
- 同一属性命名不一致导致重复

> 仔细整合不同源数据能减少甚至避免数据冗余与不一致，从而提高数据挖掘的速度和质量，对于冗余属性要先分析，检测到后再将其删除。有些冗余属性可以用相关分析检测

## 4.3 数据变换
> 数据变换主要是对数据进行规范化处理，将数据转换为“适当的”形式，以适应于挖掘任务及算法的需要。

### 4.3.1 简单函数变换
> 对原始数据进行某些数学函数变换，常用的变换包括平方、开发、取对数、差分运算等。简单的函数变换常用来将不具有正态分布的数据变换成具有正态分布的数据。有时间序列分析中，有时简单的对数变换或者差分运算就可以将非平稳序列转换成平稳序列。在数据挖掘中，简单的函数变换可能更有必要，比如个人年收入的取值范围为10000元到10亿元，这是一个很大的区间，使用对数变换对其进行压缩是常用的一种变换处理方法.

### 4.3.2 规范化
> 数据规范化处理是数据挖掘的一项基础工作。不同评价指标往往具有不同的量纲，数值间的差别可能很大，不进行处理可能会影响到数据分析的结果。为了消除指标之间的量纲和取值范围差异的影响，需要进行标准化处理，将数据按照比例进行缩放，是之落入一个特定的区域，便于进行综合分析。数据规范化对于基于距离的挖掘算法尤为重要。
> 
- **最小-最大规范化**：也称离差标准化，是对原始数据的线性变换，将数值映射到[0,1]之间。
	* 离差标准化保留了原来数据中存在的关系，是消除量纲和数据取值范围影响的最简单方法。
	* 缺点：若数值集中且某个数值很大，则规范化后各值会接近于0，并且将会相差不大，若将来遇到超过目前属性[min,max]取值范围的时候，会引起系统出错，需要重新确定min和max
- **零-均值规范化**：也称标准差标准化，经过处理的数据均值为0，标准差为1
- **小数定标规范化**：通过移动属性值的小数位数，将属性值映射到[-1,1]之间，移动的小数位数取决属性值绝对值的最大值：x^{*} = x / 10^{k}

### 4.3.3 连续属性离散化
> 一些数据挖掘算法，特别是某些分类算法如ID3算法，Apriori算法等，要求数据是分类属性形式。这样，常常需要将连续属性变换成分类属性，即连续性离散化
> 
- **离散化的过程**：连续属性的离散化就是在数据的取值范围设定若干个离散的划分点，将取值范围划分为一些离散化的区间，最后用不同的符号或整数值代表落在每个子区间中的数据值。所以，离散化涉及两个子任务：确定分类数以及如何将连续属性值映射到这些分类值。
- **常用的离散化方法**：
	* **等宽法**：将属性的值域分成具有相同宽度的区间，区间的个数由数据本身的特点决定，或者由用户制定，类似于制作频率分布表
	* **等频法**：将相同数量的记录放进每个区间
	* 这两种方法简单，易于操作，但都需要人为地规定划分区间的个数。同时等宽法的缺点在于它对离群点比较敏感，倾向于不均匀地把属性值分布到各个区间。有些区间包含许多数据，而另一些区间的数据极少，这样严重损坏建立的决策模型。等频法虽然避免了上述问题的产生，却可能将相同的数据值分到不同的区间以满足每个区间中固定的数据个数
	* **基于聚类分析的方法**：一维聚类的方法包括两个步骤，首先将连续属性的值用聚类算法如K-Means算法进行聚类，然后再将聚类得到的簇进行处理，合并到一个簇的连续属性值并做同一标记。聚类分析的离散化方法也需要用户指定簇的个数，从而决定产生的区间数。

### 4.3.4 属性构造
> 在数据挖掘的过程中，为了提取更有用的信息，挖掘更深层次的模式，提高挖掘结果的精度，我们需要利用已有的属性集构造出新的属性，并加入到现有的属性集合中。

### 4.3.5 小波变换
> 小波变换是一种新型的数据分析工具，是近年来兴起的信号分析手段。小波分析的理论和方法在信号处理、图像处理、语音处理、模式识别、量子物理等领域得到越来越广泛的应用，被认为是近年来在工具及方法上的重大突破。小波变换具有多分辨率的特点，在时域和频域都具有表征信号局部特征的能力，通过伸缩和平移等运算过程对信号进行多尺度聚焦分析，提供了一种非平稳信号的时频分析手段，可以由粗及细地逐步观察信号，从中提取有用信息。

> 能够刻画某个问题的特征量往往是隐含在一个信号中的某个或者某些分量中，小波变换可以把非平稳信号分解为表达不同层次、不同频带信息的数据序列，即小波系数。选取适当的小波系数，即完成了信号的特征提取。小波变换的信号特征提取方法有：
> 
- **基于小波变换的多尺度空间能量分布特征提取**：各尺度空间内的平滑信号和细节信号能提供原始信号的时频局域信息，特别是能提供不同频段上信号的构成信息。把不同分解尺度上信号的能量求解出来，就可以将这些能量尺度顺序排列，形成特征向量供识别用
- **基于小波变换的多尺度空间的模极大值特征提取**：利用小波变换的信号局域分析能力，求解小波变换的模极大值特性来检测信号的局部奇异性，将小波变换模极大值的尺度参数s、平移参数t及其幅值作为目标的特征量
- **基于小波包变换的特征提取**：利用小波分解，可将时域随机信号序列映射为尺度域各子空间内的随机系数序列，按小波包分解得到的最佳子空间内随机系数序列的不确定性程度最低，将最佳子空间的熵值及最佳子空间在完整二叉树中的位置参数作为特征量，可以用于目标识别
- **基于适应性小波神经网络的特征提取**：基于适应性小波神经网络的特征提取方法可以把信号通过分析小波拟合表示，进行特征提取

> **小波基函数**：是一种具有局部支集的函数，并且平均值为0，小波基函数满足†(0)=0，常用的小波基有Haar小波基、db系列小波基
> **基于小波变换的多尺度空间能量分布特征提取方法**：应用小波分析技术可以把信号在各频率波段中的特征提取出来，基于小波变换的多尺度空间能量分布特征提取方法是对信号进行频带分析，再分别以计算所得的各个频带的能量作为特征向量

## 4.4 数据规约
> 在大数据集上进行复杂的数据分析和挖掘需要很长的时间，数据规约产生更小但保持原数据完整性的新数据集。在规约后的数据集上进行分析和挖掘将更有效率。数据规约的意义在于：
> 
- 降低无效，错误数据对建模的影响，提高建模的准确性
- 少量且具代表性的数据将大幅度缩减数据挖掘所需的时间
- 降低存储数据的成本

### 4.4.1 属性规约
> 通过属性合并来创建新属性维数，或者直接通过删除不相关的属性来减少数据维数，从而提高数据挖掘的效率，降低计算成本。属性规约的目标是寻找最小的属性子集并确保新数据子集的概率分布尽可能地接近原来数据集的概率分布，属性规约常用方法有：
> 
- **合并属性**：将一些旧属性合为新属性
- **逐步向前选择**：从一个空属性集开始，每次从原来属性集合中选择一个当前最优的属性添加到当前属性子集中。直到无法选择出最优属性或满足一定域值约束为止
- **逐步向后选择**：从一个全属性集开始，每次从当前属性子集中选择一个单前最差的属性并将其从当前属性子集中消去。直到无法选择出最差属性为止或满足一定域值约束为止
- **决策树归纳**：利用决策树的归纳方法对初始数据进行分类归纳学习，获得一个初始数据进行分类归纳学习，获得一个初始决策树，所有没有出现在这个决策树上的属性均可以认为是无关属性，因此将这些属性从初始集合中删除，就可以获得一个较优的属性子集
- **主成分分析**：用较少的变量去解释原始数据中的大部分变量，即将许多相关性很高的变量转化成彼此相互独立或不相关的变量
> 逐步向前选择、逐步向后删除和决策树归纳是属于直接删除不相关属性方法。主成分分析是一种用于连续属性的数据降维方法，它构造了原始数据的一个正交变换，新空间的基底去除了原始空间基底下数据的相关性，只需使用少数新变量就能够结实原始数据中的大部分变异。在应用中，通常是选出比原始变量个数少，能解释大部分数据中的变量的几个新变量，即所谓主成分，来代替原始变量进行建模

### 4.4.2 数值规约
> 数值规约指通过选择替代的、较小的数据来减少数据量，包括有参数方法和无参数方法两类。
> 
- 有参数方法是使用一个模型来评估数据，只需存放参数，而不需要存放实际数据，例如回归和对数线性模型。
- 无参数方法就需要存放实际数据，例如直方图、聚类、抽样

## 4.5 总结
> 本节主要介绍了数据预处理的4个主要任务：数据清洗、数据集成、数据变换和数据规约
> 
- 数据清洗包括对缺失值和异常值的处理，处理缺失值的方法为3类：删除记录、数据插补和不处理，处理异常值的方法有删除含有异常值的激励、不处理、平均值修正、和视为缺失值
- 数据集成是合并多个数据源中的数据，存放到一个数据存储的过程，从实体识别问题和冗余属性两个方面进行介绍
- 数据变换介绍了从不同的应用角度对已有属性进行函数变换
- 数据规约从属性规约(纵向)和数值(横向)规约两个方面介绍了如何对数据进行规约，使挖掘的性能和效率得到很大的提高。


# 5.挖掘建模
> 经过数据探索与数据预处理，得到了可以直接建模的数据。根据挖掘目标和数据形式可以建立分类与预测、聚类分析、关联规则、时序模式和偏差检测等模型。

## 5.1 分类与预测
> 就大多数企业而言，经常会遇到如下问题：
> 
- 基于当前商品的销售情况，结合节假日、气候和竞争对手等影响因素，对产品销量进行趋势预测？
- 如何预测未来一段时间哪些客户会流失，哪些客户最有可能成为VIP客户？
- 如何预测一种新产品的销售量，以及在哪种类型的客户中会比较受欢迎？
> 分类和预测是预测问题的两种主要类型，分类主要是预测分类标号(离散属性),而预测主要是建立连续值函数模型，预测给定自变量对应的因变量的值

### 5.1.1 实现过程
> **分类**是构造一个分类模型，输入样本的属性值，输出对应的类别，将每个样本映射到预先定义好的类别
> **预测**是值建立两种或两种以上变量间相互依赖的函数模型，然后进行预测或控制
> **分类和预测的实现过程类似**，以分类模型为例，第一步是学习步，通过归纳分析训练样本集来建立分类模型得到分类规则，第二步是分类步，先用已知的测试样本集评估分类规则的准确率，如果准确率可以接受，则使用该模型对未知类标号的待测样本进行预测

### 5.1.2 常用的分类与预测算法
- 回归分析
- 决策数
- 人工神经网络
- 贝叶斯网络
- 支持向量机

### 5.1.3 分类与预测算法评价
> 分类与预测模型对训练集进行预测而得出的准确率并不能很好地反映预测模型未来的性能，为了有效判断一个预测模型的性能表现，需要一组没有参与预测模型的数据集，并在该数据集上评价预测模型的准确率，这组独立的数据集叫作测试集。模型预测效果评价，通常用相对/绝对误差、平均绝对误差、均方误差、均方根误差等指标来衡量
>
- **Kappa统计**是比较两个或多个观察者对同一事物观察结果是否一致，以由于机遇造成的一致性和实际观测的一致性之间的差别大小作为评价基础的统计指标
- **识别准确率** Acuracy = (TP + FN) / (TP + TN + FP + FN)
- **识别精确率** Precision = TP / (TP + FP)
- **反馈率** Recall = TP / (TP + TN)
- **ROC(Receiver Operating Characteristic)**曲线是一种非常有效的模型评价方法，可为选定临界值给出定量分析。将灵敏度(Sensitivity)设在纵轴，1-特异性(1-Specificity)设在横轴，就可以得出ROC曲线图。该曲线下的积分面积大小与每种方法优劣密度密切相关，反映分类器正确分类的统计概率，其值越接近1说明该算法效果越好
- **混淆矩阵(Confusion Matrix)**是模式识别领域中一种常用的表达形式，它描绘样本数据的真实属性与识别结果类型之间的关系，是评价分类器性能的一种常用方法。混淆矩阵中元素的行下标对应目标的真实属性，列下标对应分类器产生的识别属性。通过混淆矩阵，可以获得分类器的正确识别率和错误识别率

## 5.2 聚类分析

### 5.2.1 常用聚类分析算法

> 与分类不同，聚类分析是在没有给定划分类别的情况下，根据数据相似度进行样本分组的一种方法。常见的聚类方法有
> 
- **划分(分裂)方法**: K-Means算法(K平均)、K-MEDOIDS(K中心点)、CLARANS算法(基于选择的算法)
- **层次分析方法**: BIRCH算法(平衡迭代规约和聚类)、CURE算法(代表点聚类)、CHAMELEON算法(动态模型)
- **基于密度的方法**: DBSCAN算法(基于高密度连接区域)、DENCLUE算法(密度分布函数)、OPTICS算法(对象排序识别)
- **基于网络的方法**: STING算法(统计信息网络)、CLIOUE算法(聚类高纬空间)、WAVE-CLUSTER算法(小波变换)
- **基于模型的方法**: 统计学方法、神经网络方法
> 常用聚类算法有:
> 
- **K-Means**: K-均值聚类也称快速聚类法，在最小化误差函数的基础上将数据划分为预定的类数K。该算法原理简单并便于处理大量数据
- **K-MEDOIDS**: K-均值算法对孤立点的敏感性，K-中心点算法不采用簇中对象的平均值作为簇中心，而选用簇中离平均值最近的对象作为簇中心
- **系统聚类**: 系统聚类也称多层次聚类，分类的单位由高到低呈树形结构，且所处的位置越低，其所包含的对象就越少，但这些对象间的共同特征越多。该聚类方法只适合在小数据量的时候使用，数据量大的时候速度会非常慢

### 5.2.2 K-Means 聚类算法
> K-Means算法是典型的基于距离的非层次聚类算法，在最小化误差函数的基础上将数据划分成预定的类数K，采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大
> 
> 聚类结果可能依赖于初始聚类中心的随机选择，可能使得结果严重偏离全局最优分类。实践中，为了得到较好的结果，通常选择不同的初始聚类中心，多次运行K-Means算法。在所有对象分配完成后，重新计算K个聚类中心时，对于连续数据，聚类中心取该簇的均值，但是当样本的某些属性时分类变量时，均值可能无定义，可以使用K-纵数方法。
> 
> **数据类型与相似性的度量**：
> 
- 对于连续属性，要先对各属性值进行零-均值规范，再进行距离的计算。在K-Means聚类算法中，一般要度量样本之间的距离、样本与簇之间的距离以及簇与簇之间的距离。度量样本之间的相似性最常用的是欧几里得距离、曼哈顿距离和闵可夫斯基距离；样本与簇之间的距离可以用样本到簇中心的距离d(e,x)；簇与簇之间的距离可以用簇中心的距离d(e,e)。
- 对于文档数据使用余弦相似性度量，先将文档数据整理成文档-词矩阵格式
> **目标函数**：使用误差平方和SSE作为度量聚类质量的目标函数，对于两种不同的聚类结果，选择误差平方和较小的分类结果

### 5.2.3 聚类分析算法评价
> 聚类分析仅根据样本数据本身将样本分组。其目的是实现组内的对象互相之间是相似的，而不同组中的对象是不同的。组内的相似性越大，组间差别越大，聚类的效果就越好。
> 
- **Purity评价法**：purity方法是极为简单的一种聚类评价方法，只需计算正确聚类数占总数的比例
- **RI评价法**：这是一种用排列组合原理来对聚类进行评价的手段。RI = (R+W)/(R+M+D+W), 其中，R是指被聚在一类的连个对象被正确分类了，W是指不应该被聚在一类的两个对象被正确的分开了，M指不应该放在一类的对象被错误的放在了一类，D指不应该分开的对象被错误的分开了
- **F值评价法**：这是基于RI方法衍生出的一个方法，F=(1+å^2)pr/(å^2p + r),其中p=R/(R+M),r=R/(R+D)。实际上RI方法就是把准确率p和召回率r看得同等重要，事实上，有时候我们可能需要某一特性更多一点，这时候就适合使用F值方法


## 5.3 关联分析
> 用一个故事引出关联规则的概念。我们在点餐时，面对菜单中大量的菜品信息，往往无法迅速找到满意的菜品，既增加了点菜的时间，也降低了客户的就餐体验。实际上，菜品的合理搭配是由规律可循的：顾客的饮食习惯、菜品的荤素和口味，有些菜品之间是相互关联的，而有些菜品之间是对立或竞争关系，这些规律都隐藏在大量的历史菜单数据中，如果能够通过数据挖掘发现客户点餐的规律，就可以快速识别客户的口味，当下了某个菜品的订单时推荐相关联的菜品，引导客户消费，提高顾客的就餐体验和餐饮企业的业绩水平。
> 
> 关联规则也称为购物篮分析，最早时为了发现超市销售数据库中不同商品之间的关联关系。例如哪种商品可能会在一次购物时同时购买，某客户买了某商品之后，三个月后买另一个相关产品的概率有多大。我们可能会发现购买了面包的顾客同时非常有可能会购买牛奶，这就导出了一条关联规则“面包=>牛奶”，其中面包称为规则的前项，而牛奶称为后项。
> 
> 关联规则分析时数据挖掘中最活跃的研究方法之一，目的是在一个数据集中找出各项之间的关联关系，而这种关系并没有在数据中直接表示出来。

### 5.3.1 常用关联规则算法
- Apriori：关联规则最常用也是最经典的挖掘频繁项集的算法，其核心思想是通过连接产生选项及其支持度然后通过剪枝生成频繁项集
- FP-Tree：针对Apriori算法固有的多次扫描事务数据集的缺陷，提出的不产生候选频繁项集的方法。Apriori和FP-Tree都是寻找频繁项集的算法
- Eclat算法：Eclat算法是一种深度优先算法，采用垂直数据表示形式，在概念理论的基础上利用基于前缀的等价关系将搜索空间划分为较小的子空间
- 灰色关联法：分析和确定各因素之间的影响程度或是若干各子因素对主因素的贡献度而进行的一种分析方法

## 5.4 时序模式
> 对很多销售企业来说，生产和销售是同时进行的，因此销售预测对于销售企业十分必要。如何基于商品历史销售数据，做好商品销售预测，以便减少商品脱销现象和避免备料不足而造成的生产延误，而从减少商品等待时间，提供给用户更优质的服务，同时可以减少安全库存量，做到生产准时制，降低物流成本。销售预测可以看作是基于时间序列的短期数据预测，预测对象为具体商品销售量。

### 5.4.1 时间序列算法
- 平滑法：平滑法常用于趋势分析和预测，利用修匀技术，削弱短期随机波动对序列的影响，使序列平滑化。根据所用平滑技术的不同，可具体分为移动平均法和指数平滑法
- 趋势拟合法：趋势拟合法把时间作为变量，相应的序列观察者作为因变量，建立回归模型。根据序列的特征，可具体分为线性拟合和曲线拟合
- 组合模型：时间序列的变化主要受到长期趋势(T)、季节变动(S)、周期变动(C)和不规则变动(œ)这4个因素的影响。根据时序的特点，可以构建加法模型和乘法模型。加法模型：x = T + S + C + œ, 乘法模型：X = T * S * C * œ
- AR模型: X = ø_0 + ø_1*x_t-1 + ø_2*x_t-2 + .... + ø_p*x_t-p + œ_t。以前p期的序列值x_t-1, x_t-2为自变量、随机变量X_t的取值x_t为因变量建立线性回归模型
- MA模型: 随机变量X_t的取值x_t与以前各期的序列值无关，建立x_t与前q期的随机扰动œ_t-1,œ_t-2的线性回归模型
- ARMA模型: 随机变量X_t的取值x_t不仅与前p期的序列有关，还与前q期的随机扰动有关
- ARIMA模型：许多非平稳序列差分后会显示出平稳序列的性质，称这个非平稳序列为差分平稳序列。对差分平稳序列可以使用ARIMA模型进行拟合
- ARCH模型：ARCH模型能准确地模拟时间序列变量的波动性变化，适用于序列具有异方差性并且异方差函数短期自相关
- GARCH模型及其衍生模型：又称广义ARCH模型，是ARCH模型的拓展。相比于ARCH模型，GARCH模型及其衍生模型更能反映实际序列中的长期记忆性、信息的非对称性等性质

### 5.4.2 时间序列的预处理
> 拿到一个观察值序列后，首先要对它的纯随机性和平稳性进行检验，这两个重要的检验称为序列的预处理。根据检验结果可以将序列分为不同的类型，对不同类型的序列会采取不同的分析方法。
> 
- 对于纯随机序列，又称为白噪声序列，序列的各项之间没有任何相关关系，序列在进行完全无序的随机波动，可以终止对该序列的分析。白噪声序列是没有信息可提取的平稳序列
- 对于平稳非白噪声序列，它的均值和方差是常数，现已有一套非常成熟的平稳序列的建模方法。通常是建立一个线性模型来拟合该序列的发展，借此提取该序列有用信息。ARMA模型是最常用的平稳序列拟合模型。
- 对于非平稳序列，由于它的均值和方差不稳定，处理方法一般是将其转变为平稳序列，这样就可以应用有关平稳时间序列的分析方法，建立ARMA模型来进行相应的研究。如果一个时间序列经差分运算后具有平稳性，则该序列为差分平稳序列，可以使用ARIMA模型进行分析
> **平稳性检验**：
>
- 平稳时间序列的定义： 对于随机变量X，计算其期望，方差，对于连个随机变量X和Y，计算其协方差、相关系数，它们度量了两个不同事件之间的相互影响程度。如果时间序列在某一常数附近波动且波动范围有限，即有常数均值和常数方差，并且延迟k期的序列变量的自协方差和自相关系数是相等的活着说延迟k期的序列变量之间的影响程度是一样的，则称为平稳序列
- 平稳性的检验：对序列的平稳性的检验有两种方法，一种是是根据时序图和自相关图的特征做出判断的图检验，该方法操作简单、应用广泛、缺点是带有主观性；另一种是构造检验统计量进行检验的方法，目前最常用的方法是单位根检验
	* 时序图检验：根据平稳时间序列的均值和方差都为常数的性质，平稳序列的时序图显示该序列值始终在一个常数附近随机波动，而且波动的范围有界，如果有明显的趋势性或者周期性，那它通常不是平稳序列
	* 自相关图检验：平稳序列具有短期相关性，这个性质表明对平稳序列而言通常只有近期的序列值对现实值的影响比较明显，间隔越远的过去值对现时值的影响越小。随着延迟期数k的增加，平稳序列的自相关系数P_k会比较快的衰减趋向于零，并在零附近随机波动，而非平稳序列的自相关系数衰减的速度比较慢，这就是利用自相关图进行平稳检验的标准
	* 单位根检验：指检验序列中是否存在单位根，如果存在单位根就是非平稳时间序列了
- 纯随机性检验：如果一个序列时纯随机序列，那么它的序列值之间应该没有任何关系，即满足®(k)=0, k!=0这是一种理论上才会出现的理想状态，实际上纯随机序列的样本自相关系数不会绝对为零，但是很接近零，并在零附近波动。纯随机性检验也称白噪声检验，一般是构造检验统计量来检验序列的纯随机性，常用的检验统计量有Q统计量、LB统计量，由样本各延迟期数的自相关系数可以计算得到检验统计量，然后计算出对应的p值，如果p值显著大于显著水平å，则表示该序列不能拒绝纯随机的原假设，可以停止对该序列的分析

### 5.4.3 平稳时间序列分析
> ARMA模型的全程是自回归移动平均模型，它是目前最常用的拟合平稳序列的模型。它又可以细分为AR模型、MA模型和ARMA模型三大类。都可以看作是多元线性回归模型
> 
- AR模型：
	* 均值
	* 方差：平稳AR模型的房车有界，等于常数
	* 自相关系数ACF：平稳模型的自相关系数呈指数的速度衰减，始终有非零取值，不会在k大于某个常数之后就恒等于零，这个性质就是平稳AR模型的自相关系数具有拖尾性。
	* 偏自相关系数PACF：对于一个平稳模型，求出延迟k期的自相关系数p_k时，实际上得到的并不是X_t与X_t-k之间单纯的相关系数，因为X_t同时还会受到中间k-1个随机变量的影响，所以自相关系数P_k实际上参杂了其它变量对X_t与X_t-k的相关影响，为了单纯地测度X_t-k对X_t的影响，引进偏自相关系数的概念。可以证明平稳AR模型的偏自相关系数具有p阶截尾性。这个性质时连同前面的自相关系数的拖尾性时AR模型重要的识别依据。
- MA模型
- ARMA模型
- 平稳时间序列建模：某个时间序列经过预处理，被判定为平稳非白噪声序列，就可以利用ARMA模型进行建模。计算出平稳非白噪声序列{X_t}的自相关系数和偏自相关系数，再有PA模型，AM模型和ARMA的自相关系数和偏自相关系数的性质，选择合适的模型。
	* 计算ACF和PACF。先计算非平稳白噪声序列的自相关系数ACF和偏自相关系数PACF
	* ARMA模型识别。也称模型定阶，由AR模型、MA模型、ARMA的自相关系数和偏自相关系数的性质，选择合适的模型
		- AR->拖尾->p阶截尾
		- MA->q阶截尾->拖尾
		- ARMA->p阶拖尾->q阶截尾
	* 估计模型中未知参数的值进行参数检验
	* 模型检验
	* 模型优化
	* 模型应用：进行短期预测

### 5.4.4 非平稳时间序列分析
> 实际上，在自然界中绝大部分序列都是非平稳的。因而对非平稳序列的分析更普遍、更重要，创造出来的分析方法也更多。对非平稳时间序列的分析方法可以分为确定性因素分解的时序分析和随机时序分析两大类。
> 
> 确定性因素分解的方法把所有序列的变化都归结为4个因素(长期趋势、季节变动、循环变动和随机波动)的综合影响，其中长期趋势和季节变动的规律性信息通常比较容易提取，而由随机因素导致的波动则非常难确定和分析，对随机信息浪费严重，会导致模型拟合精度不够理想。
> 
> 随机时序分析法的发展就是为了弥补确定性因素分解方法的不足。根据时间序列的不同特点，随机时序分析可以建立的模型由ARIMA模型、残差自回归模型、季节模型、异方差模型
> 
> **差分运算**

- p阶差分：相距一期的两个序列值之间的减法运算称为1阶差分运算
- k步差分：相距k期的两个序列值之间的减法运算称为k阶差分运算

> **ARIMA模型**：差分运算具有强大的确定性信息提取能力，许多非平稳序列差分后会显示出平稳序列的性质，这时称这个非平稳序列为差分平稳序列。对差分平稳序列可以使用ARMA模型进行拟合。ARIMA模型的实质就是差分运算与ARMA模型的组合。
> 

## 5.5 离群点检测
> 离群点检测是数据挖掘中重要的一部分，它的任务是发现与大部分其它对象显著不同的对象。大部分数据挖掘方法都将这种差异信息视为噪声而丢弃，然而在一些应用中，罕见的数据可能蕴含着更大的研究价值。

- 离群点的成因：来源于不同的类、自然变异、数据测量和收集误差
- 离群点的类型：
	* 从数据范围：全局离群点和局部离群点
	* 从数据类型：数值型离群点和分类型离群点
	* 从属性的个数：一维离群点和多维离群点

### 5.5.1 离群点检测方法
- 基于统计：
	* 大部分的基于统计的离群点检测方法是构建一个概率分布模型，并计算对象符合该模型的概率，把具有低概率的对象视为离群点
	* 基于统计模型的离群点检测方法的前提是必须知道数据集服从什么分布；对于高维度数据，检验效果可能很差
- 基于邻近度：
	* 通常可以在数据对象之间定义邻近性度量，把远离大部分点的对象视为离群点
	* 简单，二维或三维的数据可以做散点图观察；大数据集不适用；对参数选择敏感；具有全局阈值，不能处理具有不同密度区域的数据集
- 基于密度：
	* 考虑数据集可能存在不同密度区域这一事实，从基于密度的观点分析，离群点事在低密度区域中的对象。一个对象的离群点得分是该对象周围密度的逆
	* 给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能很好处理；大数据集不适用；参数选择是困难的
- 基于聚类：
	* 一种利用聚类检测离群点的方法是丢弃远离其他簇的小簇，另一种更系统的方法，首先先聚类所有对象，然后评估对象属于簇的程度(离群点得分)
	* 基于聚类技术来发展离群点可能是高度有效的；聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大

> 基于统计模型的离群点检测方法需要满足统计学原理，如果分布已知，则检测可能非常有效。基于邻近度的离群点检测方法比统计学方法更一般、更容易使用，因为确定数据集有意义的邻近度量比确定它的统计分布更容易。基于密度的离群点检测与基于邻近度的离群点检测密切相关，因为密度常用邻近度定义：一种是定义密度到K个最近邻近的平均距离的倒数，如果该距离小，则密度高；另一种是使用DBSCAN聚类算法，一个对象周围的密度等于该对象指定距离d内对象的个数。

### 5.5.2 基于模型的离群点检测方法
> 通过估计概率分布的参数来建立一个数据模型。如果一个数据对象不能很好地同该模型拟合，即如果它很可能不服从该分布，则它是一个离群点。
> 
- **一元正态分布中的离群点检测**：N(0,1)的数据对象出现在该分布的两边尾部的机会很小，因此可以用它作为检测数据对象是否是离群点的基础。数据对象落在3倍标准差中心区域之外的概率仅有0.0027.
- **混合模型的离群点检测**：混合模型是一种特殊的统计模型，它使用若干统计分布对数据建模。每一个分布对于一个簇，而每个分布的参数提供对于簇的描述，通常用中心和发散描述。混合模型将数据看作从不同的概率分布得到的观测值的集合。概率分布可以是任何分布，但是通常是多元正态的，因为这种类型的分布不难理解，容易从数学上进行处理，并且已经证明在许多情况下都能产生好的结果。这种类型的分布可以对椭圆簇建模。总的来说，混合型数据产生过程为：给定几个类型相同但参数不同的分布，随机地选取一个分布并由它产生一个对象。重复该过程m次，其中m是对象的个数。

### 5.5.3 基于聚类的离群点检测方法
> 聚类分析用于发现局部相关的对象组，而异常检测用来发现不与其它对象强相关的对象。因此，聚类分析非常自然地可以用于离群点检测。
> 
- **丢弃远离其它簇的小簇**：一种利用聚类检测离群点的方法是丢弃远离其它簇的小簇。通常这个过程可以简化为丢弃小于某个最小阈值的所有簇。这个方法可以和其它任何聚类技术一起使用，但是需要最小簇大小和小簇与其它簇之间距离的阈值。而且这种方案对簇个数的选择高度敏感，使用这个方案很难将离群点得分附加到对象上
- **基于原型的聚类**：这是另一种更系统的方法。首先聚类所有对象，然后评估对象属于簇的程度。在这种方法中，可以用对象到它的簇中心的距离来度量属于簇的程度。特别地，如果删除一个对象导致该目标的显著改进，则可将该对象视为离群点。对于基于原型的聚类，主要由两种方法评估对象属于簇的程度：一是度量对象到簇原型的距离，并用它作为该对象的离群点得分；而是考虑到簇具有不同的密度，可以度量簇到原型的相对距离，相对距离是点到质心的距离的中位数之比
	* 诊断步骤
		- 进行聚类。选择聚类算法，将样本集聚为K簇，并找到各簇的质心。
		- 计算各对象到它的最近质心的距离
		- 计算各对象到它最近质心的相对距离
		- 与给定的阈值作比较，如果某对象距离大于该阈值，就认为该对象是离群点

## 5.6 小结
> 数据挖掘技术的基本任务主要体现在分类与预测、聚类、关联规则、时序模型、离群点检测5个方面。
