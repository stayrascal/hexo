---
title: 数据科学家之路
date: 2020-02-01 13:07:36
tags:
---

# 特征工程

数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。特征工程在机器学习中占有相当重要的地位。
一个完整的特征工程，大致可以分为探索性数据分析(Exploratory)、数据预处理(Data Preprocessing)、特征提取(Feature Extraction)、特征筛选(Feature Selection)、特征构造(Feature construction)等过程。

## 1.0. EDA

每个数据科学家都必须掌握的最重要的技能之一是正确研究数据的能力。彻底的探索性数据分析 (EDA， Exploratory Data Analysis) 是必要的，这是为了确保收集数据和执行分析的完整性。

### 探索性数据分析（EDA）目标

- 快速描述一份数据集：行/列数、数据丢失情况、数据的类型、数据预览。
- 清除脏数据：处理丢失的数据、无效的数据类型和不正确的值。
- 可视化数据分布：条形图，直方图，箱型图等。
- 计算并可视化展示变量之间的相关性(关系)：热图 (heatmap)。

### 常用库

```
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
```

### 常用工具库

- pandas_profiling
- easyeda

## 1.2 数据预处理

数据预处理往往包括数据清洗、数值特征无量纲化、数值型特征特征分箱、统计变换、特征编码等几步。而数据清洗是数据预处理的重要组成部分，会直接影响机器学习的效果。

### 数据清洗

数据清洗(Data cleaning)，对数据进行重新审查和校验的过程，目的在于删除重复信息、纠正存在的错误，并提供数据一致性。
数据清洗的一般流程：

- 格式内容清洗
- 逻辑错误清洗
- 异常数据清洗
- 缺失数据清洗
- 非需求数据清洗

不同的数据质量不一样，并不是所有项目中都需要进行每一项数据清洗，应该根据实际情况选择必要的数据清洗方式。在实际操作中，如果不知道哪些是非需求数据，可以不进行非需求数据清洗，在数据预处理之后再进行特征筛选。

#### a) 格式内容清洗

- 格式内容问题产生的原因
  - 数据是由人工收集或用户填写而来，则有很大可能性在格式和内容上存在一些问题；
  - 不同版本的程序产生的内容或格式不一致；
  - 不同数据源采集而来的数据内容和格式定义不一致。
- 时间、日期格式不一致清洗
  - 把时间/日期数据库转换成统一的表示方式，包括格式，单位
- 数值格式不一致清洗
  - 把数值转换成统一的表示方式
- 全半角等显示格式不一致清洗
- 内容中有不该存在的字符清洗
  - 比如姓名中存在数字符号、身份证号中出现汉字等问题，需要以半自动校验半人工方式来找出可能存在的问题，并去除不需要的字符。
- 内容与该字段应有内容不符清洗
  - 姓名写了性别，身份证号写了手机号等等
  - 并不能简单的以删除来处理，因为成因有可能是人工填写错误，也有可能是前端没有校验，还有可能是导入数据时部分或全部存在列没有对齐的问题，因此要详细识别问题类型。
- 数据类型不符清洗
  - 由于人为定义错误、转存、加载等原因，数据类型经常会出现数据类型不符的情况，比如金额特征是字符串类型。

#### b) 逻辑错误清洗

- 数据重复清洗
  - 存在各个特征值完全相同的两条/多条数据：df.drop_duplicates()
  - 数据不完全相同，但从业务角度看待数据是同一个数据，比如页面埋点时，进入页面和退出页面都会上报一次数据，只有时间不一样：df.drop_duplicates(subset=['ID'], keep='last')
- 不合理值清洗，比如年龄超过 200 岁
- 矛盾内容修正，比如身份证号和年龄

#### c) 异常值清洗

异常值是数据分布的常态，处于特定分布区域或范围之外的数据通常被定义为异常或噪声。异常分为两种：

- “伪异常”，由于特定的业务运营动作产生，是正常反应业务的状态，而不是数据本身的异常；
- “真异常”，不是由于特定的业务运营动作产生，而是数据本身分布异常，即离群点。

* 1.异常值检查方法

  - 基于统计分析

    通过分析统计数据的散度情况，即数据变异指标，来对数据的总体特征有更进一步的了解，对数据的分布情况有所了解，进而通过数据变异指标来发现数据中的异常点数据。常用的数据变异指标：

    - 极差
    - 四分位数间距
    - 均差
    - 标准差
    - 变异系数: 变异指标的值大表示变异大、散布广；值小表示离差小，较密集
    - MAD: MAD 的方法相对于分位数方法的一大优势即在于 MAD 方法对样本大小是不敏感也即是稳定的鲁棒的一种评价指标

      ```
      from scipy.stats import norm

      def mad_based_outlier(points, thresh=3.5):
          if type(points) is list:
              points = np.asarray(points)
          if len(points.shape) == 1:
              points = points[:, None]
          med = np.median(points, axis=0)
          abs_dev = np.absolute(points - med)
          med_abs_dev = np.median(abs_dev)

          mod_z_score = norm.ppf(0.75) * abs_dev / med_abs_dev
          return mod_z_score > thresh

      def percentile_based_outlier(data, threshold=95):
          diff = (100 - threshold) / 2.0
          minval, maxval = np.percentile(data, [diff, 100 - diff])
          return (data < minval) | (data > maxval)
      ```

  - 3σ 原则
    - 若数据存在正态分布，在 3σ 原则下，异常值为一组测定值中与平均值的偏差超过 3 倍标准差的值，如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。
  - 箱线图分析
    - 箱线图提供了识别异常值的一个标准：如果一个值小于 QL-1.5IQR 或大于 OU+1.5IQR 的值，则被称为异常值。
    - 箱型图判断异常值的方法以四分位数和四分位距为基础，四分位数具有鲁棒性。
  - 基于模型检测
    - 首先建立一个数据模型，异常是那些同模型不能完美拟合的对象；如果模型是簇的集合，则异常是不显著属于任何簇的对象；在使用回归模型时，异常是相对远离预测值的对象。
    - 优点：有坚实的统计学理论基础，当存在充分的数据和所用的检验类型的知识时，这些检验可能非常有效。
    - 缺点：对于多元数据，可用的选择少一些，并且对于高维数据，这些检测可能性很差。
  - 基于距离
    - 即若一个数据对象和大多数点距离都很远，那这个对象就是异常。
    - 主要使用的距离度量方法有绝对距离(曼哈顿距离)、欧氏距离和马氏距离等方法。
    - 优点：基于距离的方法比基于统计类方法要简单得多，因为为一个数据集合定义一个距离的度量要比确定数据集合的分布容易的多。
    - 缺点：
      - 基于邻近度的方法需要 O(m2)时间，大数据集不适用
      - 该方法对参数的选择也是敏感的；
      - 不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。
  - 基于密度
    - 考察当前点周围密度，可以发现局部异常点，离群点的局部密度显著低于大部分近邻点，适用于非均匀的数据集。
    - 优点：给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理。
    - 缺点：
      - 与基于距离的方法一样，这些方法必然具有 O(m2)的时间复杂度。对于低维数据使用特定的数据结构可以达到 O(mlogm)；
      - 参数选择困难。虽然算法通过观察不同的 k 值，取得最大离群点得分来处理该问题，但是，仍然需要选择这些值的上下界。
  - 基于聚类
    - 对象是否被认为是异常点可能依赖于簇的个数（如 k 很大时的噪声簇）。
    - 如果存在大量小簇时一个对象是异常点，则它多半是一个真正的异常点。
    - 优点：
      - 基于线性和接近线性复杂度（k 均值）的聚类技术来发现离群点可能是高度有效的；
      - 簇的定义通常是离群点的补，因此可能同时发现簇和离群点。
    - 缺点：
      - 产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性；
      - 聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。
  - 基于邻近度的异常点检测
    - 如果它远离大部分点。那么它是异常的。这种方法比统计学方法更一般、更容易使用，因为确定数据集的有意义的邻近性度量比确定它的统计分布更容易。
    - 优点：简单
    - 缺点：
      - 基于邻近度的方法需要 O(m2)时间，大数据集不适用；
      - 该方法对参数的选择也是敏感的；
      - 不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。

  在数据处理阶段将离群点作为影响数据质量的异常点考虑，而不是作为通常所说的异常检测目标点，因此一般采用较为简单直观的方法，结合箱线图和 MAD 的统计方法判断变量的离群点。

* 2.数据光滑处理

  除了检测出异常值然后再处理异常值外，还可以使用以下方法对异常数据进行光滑处理。

  - 分箱

    - 分箱方法通过考察数据的“近邻”（即周围的值）来光滑有序数据的值，有序值分布到一些“桶”或箱中。由于分箱方法考察近邻的值，因此进行局部光滑。
    - 分箱方法: 等频、等宽
    - 分箱光滑技术：
      - 箱均值平滑
      - 箱中位数平滑
      - 箱边界平滑

  - 回归
    - 可以用一个函数（如回归函数）拟合数据来光滑数据

* 3.异常值处理方法
  - 对异常值处理，需要具体情况具体分析，异常值处理的方法常用有四种：
    - 删除含有异常值的记录；某些筛选出来的异常样本是否真的是不需要的异常特征样本，最好找懂业务的再确认一下，防止我们将正常的样本过滤掉了。
    - 将异常值视为缺失值，交给缺失值处理方法来处理；
    - 使用均值/中位数/众数来修正；
    - 不处理。

#### d) 缺失值清洗

没有高质量的数据，就没有高质量的数据挖掘结果，数据值缺失是数据分析中经常遇到的问题之一。

- 造成缺失值的原因

  - 信息暂时无法获取，比如数据有滞后性
  - 信息别遗漏，比如输入时认为不重要、忘记填写了或对数据理解错误而遗漏，也可能是采集过程发生故障导致
  - 获取这些信息的代价太大
  - 有些对象的某个或某些熟悉是不可用的，比如一个未婚者的配偶信息，一个儿童的固定收入状况等。

- 缺失值数据处理的方法

  - 删除元组
    - 优点：简单易行，在对象有多个属性缺失值、被删除的含缺失值的对象与初始数据集的数据量相比非常小的情况下非常有效；
    - 缺点：当缺失数据所占比例较大，特别当遗漏数据非随机分布时，这种方法可能导致数据发生偏离，从而引出错误的结论。
  - 数据填充
  - 不处理

- 数据填充方法

  - 人工填充
  - 特殊值填充: df['Feature'].fillna('unknown', inplace=True)
  - 统计量填充:

    - 若缺失率较低（小于 95%）且重要性较低，则根据数据分布的情况进行填充。
    - 常用的填充统计量：

      - 平均值: 对于数据符合均匀分布，用该变量的均值填补缺失值。df['Feature1'].fillna(df['Feature1'].mean(), inplace=True)
      - 中位数: 对于数据存在倾斜分布的情况，采用中位数填补缺失值。df['Feature2'].fillna(df['Feature2'].mode().iloc[0], inplace=True)
      - 众数: 离散特征可使用众数进行填充缺失值。
      - 条件平均值填充: 在该方法中，用于求平均值/众数/中位数并不是从数据集的所有对象中取，而是从与该对象具有相同决策属性值的对象中取得。

        ```
        # 条件平均值填充
        def condition_mean_fillna(df, label_name, feature_name):
            mean_feature_name = '{}Mean'.format(feature_name)
            group_df = df.groupby(label_name).mean().reset_index().rename(columns={feature_name: mean_feature_name})

            df = pd.merge(df, group_df, on=label_name, how='left')
            df.loc[df[feature_name].isnull(), feature_name] = df.loc[df[feature_name].isnull(), mean_feature_name]
            df.drop(mean_feature_name, inplace=True, axis=1)
            return df

        df = condition_mean_fillna(df, 'Label', 'Feature2')
        ```

  - 模型预测填充

    - 使用待填充字段作为 Label，没有缺失的数据作为训练数据，建立分类/回归模型，对待填充的缺失字段进行预测并进行填充。常用方法：

      - 最近距离邻法：先根据欧式距离或相关分析来确定距离具有缺失数据样本最近的 K 个样本，将这 K 个值加权平均/投票来估计该样本的缺失数据。

        ```
        from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor

        def knn_missing_filled(x_train, y_train, test, k = 3, dispersed = True):
            '''
            @param x_train: 没有缺失值的数据集
            @param y_train: 待填充缺失值字段
            @param test: 待填充缺失值数据集
            '''
            if dispersed:
                clf = KNeighborsClassifier(n_neighbors = k, weights = "distance")
            else:
                clf = KNeighborsRegressor(n_neighbors = k, weights = "distance")

            clf.fit(x_train, y_train)
            return test.index, clf.predict(test)
        ```

      - 回归

  - 插值法填充
    - 包括随机插值，多重插补法，热平台插补，拉格朗日插值，牛顿插值等。
    - 线性插值法: 通过两点（x0，y0），（x1，y1）估计中间点的值，假设 y=f(x)是一条直线，通过已知的两点来计算函数 f(x)，然后只要知道 x 就能求出 y，以此方法来估计缺失值。
      ```
      df['Feature'] = df['Feature'].interpolate()
      ```
    - 多重插值:多值插补的思想来源于贝叶斯估计，认为待插补的值是随机的，它的值来自于已观测到的值。具体实践上通常是估计出待插补的值，然后再加上不同的噪声，形成多组可选插补值。根据某种选择依据，选取最合适的插补值。
      - 多重插补方法分为三个步骤
      - 为每个空值产生一套可能的插补值，这些值反映了无响应模型的不确定性；每个值都可以被用来插补数据集中的缺失值，产生若干个完整数据集合；
      - 每个插补数据集合都用针对完整数据集的统计方法进行统计分析；
      - 对来自各个插补数据集的结果，根据评分函数进行选择，产生最终的插补值。
    - 哑变量填充
      - 若变量是离散型，且不同值较少，可转换成哑变量
      - 例如性别 SEX 变量，存在 male，fameal，NA 三个不同的值，可将该列转换成 IS_SEX_MALE、IS_SEX_FEMALE、IS_SEX_NA
      - 若某个变量存在十几个不同的值，可根据每个值的频数，将频数较小的值归为一类’other’，降低维度。此做法可最大化保留变量的信息。
    - 热卡填充(Hot deck imputation，就近补齐)
      - 热卡填充法在完整数据中找到一个与它最相似的对象，然后用这个相似对象的值来进行填充。这个方法的缺点在于难以定义相似标准，主观因素较多。
    - 期望值最大化填充（Expectation maximization，EM）
      - EM 算法是一种在不完全数据情况下计算极大似然估计或者后验分布的迭代算法。在每一迭代循环过程中交替执行两个步骤：E 步（Excepctaion step，期望步），在给定完全数据和前一次迭代所得到的参数估计的情况下计算完全数据对应的对数似然函数的条件期望；M 步（Maximzation step，极大化步），用极大化对数似然函数以确定参数的值，并用于下步的迭代。算法在 E 步和 M 步之间不断迭代直至收敛，即两次迭代之间的参数变化小于一个预先给定的阈值时结束。该方法可能会陷入局部极值，收敛速度也不是很快，并且计算很复杂。
      - 缺点： 由线性模型化所报告的软件标准误和检验统计量并不正确，且对于过度识别模型，估计值不是全然有效的。

- 缺失值处理步骤
  - 确定缺失值范围：对每个字段都计算其缺失值比例，然后按照缺失比例和字段重要性，分别制定策略
  - 去除不需要的字段
  - 填充缺失内容
  - 重新取数: 如果某些指标非常重要又缺失率高，那就需要和取数人员或业务人员了解，是否有其他渠道可以取到相关数据

#### e) 非需求数据清洗

简单来说就是把不要的字段删了。

数据清洗结束后我们，就可以对特征进行处理了，比如无量纲化、特征分箱、统计变换等。这部分操作可以在后阶段再进行，因为有些特征处理和最终选取的模型有关，比如树模型（Random Forest、GBDT、xgboost 等）对特征数值幅度不敏感，可以不进行无量纲化和统计变换处理；同时，由于树模型依赖于样本距离来进行学习，所以也可以不进行类别特征编码（但字符型特征不能直接作为输入，所以需要至少要进行标签编码）。但依赖样本距离来学习的模型（如线性回归、SVM、深度学习等）需要进行数据预处理，比如

- 对于数值型特征需要进行无量纲化处理；
- 对于一些长尾分布的数据特征，可以做统计变换，使得模型能更好优化；
- 对于线性模型，特征分箱可以提升模型表达能力；

另外对数值型特征进行特征分箱可以让模型对异常数据有很强的鲁棒性，模型也会更稳定。同时分箱后需要进行特征编码。

### 数值特征无量纲化

#### a) 数据标准化

标准化的前提是特征值服从正态分布，标准化后转换成标准正态分布

- Z 分数标准化: 基于原始数据的均值（mean）和标准差（standarddeviation）进行数据的标准化。适用于属性 A 的最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。

  - 优点：简单，容易计算，应用于数值型的数据，不受数据量级的影响
  - 缺点：

    - 需要总体平均值和方差，这一值在真实的分析挖掘中很难得到，大部分情况下用样本均值和标准差代替
    - 对数据分布有一定的要求，正太分布式最有利于 Z-Score 计算的
    - Z-Score 消除了数据具有的实际意义，不同字段的 Z-Score 不再有关系，Z-Score 的结果只能用于比较数据的结果，数据的真实意义还需要还原原值
    - 存在异常值时，无法保证平衡的特征尺度。

    ```
    from sklearn.preprocessing import StandardScaler
    standardScaler  = StandardScaler().fit(X_train)
    standardScaler.transform(X_train)
    ```

#### b) 数据归一化

- MinMax 归一化: 区间缩放法利用了边界值信息，将属性缩放到[0,1]

  - 缺点：当有新数据加入时，可能导致 max 和 min 的变化，需要重新定义；对异常值的存在非常敏感

  ```
  from sklearn.preprocessing import MinMaxScaler
  minMaxScaler  = MinMaxScaler().fit(X_train)
  minMaxScaler.transform(X_train)
  ```

- MaxAbs 归一化: 单独地缩放和转换每个特征，使得训练集中的每个特征的最大绝对值将为 1.0，将属性缩放到[-1,1]。它不会移动/居中数据，因此不会破坏任何稀疏性。
  - 缺点：
    - 当有新数据加入时，可能导致 max 的变化，需要重新定义
    - 对正异常值敏感
  ```
  from sklearn.preprocessing import MaxAbsScaler
  maxAbsScaler  = MaxAbsScaler().fit(X_train)
  maxAbsScaler.transform(X_train)
  ```
- 行归一化

#### c) 标准化 VS 归一化

- 与标准化的相同点
  - 都能取消由于量纲不同引起的误差
  - 都是一种线性变换，都是对向量 X 按照比例压缩再进行平移
- 与标准化的不同点
  - 目的不同，归一化是为了消除量纲压缩到[0,1]区间；标准化知识调整特征的整体分布
  - 归一化与最大，最小值有关；标准化与均值，标准差有关
  - 归一化输出在[0,1]之间；标注化无限制
- 什么时候用归一化
  - 对结果范围有要求
  - 数据稳定，不存在极端的最大值最小值
  - 不涉及距离度量、协方差计算、数据不符合正态分布的时候，比如图像处理，将 RGB 图像转换为灰度图像后，将值限定在[0 155 的范围]
  - 基于参数、距离的模型，需要对参数或者距离进行归一化，基于树的方法不需要归一化
- 什么时候用标准化

  - 数据存在异常值和较多噪音，可以间接通过中心化避免异常值和极端值
  - 在分类、聚类算法中，需要使用距离来度量相似性的时候(比如 SVM、KNN)，或者使用 PCA 技术进行降维的时候

  一般来说，优先使用标准化，如果对数据的分布有假设的话，可以使用相对应的概率密度函数来转换

#### d) 正太分布化(Normalization)

正则化的过程是将每个样本缩放到单位范数。该方法是文本分类和聚类分析中经常使用的向量空间模型（Vector Space Model)的基础。
Normalization 主要思想是对每个样本计算其 p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的 p-范数(l1-norm,l2-norm)等于 1。

```
from sklearn.preprocessing import Normalizer
#归一化，返回值为归一化后的数据
normalizer  = Normalizer(norm='l2').fit(X_train)
normalizer.transform(X_train)
```

### 数值型特征特征分箱

特征进行分箱后，需要对分箱后的每组（箱）进行 woe 编码和 IV 值的计算，通过 IV 值进行变量筛选后，然后才能放进模型训练。

#### a) 分箱的重要性和优势

- 离散特征的增加和减少都很容易，易于模型的快速迭代；
- 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
- 离散化后的特征对异常数据有很强的鲁棒性；比如一个特征是年龄>30 是 1，否则 0。如果特征没有离散化，一个异常数据“年龄 300 岁”会给模型造成很大的干扰；
- 对于线性模型，表达能力受限；单变量离散化为 N 个后，每个变量有单独的权重，相当于模型引入了非线性，能够提升模型表达能力，加大拟合；
- 离散化后可以进行特征交叉，由 M+N 个变量变为 M\*N 个变量，进一步引入非线性，提升表达能力；
- 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险；
- 可以将缺失作为独立的一类带入模型；
- 将所有变量变换到相似的尺度上。

#### b) 无监督分箱法

- 自定义分箱
- 等距分箱: pandas.cut(col, n)
- 等频分箱: pandas.pcut(col, n)
- 聚类分箱
  - Step0: 对预处理后的数据进行归一化处理
  - Step1: 将归一化处理过的数据，应用 k-means 聚类算法，划分为多个区间，采用等距法设定 k-means 聚类算法的初始中心，得到聚类中心
  - Step2: 将相邻的聚类中心的中点作为分类的划分点，将各个对象加入到距离最近的类中
  - Step3: 重新计算每个聚类中心，重新划分数据，直到每个聚类中心不再变化，得到最终的聚类结果
  ```
  from sklearn.cluster import KMeans
  kmodel=KMeans(n_clusters=k)  #k为聚成几类
  kmodel.fit(data.reshape(len(data),1))) #训练模型
  c=pd.DataFrame(kmodel.cluster_centers_) #求聚类中心
  c=c.sort_values(by=’列索引') #排序　　
  w=pd.rolling_mean(c,2).iloc[1:] #用滑动窗口求均值的方法求相邻两项求中点，作为边界点
  w=[0] +list(w[0] + [ data.max() ]  #把首末边界点加上
  d3= pd.cut(data,w,labels=range(k)) #cut函数
  ```
- 二值化

  ```
  from sklearn.preprocessing import Binarizer
  # Binarizer函数也可以设定一个阈值，结果数据值大于阈值的为1，小于阈值的为0
  binarizer = Binarizer(threshold=0.0).fit(X_train)
  binarizer.transform(X_train)
  ```

- IV(Information Value) & WOE(Weights of Evidence)

#### c) 有监督分箱法

- 卡方分箱: 自底向上的(即基于合并的)数据离散化方法。它依赖于卡方检验：具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。

  - 步骤
    - Step0: 预先定义一个卡方的阈值
    - Step1: 初始化，根据要离散的属性对实例进行排序，没个实例属于一个区间
    - Step2: 合并区间，计算没一对相邻区间的卡方值，将卡方值最小的一对区间合并
  - 注意
    - ChiMerge 算法推荐使用 0.90、0.95、0.99 置信度，最大区间数取 10 到 15 之间；
    - 也可以不考虑卡方阈值，此时可以考虑最小区间数或者最大区间数。指定区间数量的上限和下限，最多几个区间,最少几个区间；
    - 对于类别型变量，需要分箱时需要按照某种方式进行排序。

- 最小熵法分箱：数据集的熵越低，说明数据之间的差异越小。

### 统计变换

数据分布的倾斜有很多负面的影响。我们可以使用特征工程技巧，利用统计或数学变换来减轻数据分布倾斜的影响。使原本密集的区间的值尽可能的分散，原本分散的区间的值尽量的聚合。这些变换函数的主要作用在于它能帮助稳定方差，始终保持分布接近于正态分布并使得数据与分布的平均值无关。

#### a) Log 变换

Log 变换通常用来创建单调的数据变换。它的主要作用在于帮助稳定方差，始终保持分布接近于正态分布并使得数据与分布的平均值无关。当应用于倾斜分布时 Log 变换是很有用的，因为 Log 变换倾向于拉伸那些落在较低的幅度范围内自变量值的范围，倾向于压缩或减少更高幅度范围内的自变量值的范围。从而使得倾斜分布尽可能的接近正态分布。

$$
y = log_b(x)
$$

```
fcc_survey_df['Income_log'] = np.log((1+fcc_survey_df['Income']))
```

#### b) Box-Cox 变换

Box-Cox 变换是另一个流行的幂变换函数簇中的一个函数。该函数有一个前提条件，即数值型值必须先变换为正数（与 log 变换所要求的一样）。万一出现数值是负的，使用一个常数对数值进行偏移是有帮助的。用于连续的响应变量不满足正态分布的情况。Box-Cox 变换之后，可以一定程度上减小不可观测的误差和预测变量的相关性。Box-Cox 变换的主要特点是引入一个参数，通过数据本身估计该参数进而确定应采取的数据变换形式，Box-Cox 变换可以明显地改善数据的正态性、对称性和方差相等性，对许多实际数据都是行之有效的。参数 λ 的最佳取值通常由最大似然或最大对数似然确定。

$$
y = f(x, \lambda) = x^{\lambda} =
\begin{cases}
\frac{x^{\lambda} - 1}{\lambda},\quad \lambda > 0
\\\\
{log_{\theta}(x)}, \quad \lambda = 0
\end{cases}
$$

```
import scipy.stats as spstats
# 从数据分布中移除非零值
income = np.array(fcc_survey_df['Income'])
income_clean = income[~np.isnan(income)]
# 计算最佳λ值
l, opt_lambda = spstats.boxcox(income_clean)
print('Optimal lambda value:', opt_lambda)

# 进行Box-Cox变换
fcc_survey_df['Income_boxcox_lambda_opt'] = spstats.boxcox(fcc_survey_df['Income'],lmbda=opt_lambda)
```

### 类别特征编码

- LabelEncode:标签编码
  - 优点：相对于 OneHot 编码，LabelEncoder 编码占用内存空间小，并且支持文本特征编码。
  - 缺点：它隐含了一个假设：不同的类别之间，存在一种顺序关系。所以目前还没有发现标签编码的广泛使用，一般在树模型中可以使用。
  ```
  from sklearn.preprocessing import LabelEncoder
  le = LabelEncoder()
  le.fit(["paris", "paris", "tokyo", "amsterdam"])
  ```
- OneHotEncode:独热编码

  - 优点：独热编码解决了分类器不好处理属性数据的问题，在一定程度上也起到了扩充特征的作用。它的值只有 0 和 1，不同的类型存储在垂直的空间。
  - 缺点：当类别的数量很多时，特征空间会变得非常大。在这种情况下，一般可以用 PCA 来减少维度。而且 one hot encoding+PCA 这种组合在实际中也非常有用。

  ```
  from sklearn.preprocessing import OneHotEncoder
  enc = OneHotEncoder()
  enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])    # fit来学习编码 enc.transform([[0, 1, 3]]).toarray()    # 进行编码
  # 输出：array([[ 1., 0., 0., 1., 0., 0., 0., 0., 1.]])
  ```

  ```
  import pandas as pd
  import numpy as np

  sex_list = ['MALE', 'FEMALE', np.NaN, 'FEMALE', ]
  df = pd.DataFrame({'SEX': sex_list})
  df = pd.get_dummies(df['SEX'],prefix='IS_SEX')
  ```

- LabelBinarizer(标签二值化):功能与 OneHotEncoder 一样，但是 OneHotEncode 只能对数值型变量二值化，无法直接对字符串型的类别变量编码，而 LabelBinarizer 可以直接对字符型变量二值化。

  ```
  from sklearn.preprocessing import LabelBinarizer
  lb = LabelBinarizer()
  lb.fit([1, 2, 6, 4, 2])

  print(lb.classes_)
  # 输出 array([1, 2, 4, 6])

  print(lb.transform([1, 6]))
  # 输出 array([[1, 0, 0, 0],
              [0, 0, 0, 1]])

  print(lb.fit_transform(['yes', 'no', 'no', 'yes']))
  # 输出 array([[1],
              [0],
              [0],
              [1]])
  ```

- MultiLabelBinarizer(多标签二值化):用于 label encoding，生成一个(n_examples \* n_classes)大小的 0~1 矩阵，每个样本可能对应多个 label。

  - 适用情况

    - 每个特征中有多个文本单词；用户兴趣特征（如特征值：”健身 电影 音乐”）适
    - 多分类类别值编码的情况。电影分类标签中（如：[action, horror]和[romance, commedy]）需要先进行多标签二值化

    ```
    from sklearn.preprocessing import MultiLabelBinarizer
    mlb = MultiLabelBinarizer()
    print(mlb.fit_transform([(1, 2), (3,)]))
    # 输出
    array([[1, 1, 0],
        [0, 0, 1]])

    print(mlb.classes_)
    # 输出：array([1, 2, 3])

    print(mlb.fit_transform([{'sci-fi', 'thriller'}, {'comedy'}]))
    # 输出：array([[0, 1, 1],
        [1, 0, 0]])

    print(list(mlb.classes_))
    # 输出：['comedy', 'sci-fi', 'thriller']
    ```

- MeanEncoding(平均数编码):针对高基数类别特征的有监督编码。当一个类别特征列包括了极多不同类别时（如家庭地址，动辄上万）时，可以采用。在贝叶斯的架构下，利用所要预测的应变量（target variable），有监督地确定最适合这个定性特征的编码方式。
  - 优点: 和独热编码相比，节省内存、减少算法计算时间、有效增强模型表现。
  ```
  MeanEnocodeFeature = ['item_city_id','item_brand_id'] #声明需要平均数编码的特征
  ME = MeanEncoder(MeanEnocodeFeature) #声明平均数编码的类
  trans_train = ME.fit_transform(X,y)#对训练数据集的X和y进行拟合
  test_trans = ME.transform(X_test)#对测试集进行编码
  ```
- HashEncoding(哈希编码):对固定长度的数组执行“OneHot 编码”
  - 优点：能优雅地处理新变量，避免极为稀疏的数据
  - 缺点：
    - 可能会引起碰撞，可以使用不同的 hash 算法得到不同的编码值然后 concat 到一起尽量避免碰撞的发生
    - 碰撞 collisions 通常会降低结果，但可能会改善结果（增强泛化性能）

## 1.3 特征构建

特征构建需要很强的洞察力和分析能力，要求我们能够从原始数据中找出一些具有物理意义的特征。对于表格数据，特征构建意味着将特征进行混合或组合以得到新的特征，或通过对特征进行分解或切分来构造新的特征；对于文本数据，特征够自己按意味着设计出针对特定问题的文本指标；对于图像数据，这意味着自动过滤，得到相关的结构。

特征构造是一个非常耗时的过程，因为每个新的特征通常需要几步才能构造，特别是当使用多张表的信息时。我们可以将特征构造的操作分为两类：“转换”和“聚合”。

### a) 时间特征构建

对于时间型数据来说，即可以把它转换成连续值，也可以转换成离散值。时间特征主要有两大类：

- 从时间变量提取出来的特征
  - 如果每条数据为一条训练样本，时间变量提取出来的特征可以直接作为训练样本的特征使用
  - 如果每条数据不是一条训练样本，时间变量提取出来的特征需要进行二次加工（聚合操作）才能作为训练样本的特征使用。
- 对时间变量进行条件过滤，然后再对其他变量进行聚合操作所产生的特征
  - 主要是针对类似交易流水这样的数据，从用户角度进行建模时，每个用户都有不定数量的数据，因此需要对数据进行聚合操作来为每个用户构造训练特征。而包含时间的数据，可以先使用时间进行条件过滤，过滤后再构造聚合特征。

时间序列数据可以从带有时间的流水数据统计得到，实际应用中可以分别从带有时间的流水数据以及时间序列数据中构造特征，这些特征可以同时作为模型输入特征，比如美团的商家销售量预测中，每个商家的交易流水经过加工后可以得到每个商家每天的销售量，这个就是时间序列数据。

- 连续值时间特征
  - 持续时间: 比如单页浏览时长
  - 间隔时间
    - 上次购买、点击离现在的时长
    - 产品上线到现在经过的时长
- 离散值时间特征

  - 时间特征拆解

    - 年、月、日、时、分、秒
    - 一天的第几分钟
    - 星期几
    - 一年的中第几天
    - 一年中的的第几周
    - 一天中的哪个个时间段：凌晨、早晨、上午、中午、下午、傍晚、晚上、深夜
    - 一年中的哪个季度

    ```
    from sklearn.base import TransformerMixin
    class CustomTimeTransformer(TransformerMixin):
        def __init__(self, time_col='timestamp', period_dict=None, season_dict=None):
            self.time_col = time_col
            self.period_dict = period_dict
            self.season_dict = season_dict

        def transform(self, df):
            data = df.copy()
            data[self.time_col] = data[self.time_col].apply(lambda x: pd.Timestamp(x))
            data['year'] = data[self.time_col].apply(lambda x: x.year)
            data['mth'] = data[self.time_col].apply(lambda x: x.month)
            data['day'] = data[self.time_col].apply(lambda x: x.day)
            data['hour'] = data[self.time_col].apply(lambda x: x.hour)
            data['minute'] = data[self.time_col].apply(lambda x: x.minute)
            data['second'] = data[self.time_col].apply(lambda x: x.second)
            data['minute_of_day'] = data[self.time_col].apply(lambda x: x.minute + x.hour * 60)
            data['week'] = data[self.time_col].apply(lambda x: x.dayofweek)
            data['day_of_year'] = data[self.time_col].apply(lambda x: x.dayofyear)
            data['week_of_year'] = data[self.time_col].apply(lambda x: x.week)
            if self.period_dict:
                data['period'] = data['hour'].map(period_dict)
            if self.season_dict:
                data['season'] = data['mth'].map(season_dict)
            return data

        def fit(self, *_):
            return self

    # 一天中哪个时间段：凌晨、早晨、上午、中午、下午、傍晚、晚上、深夜；
    period_dict ={
        23: '深夜', 0: '深夜', 1: '深夜',
        2: '凌晨', 3: '凌晨', 4: '凌晨',
        5: '早晨', 6: '早晨', 7: '早晨',
        8: '上午', 9: '上午', 10: '上午', 11: '上午',
        12: '中午', 13: '中午',
        14: '下午', 15: '下午', 16: '下午', 17: '下午',
        18: '傍晚',
        19: '晚上', 20: '晚上', 21: '晚上', 22: '晚上',
    }

    # 一年中的哪个季度
    season_dict = {
        1: '春季', 2: '春季', 3: '春季',
        4: '夏季', 5: '夏季', 6: '夏季',
        7: '秋季', 8: '秋季', 9: '秋季',
        10: '冬季', 11: '冬季', 12: '冬季',
    }
    date_time_str_list = [
        '2019-01-01 01:22:26', '2019-02-02 04:34:52', '2019-03-03 06:16:40',
        '2019-04-04 08:11:38', '2019-05-05 10:52:39', '2019-06-06 12:06:25',
        '2019-07-07 14:05:25', '2019-08-08 16:51:33', '2019-09-09 18:28:28',
        '2019-10-10 20:55:12', '2019-11-11 22:55:12', '2019-12-12 00:55:12',
    ]
    df = pd.DataFrame({'timestamp': date_time_str_list})
    timeTransformer = CustomTimeTransformer(period_dict=period_dict, season_dict=season_dict)
    timeTransformer.fit_transform(df)
    ```

  - 时间特征判断

    - 是否闰年
    - 是否月初、月末
    - 是否季节初、季节末
    - 是否年初、年末
    - 是否周末
    - 是否公共假期
    - 是否营业时间
    - 两个时间间隔之间是否包含节假日/特殊日期

    ```
    class CustomTimeBooleanTransformer(TransformerMixin):
        def __init__(self, time_col='timestamp', public_vacations=[]):
            self.time_col = time_col
            self.public_vacations = public_vacations

        def transform(self, df):
            data = df.copy()
            data[self.time_col] = data[self.time_col].apply(lambda x: pd.Timestamp(x))
            data['is_leap_year'] = data[self.time_col].apply(lambda x: x.is_leap_year)
            data['is_mth_start'] = data[self.time_col].apply(lambda x: x.is_month_start)
            data['is_mth_end'] = data[self.time_col].apply(lambda x: x.is_month_end)
            data['is_quarter_start'] = data[self.time_col].apply(lambda x: x.is_quarter_start)
            data['is_quarter_end'] = data[self.time_col].apply(lambda x: x.is_quarter_end)
            data['is_year_start'] = data[self.time_col].apply(lambda x: x.is_year_start)
            data['is_year_end'] = data[self.time_col].apply(lambda x: x.is_year_end)
            data['is_weekend'] = data[self.time_col].apply(lambda x: True if x.dayofweek in [5, 6] else False)

            data['is_working'] = False
            data['tmp_hour'] = data[self.time_col].apply(lambda x: x.hour)
            data.loc[((data['tmp_hour'] >= 8) & (data['tmp_hour'] < 22)), 'is_working'] = True
            data.drop(['tmp_hour'], axis=1, inplace=True)

            if self.public_vacations:
                data['is_public_holiday'] = data[self.time_col].apply(
                    lambda x: True if x.strftime('%Y%m%d') in self.public_vacations else False)
            return data

        def fit(self, *_):
            return self

    # 构造时间数据
    date_time_str_list = [
        '2010-01-01 01:22:26', '2011-02-03 04:34:52', '2012-03-05 06:16:40',
        '2013-04-07 08:11:38', '2014-05-09 10:52:39', '2015-06-11 12:06:25',
        '2016-07-13 14:05:25', '2017-08-15 16:51:33', '2018-09-17 18:28:28',
        '2019-10-07 20:55:12', '2020-11-23 22:55:12', '2021-12-25 00:55:12',
        '2022-12-27 02:55:12', '2023-12-29 03:55:12', '2024-12-31 05:55:12',
    ]

    # 是否公共假期
    public_vacation_list = [
        '20190101', '20190102', '20190204', '20190205', '20190206',
        '20190207', '20190208', '20190209', '20190210', '20190405',
        '20190406', '20190407', '20190501', '20190502', '20190503',
        '20190504', '20190607', '20190608', '20190609', '20190913',
        '20190914', '20190915', '20191001', '20191002', '20191003',
        '20191004', '20191005', '20191006', '20191007',
    ] # 此处未罗列所有公共假期
    model = CustomTimeBooleanTransformer(public_vacations=public_vacation_list)
    model.fit_transform(df)
    ```

- 结合时间维度的聚合特征
  - 首日聚合特征
    - 注册首日投资总金额
    - 注册首日页面访问时长
    - 注册首日总惦记次数
  - 最近时间聚合特征
    - 最近 N 天 APP 登录天数
    - 最近一个月的购买金额
    - 最近购物至今天数
  - 区间内聚合特征
    - 2018 年至 2019 年的总购买金额
    - 每天下午的平均客流量
    - 在某公司工作期间加班的天数

### b) 时间序列特征构造

时间序列不仅包含一维时间变量，还有一维其他变量，如股票价格、天气温度、降雨量、订单量等。时间序列分析的主要目的是基于历史数据来预测未来信息。对于时间序列，我们关心的是长期的变动趋势、周期性的变动（如季节性变动）以及不规则的变动。

按固定时间长度把时间序列划分成多个时间窗，然后构造每个时间窗的特征。

- 时间序列聚合特征：按固定时间长度把时间序列划分成多个时间窗，然后使用聚合操作构造每个时间窗的特征

  - 均值：历史销售量平均值、最近 N 天销售量平均值
  - 最大值：历史销售量最小值、最近 N 天销售量最小值
  - 最小值：历史销售量最大值、最近 N 天销售量最大值
  - 扩散值：分布的扩散性，如标准差、平均绝对偏差或四分位差，可以反映测量的整体变化趋势
  - 离散系数值：是策略数据离散程度的相对统计量，主要用于比较不同样本数据的离散程度
  - 分布性：

  ```
  import pandas as pd
  # 加载洗发水销售数据集
  df = pd.read_csv('shampoo-sales.csv')
  df.dropna(inplace=True)
  df.rename(columns={'Sales of shampoo over a three year period': 'value'}, inplace=True)

  # 平均值
  mean_v = df['value'].mean()
  print('mean: {}'.format(mean_v))

  # 最小值
  min_v = df['value'].min()
  print('min: {}'.format(min_v))

  # 最大值
  max_v = df['value'].max()
  print('max: {}'.format(max_v))

  # 扩散值：标准差
  std_v = df['value'].std()
  print('std: {}'.format(std_v))

  # 扩散值：平均绝对偏差
  mad_v = df['value'].mad()
  print('mad: {}'.format(mad_v))

  # 扩散值：四分位差
  q1 = df['value'].quantile(q=0.25)
  q3 = df['value'].quantile(q=0.75)
  irq = q3 - q1
  print('q1={}, q3={}, irq={}'.format(q1, q3, irq))

  # 离散系数
  variation_v = std_v/mean_v
  print('variation: {}'.format(variation_v))

  # 分布性：偏态系数
  skew_v = df['value'].skew()
  print('skew: {}'.format(skew_v))
  # 分布性：峰态系数
  kurt_v = df['value'].kurt()
  print('kurt: {}'.format(kurt_v))

  # 输出：
  mean: 312.59999999999997
  min: 119.3
  max: 682.0
  std: 148.93716412347473
  mad: 119.66666666666667
  q1=192.45000000000002, q3=411.1, irq=218.65
  variation: 0.47644646232717447
  skew: 0.8945388528534595
  kurt: 0.11622821118738624
  ```

- 时间序列历史特征
  - 前一(n)个窗口的取值
  - 周期性时间序列前一(n)周期的前一(n)个窗口的取值
  ```
  import pandas as pd
  # 加载洗发水销售数据集
  df = pd.read_csv('shampoo-sales.csv')
  df.dropna(inplace=True)
  df.rename(columns={'Sales of shampoo over a three year period': 'value'}, inplace=True)
  ```


    df['-1day'] = df['value'].shift(1)
    df['-2day'] = df['value'].shift(2)
    df['-3day'] = df['value'].shift(3)

    df['-1period'] = df['value'].shift(1*12)
    df['-2period'] = df['value'].shift(2*12)

    display(df.head(60))
    ```

- 时间序列复合特征

  - 趋势特征:

    - User 一天对 Item 的行为次数/User 三天对 Item 的行为次数的均值，表示短期 User 对 Item 的热度趋势
    - 三天 User 对 Item 的行为次数的均值/七天 User 对 Item 的行为次数的均值表示中期 User 对 Item 的活跃度的变化情况
    - 七天 User 对 Item 的行为次数的均值/ 两周 User 对 Item 的行为次数的均值表示“长期”（相对）User 对 Item 的活跃度的变化情况

      ```
      import pandas as pd
      # 加载洗发水销售数据集
      df = pd.read_csv('shampoo-sales.csv')
      df.dropna(inplace=True)
      df.rename(columns={'Sales of shampoo over a three year period': 'value'}, inplace=True)

      df['last 3 day mean'] = (df['value'].shift(1) + df['value'].shift(2) + df['value'].shift(3))/3
      df['最近3天趋势'] = df['value'].shift(1)/df['last 3 day mean']
      display(df.head(60))
      ```

  - 窗口差异值特征：一个窗口到下一个窗口的差异。例子：商店销售量时间序列中，昨天的销售量与前天销售量的差值

  ```
  import pandas as pd
    # 加载洗发水销售数据集
    df = pd.read_csv('shampoo-sales.csv')
    df.dropna(inplace=True)
    df.rename(columns={'Sales of shampoo over a three year period': 'value'}, inplace=True)

    df['最近两月销量差异值'] = df['value'].shift(1) - df['value'].shift(2)
    display(df.head(60))

  ```

  - 自相关特征：原时间序列与自身左移一个时间空格（没有重叠的部分被移除）的时间序列相关联。

    ```
    import statsmodels.tsa.api as smt
    import pandas as pd
    # 加载洗发水销售数据集
    df = pd.read_csv('shampoo-sales.csv')
    df.dropna(inplace=True)
    df.rename(columns={'Sales of shampoo over a three year period': 'value'}, inplace=True)

    print('滞后数为1的自相关系数：{}'.format(df['value'].autocorr(1)))
    print('滞后数为2的自相关系数：{}'.format(df['value'].autocorr(2)))
    # 输出：
    滞后数为1的自相关系数：0.7194822398024308
    滞后数为2的自相关系数：0.8507433352850972
    ```

### c) 空间特征构建

- 按经纬度对空间进行划分的特征
  - 划分流程
    - 对经纬度进行特征分桶，得到类别特征
    - 使用笛卡尔乘积生成空间划分后的特征
    - 对每块子空间进行编码，得到类别特征
- 使用坐标拾取系统获取行政区域信息
  - 省份 ID/名字
  - 城市 ID/名字
  - 市辖区 ID/名字
  - 街道 ID/名字
- 结合其他地址计算距离
  - 欧式距离
  - 球面距离
  - 曼哈顿距离
  - 真实距离

### d) 文本特征构建

长文本特征的特征提取流程

- Step 1：文本清洗，去除 HTML 标记、去停用词、转小写、去除噪声、统一编码等。
- Step 2：分词。
- Step 3：提取特征

文本特征构建类别有：

- 文本统计特征
  - 文本长度；
  - 单词、数字、字母、大小写单词、大小写字母、标点符号、特殊字符个数；
  - 数字、字母、特殊字符占比；
  - 名词、动词个数；
  - 适用范围：所有文本特征
- 字典映射：有序特征的映射，使用的方法是先构建一个映射字典 mapping，再用 pandas 的 map() 或者 replace() 函数进行映射转换
  - 适用范围：只有一个词语的有序特征
- 标签二值化(LabelBinarizer)：功能与 OneHotEncoder 一样，但是 OneHotEncode 只能对数值型变量二值化，无法直接对字符串型的类别变量编码，而 LabelBinarizer 可以直接对字符型变量二值化。
  - 适用范围：只有一个词语或者包含多个词语的特征。
- 多标签二值化(MultiLabelBinarizer)：把有多个单词的文本转换 01 特征向量，转换后的结果可能会有多个为 1 的值。
  - 适用范围：包含多个词语的特征。
- 特征哈希(Feature Hashing)

  - 适用范围：大数据集文本。

  ```
  from sklearn.feature_extraction.text import HashingVectorizer
  corpus = [
      'This is the first document.',
      'This is the second second document.',
      'And the third one.',
      'Is this the first document?',
  ]
  hv = HashingVectorizer(n_features=10)
  X = hv.fit_transform(corpus)

  print('shape={}'.format(X.shape))
  # 输出：shape=(4, 10)
  print(X.toarray())
  # 输出：
  [[ 0.  0.   0.  0.  0.    0.         -0.57735027  0.57735027  -0.57735027 0.]
  [ 0.  0.   0.  0.  0.    0.81649658 0.           0.40824829  -0.40824829 0.]
  [ 0.  0.5  0.  0.  -0.5  -0.5       0.           0.          -0.5        0.]
  [ 0.  0.   0.  0.  0.    0.         -0.57735027  0.57735027  -0.57735027 0.]]
  ```

- 词袋模型(BOW):假设我们不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重。而权重与词在文本中出现的频率有关。

  - 适用范围：长文本特征。

  ```
  from sklearn.feature_extraction.text import CountVectorizer
  corpus = [
      'This is the first document.',
      'This is the second second document.',
      'And the third one.',
      'Is this the first document?',
  ]
  vectorizer = CountVectorizer(min_df=1)
  X = vectorizer.fit_transform(corpus)
  print(X.toarray())
  # 输出：
  [[0 1 1 1 0 0 1 0 1]
  [0 1 0 1 0 2 1 0 1]
  [1 0 0 0 1 0 1 1 0]
  [0 1 1 1 0 0 1 0 1]]

  feature_name = vectorizer.get_feature_names()
  print(feature_name)
  # 输出：['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
  ```

- TF-IDF

  - 适用范围：长文本特征。
  - 实际使用时，特别是当文本内容比较长时，可以只保留权重值 top n 的向量
    - 使用 Top n 个单词的 TF-IDF 权重值作为特征值；
    - 提取 Top n 个单词，然后使用多标签二值化、词袋模型和词嵌入向量等相关方法来构造特征；

  ```
  from sklearn.feature_extraction.text import TfidfVectorizer
  # 构造数据集
  corpus = [
      'This is the first document.',
      'This is the second second document.',
      'And the third one.',
      'Is this the first document?',
  ]
  tfidf = TfidfVectorizer()
  X = tfidf.fit_transform(corpus)
  print(X.toarray())
  # 输出：
  [[0.         0.43877674 0.54197657 0.43877674 0.         0.         0.35872874 0.         0.43877674]
  [0.         0.27230147 0.         0.27230147 0.         0.85322574 0.22262429 0.         0.27230147]
  [0.55280532 0.         0.         0.         0.55280532 0.         0.28847675 0.55280532 0.        ]
  [0.         0.43877674 0.54197657 0.43877674 0.         0.         0.35872874 0.         0.43877674]]

  # feature_name = tfidf.get_feature_names()
  print(feature_name)
  # 输出：['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
  ```

- LDA 主题模型:一种主题模型，它可以将文档集中每篇文档的主题以概率分布的形式给出，从而通过分析一些文档抽取出它们的主题（分布）出来后，便可以根据主题（分布）进行主题聚类或文本分类。同时，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的关系。

  - 适用范围：长文本特征。

  ```
  import pandas as pd
  import jieba
  from sklearn.feature_extraction.text import CountVectorizer
  from sklearn.decomposition import LatentDirichletAllocation

  article_list = [
      '沙瑞金赞叹易学习的胸怀，是金山的百姓有福，可是这件事对李达康的触动很大。易学习又回忆起他们三人分开的前一晚，大家一起喝酒话别，易学习被降职到道口县当县长，王大路下海经商，李达康连连赔礼道歉，觉得对不起大家，他最对不起的是王大路，就和易学习一起给王大路凑了5万块钱，王大路自己东挪西撮了5万块，开始下海经商。没想到后来王大路竟然做得风生水起。沙瑞金觉得他们三人，在困难时期还能以沫相助，很不容易。',
      '沙瑞金向毛娅打听他们家在京州的别墅，毛娅笑着说，王大路事业有成之后，要给欧阳菁和她公司的股权，她们没有要，王大路就在京州帝豪园买了三套别墅，可是李达康和易学习都不要，这些房子都在王大路的名下，欧阳菁好像去住过，毛娅不想去，她觉得房子太大很浪费，自己家住得就很踏实。',
      '347年（永和三年）三月，桓温兵至彭模（今四川彭山东南），留下参军周楚、孙盛看守辎重，自己亲率步兵直攻成都。同月，成汉将领李福袭击彭模，结果被孙盛等人击退；而桓温三战三胜，一直逼近成都。',
  ]
  df = pd.DataFrame({'文章': article_list})
  df['文章分词'] = df['文章'].apply(lambda x: ' '.join(jieba.cut(x)))

  # 去停用词
  stopwords_filename = 'data/HIT_stopwords.txt'
  def get_stopwords_list(filepath):
      stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]
      return stopwords

  stopwords_list = get_stopwords_list(stopwords_filename)

  # 把词转化为词频向量，注意由于LDA是基于词频统计的，因此一般不用TF-IDF来做文档特征
  cnt_vector = CountVectorizer(stop_words=stopwords_list)
  cnt_tf = cnt_vector.fit_transform(df['文章分词'])
  print(cnt_tf)

  # LDA建模
  lda = LatentDirichletAllocation(n_topics=2,
                                  learning_offset=50.,
                                  random_state=0)
  docres = lda.fit_transform(cnt_tf)
  # 文档主题的分布
  print(docres)

  # 词列表
  print(cnt_vector.get_feature_names())
  # 主题和词的分布
  print(lda.components_)
  ```

- 词嵌入向量(Word Embedding):一种单词的表示形式，它允许意义相似的单词具有类似的表示形式

  - 适用范围：所有文本特征
  - 只有一个词语的特征

    ```
    from mitie import total_word_feature_extractor
    import pandas as pd
    import numpy as np
    # 构造数据集
    df = pd.DataFrame({'职业': ['老师', '程序员', '警察', '销售', '销售', ]})

    # 加载语言模型，此处使用mitie的中文语言模型，也可以使用其他语言模型
    twfe = total_word_feature_extractor('./total_word_feature_extractor_zh.dat')
    # 把词语转换成embedding向量
    embedding_array = np.array(list(df['职业'].apply(lambda x :twfe.get_feature_vector(x))))
    print('shape={}'.format(embedding_array.shape))
    # 输出：shape=(5, 271)
    print(embedding_array)
    # 输出：
    [[ 0.          5.94052029 -0.55087203 ...  0.98895782 -0.45328307 -2.60366035]
    [ 0.          5.73116112 -0.10071201 ...  0.08086307 -2.83375955 -1.30700874]
    [ 0.          5.46504879  0.31765267 ... -0.67403883 -0.32645369 -0.19468342]
    [ 0.          5.64302588 -0.60524434 ... -1.24884379 -1.201828 0.77896601]
    [ 0.          5.64302588 -0.60524434 ... -1.24884379 -1.201828 0.77896601]]
    ```

    - 包含多个单词的特征或者长文本:把每个多个单词对应的 embedding 向量求和/求平均值作为最终的特征向量

      ```
      import pandas as pd
      import numpy as np
      from mitie import total_word_feature_extractor

      # 构造数据集
      df = pd.DataFrame({'兴趣': ['健身 电影 音乐', '电影 音乐', '电影 篮球', '篮球 羽毛球', ]})
      df['兴趣列表'] = df['兴趣'].apply(lambda x: x.split())
      display(df.head())
      # 输出：
          兴趣            兴趣列表
      0    健身 电影 音乐    [健身, 电影, 音乐]
      1    电影 音乐        [电影, 音乐]
      2    电影 篮球        [电影, 篮球]
      3    篮球 羽毛球      [篮球, 羽毛球]

      # 加载Embedding模型
      mitie_model_filename = 'data/total_word_feature_extractor_zh.dat'
      twfe = total_word_feature_extractor(mitie_model_filename)

      # 使用加法模型获取特征向量
      add_embeding_array = np.array(list(df['兴趣列表'].apply(
          lambda x : np.sum([twfe.get_feature_vector(w) for w in x], axis=0))))
      print(add_embeding_array)
      # 输出：
      [[ 0.         17.69472837  0.99066588 ...  1.80016923 -3.46660849 1.61377704]
      [ 0.         11.59600973  1.27471587 ...  1.51293385 -0.89438912 2.50961554]
      [ 0.         11.41014719  0.71066754 ...  1.70346397  2.00526482 1.10128853]
      [ 0.         10.79806709 -0.79963933 ...  1.38594878  3.12447354 0.0570921 ]]

      # 使用平均值模型获取特征向量
      mean_embeding_array = np.array(list(df['兴趣列表'].apply(
          lambda x : np.mean([twfe.get_feature_vector(w) for w in x], axis=0))))
      print(mean_embeding_array)
      # 输出：
      [[ 0.          5.89824279  0.33022196 ...  0.60005641 -1.15553616 0.53792568]
      [ 0.          5.79800487  0.63735794 ...  0.75646693 -0.44719456 1.25480777]
      [ 0.          5.7050736   0.35533377 ...  0.85173199  1.00263241 0.55064426]
      [ 0.          5.39903355 -0.39981966 ...  0.69297439  1.56223677 0.02854605]]
      ```

    - 长文本

      ```
      import pandas as pd
      import jieba
      import numpy as np
      from mitie import total_word_feature_extractor

      article_list = [
          '沙瑞金赞叹易学习的胸怀，是金山的百姓有福，可是这件事对李达康的触动很大。易学习又回忆起他们三人分开的前一晚，大家一起喝酒话别，易学习被降职到道口县当县长，王大路下海经商，李达康连连赔礼道歉，觉得对不起大家，他最对不起的是王大路，就和易学习一起给王大路凑了5万块钱，王大路自己东挪西撮了5万块，开始下海经商。没想到后来王大路竟然做得风生水起。沙瑞金觉得他们三人，在困难时期还能以沫相助，很不容易。',
          '沙瑞金向毛娅打听他们家在京州的别墅，毛娅笑着说，王大路事业有成之后，要给欧阳菁和她公司的股权，她们没有要，王大路就在京州帝豪园买了三套别墅，可是李达康和易学习都不要，这些房子都在王大路的名下，欧阳菁好像去住过，毛娅不想去，她觉得房子太大很浪费，自己家住得就很踏实。',
          '347年（永和三年）三月，桓温兵至彭模（今四川彭山东南），留下参军周楚、孙盛看守辎重，自己亲率步兵直攻成都。同月，成汉将领李福袭击彭模，结果被孙盛等人击退；而桓温三战三胜，一直逼近成都。',
      ]
      df = pd.DataFrame({'文章': article_list})
      df['文章分词'] = df['文章'].apply(lambda x: ' '.join(jieba.cut(x)))

      # 去停用词
      stopwords_filename = 'data/HIT_stopwords.txt'
      def get_stopwords_list(filepath):
          stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]
          return stopwords

      stopwords_list = get_stopwords_list(stopwords_filename)
      def remove_stopwords_apply(article):
          article_list = article.split(' ')
          w_list = []
          for w in article_list:
              if w not in stopwords_list and len(w) > 0:
                  w_list.append(w)

          return ' '.join(w for w in w_list)

      df['文章分词去停用词'] = df['文章分词'].apply(remove_stopwords_apply)
      display(df.head())
      ```


        //# 加载Embedding模型
        mitie_model_filename = 'data/total_word_feature_extractor_zh.dat'
        twfe = total_word_feature_extractor(mitie_model_filename)

        //# 使用加法模型获取特征向量
        add_embeding_array = np.array(list(df['文章分词去停用词'].apply(
            lambda x : np.sum([twfe.get_feature_vector(w) for w in x], axis=0))))
        print('加法模型结果：\n{}'.format(add_embeding_array))

        //# 使用平均值模型获取特征向量
        mean_embeding_array = np.array(list(df['文章分词去停用词'].apply(
            lambda x : np.mean([twfe.get_feature_vector(w) for w in x], axis=0))))
        print('平均模型结果：\n{}'.format(mean_embeding_array))
        ```

### e) GBDT 特征构造

GBDT 是一种常用的非线性模型，基于集成学习中 boosting 的思想，由于 GBDT 本身可以发现多种有区分性的特征以及特征组合，决策树的路径可以直接作为 LR 输入特征使用，省去了人工寻找特征、特征组合的步骤。所以可以将 GBDT 的叶子结点输出，作为 LR 的输入。
GBDT 特征构造的关键点在于:

- 采用 ensemble 决策树而非单颗树
- 采用 GBDT 而非 RF：GBDT 前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前 N 颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理。

GBDT 的优点：GBDT 可以自动进行特征组合和离散化，LR 可以有效利用海量 id 类离散特征，保持信息的完整性。

GBDT 的缺点: LR 预测的时候需要等待 GBDT 的输出，一方面 GBDT 在线预测慢于单 LR，另一方面 GBDT 目前不支持在线算法，只能以离线方式进行更新。
代码实现:

```
import numpy as np
import random
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score

# 生成随机数据
np.random.seed(10)
X, Y = make_classification(n_samples=1000, n_features=30)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=233, test_size=0.4)

# 训练GBDT模型
gbdt = GradientBoostingClassifier(n_estimators=10)
gbdt.fit(X_train, Y_train)
# 对GBDT预测结果进行onehot编码
onehot = OneHotEncoder()
onehot.fit(gbdt.apply(X_train)[:, :, 0])
# 训练LR模型
lr = LogisticRegression()
lr.fit(onehot.transform(gbdt.apply(X_train)[:, :, 0]), Y_train)
# 测试集预测
Y_pred = lr.predict_proba(onehot.transform(gbdt.apply(X_test)[:, :, 0]))[:, 1]

fpr, tpr, _ = roc_curve(Y_test, Y_pred)
auc = roc_auc_score(Y_test, Y_pred)
print('GradientBoosting + LogisticRegression: ', auc)
```

### f) 聚类特征构造

聚类算法构造特征流程：

- Step 1：从预处理后的特征集中选择一个或多个特征；当只选择一个数值型特征时，聚类算法构造特征相当于使用聚类算法进行特征分箱
- Step 2：选择适合聚类算法对已选择的特征进行聚类，并输出聚类类标结果；
- Step 3：对聚类类标结果进行编码；类似 sklearn 这种机器学习库，一般聚类类标结果为一个数值，但实际上这个数值并没有大小之分，所以一般需要进行特征编码

代码实现:

```
import pandas as pd
import jieba
import numpy as np
from mitie import total_word_feature_extractor
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder
# 构造特征集
hobby = [
    '健身', '电影', '音乐', '读书', '历史',
    '篮球', '羽毛球', '足球',
]
df = pd.DataFrame({'兴趣': hobby})
display(df.head(20))

# 加载Embedding模型
mitie_model_filename = 'total_word_feature_extractor_zh.dat'
twfe = total_word_feature_extractor(mitie_model_filename)

# 把词语转换成embedding向量
embeding_array = np.array(list(df['兴趣'].apply(
    lambda w: twfe.get_feature_vector(w))))

# k-mean距离
kmeans = KMeans(n_clusters=2, random_state=0).fit(embeding_array)
kmean_label = kmeans.labels_
print('kmeans.labels_:{}'.format(kmean_label))
# 输出：kmeans.labels_:[1 1 1 1 1 0 0 0]
kmean_label = kmean_label.reshape(-1, 1)
print('kmean_label shape={}'.format(kmean_label.shape))
# 输出：kmean_label shape=(8, 1)

# 特征编码
enc = OneHotEncoder()
onehot_code = enc.fit_transform(kmean_label)
print(onehot_code.toarray())
```

### g) 聚合特征构造

- 分组统计特征
  - 中位数: df.groupby(['C1']).agg({'N1': 'median'})
  - 平均数: df.groupby(['C1']).agg({'N1': 'mean'})
  - 众数: df.groupby(['C1'])['N1'].agg(lambda x: stats.mode(x)[0][0])
  - 最小值: df.groupby(['C1']).agg({'N1': 'min'})
  - 最大值: df.groupby(['C1']).agg({'N1': 'max'})
  - 标准差: df.groupby(['C1']).agg({'N1': 'std'})
  - 方差: df.groupby(['C1']).agg({'N1': 'var'})
  - 频数: df.groupby(['C1']).agg({'C2': 'count'})
- 统计频数构造特征：df['C1'].count()
- 分组统计和基础特征工程方法结合
  - 中位数分组和线性组合结合
  - 均值分组和线性组合结合

### h) 转换特征构造

- 单列特征加/减/乘/除一个常数: df['Feature'] = df['Feature'] + n

  - 单列特征单调变换:
    - df['Feature'] = df['Feature']\*\*2
    - df['Feature'] = np.log(df['Feature'])
  - 线性组合
    - df['Feature'] = df['A'] \* df['B']
    - df['Feature'] = df['A'] _ df['B'] _ df['C'] _ df['D'] _ df['E']
    - df['Feature'] = df['A'] \* df['A']
  - 多项式特征

    - 当两个变量各自与 y 的关系并不强时候，把它们结合成为一个新的变量可能更会容易体现出它们与 y 的关系。特征 a 和特征 b 的多项式输出是：[1, a, b, a^2, ab, b^2]或者[1, a, b, ab]。
    - 代码实现:

      ```
      import numpy as np
      from sklearn.preprocessing import PolynomialFeatures
      X = np.arange(6).reshape(3, 2)
      print(X)
      # 输出：array([[0, 1],
                    [2, 3],
                    [4, 5]])

      # 设置多项式阶数为２
      poly = PolynomialFeatures(2)
      print(poly.fit_transform(X))
      # 输出：array([[ 1.,  0.,  1.,  0.,  0.,  1.],
                    [ 1.,  2.,  3.,  4.,  6.,  9.],
                    [ 1.,  4.,  5., 16., 20., 25.]])

      ＃默认的阶数是２，同时设置交互关系为true
      poly = PolynomialFeatures(interaction_only=True)
      print(poly.fit_transform(X))
      # 输出：array([[ 1.,  0.,  1.,  0.],
                    [ 1.,  2.,  3.,  6.],
                    [ 1.,  4.,  5., 20.]])
      ```

  - 比例特征: df['Feature'] = df['X1']/df['X2']
  - 绝对值特征: df['Feature'] = np.abs(df['Feature'])
  - 最大值特征: df['Feature'] = df.apply(lambda x: max(x['X1'], x['X2']), axis=1)
  - 最小值特征: df['Feature'] = df.apply(lambda x: min(x['X1'], x['X2']), axis=1)
  - 排名编码特征: df['num'] = df['X'].rank(ascending=0, method='dense')
  - 异或值特征: df['Feature'] = df.apply(lambda x: x['X1'] ^ x['X2'], axis=1)

### i) 笛卡尔积特征构造

笛卡尔乘积是指在数学中，两个集合 X 和 Y 的笛卡尓积（ Cartesian product ），又称直积，表示为 X×Y, 通过将单独的特征求笛卡尔乘积的方式来组合 2 个或更多个特征，从而构造出组合特征。最终获得的预测能力将远远超过任一特征单独的预测能力。
笛卡尔乘积组合特征方法一般应用于类别特征之间，连续值特征使用笛卡尔乘积组合特征时一般需要先进行离散化。

代码实现:

```
def cartesian_product_feature_crosses(df, feature1_name, feature2_name):
    feature1_df = pd.get_dummies(df[feature1_name], prefix=feature1_name)
    feature1_columns = feature1_df.columns

    feature2_df = pd.get_dummies(df[feature2_name], prefix=feature2_name)
    feature2_columns = feature2_df.columns

    combine_df = pd.concat([feature1_df, feature2_df], axis=1)

    crosses_feature_columns = []
    for feature1 in feature1_columns:
        for feature2 in feature2_columns:
            crosses_feature = '{}&{}'.format(feature1, feature2)
            crosses_feature_columns.append(crosses_feature)

            combine_df[crosses_feature] = combine_df[feature1] * combine_df[feature2]

    combine_df = combine_df.loc[:, crosses_feature_columns]
    return combine_df

combine_df = cartesian_product_feature_crosses(df, 'color', 'light')
display(combine_df.head())
```

### j) 遗传编程特征构造

遗传编程创造新特征是基于 genetic programming 的 symbolic regression（符号回归），具体的原理和实现可参考https://www.researchgate.net/publication/322795740_Symbolic_Regression。符号回归的具体实现方式是遗传算法（genetic algorithm）。一开始，一群天真而未经历选择的公式会被随机生成。此后的每一代中，最「合适」的公式们将被选中。随着迭代次数的增长，它们不断繁殖、变异、进化，从而不断逼近数据分布的真相。

目前，python 环境下最好用的基因编程库为 gplearn。虽然遗传编程 (GP) 可以用来执行非常广泛的任务，但是 gplearn 有目的地约束了符号回归问题。gplearn 这个库提供了解决的思路：

- 随机化生成大量特征组合方式，解决了没有先验知识，手工生成特征费时费力的问题。
- 通过遗传算法进行特征组合的迭代，而且这种迭代是有监督的迭代，存留的特征和 label 相关性是也来越高的，大量低相关特征组合会在迭代中被淘汰掉，用决策树算法做个类比的话，我们自己组合特征然后筛选，好比是后剪枝过程，遗传算法进行的则是预剪枝的方式。

把已有的特征进行组合转换，组合的方式（一元、二元、多元算子）可以由用户自行定义，也可以使用库中自带的函数（如加减乘除、min、max、三角函数、指数、对数）。组合的目的，是创造出和目标 y 值最“相关”的新特征。这种相关程度可以用 spearman 或者 pearson 的相关系数进行测量。spearman 多用于决策树（免疫单特征单调变换），pearson 多用于线性回归等其他算法。

代码实现:

```
# 数据集：波士顿数据集
# 训练Ridge模型
est = Ridge()
est.fit(boston.data[:300, :], boston.target[:300])
print(est.score(boston.data[300:, :], boston.target[300:]))
# 输出：0.759145222183

# 使用超过20代的2000人。选择最好的100个hall_of_fame，然后使用最不相关的10作为我们的新功能。因为我们使用线性模型作为估算器，所以这里使用默认值metric='pearson'。
function_set = ['add', 'sub', 'mul', 'div',
                'sqrt', 'log', 'abs', 'neg', 'inv',
                'max', 'min']
gp = SymbolicTransformer(generations=20, population_size=2000,
                         hall_of_fame=100, n_components=10,
                         function_set=function_set,
                         parsimony_coefficient=0.0005,
                         max_samples=0.9, verbose=1,
                         random_state=0, n_jobs=3)
gp.fit(boston.data[:300, :], boston.target[:300])
# 将新构造的特征拼接到原始数据上
gp_features = gp.transform(boston.data)
new_boston = np.hstack((boston.data, gp_features))
# 使用新的特征重新训练Ridge模型
est = Ridge()
est.fit(new_boston[:300, :], boston.target[:300])
print(est.score(new_boston[300:, :], boston.target[300:]))
# 输出：0.841750404385
```

### k) 自动化特征构造

目前，很多机器学习项目的模型选择开始转向自动化，而特征工程仍然主要以人工为主。自动化特征工程旨在通过从数据集中自动创建候选特征，且从中选择若干最佳特征进行训练的一种方式。自动化特征工程工具包有 Feature Tools 和 tsfresh。

Feature Tools 是执行自动化功能工程的框架。它擅长将时态和关系数据集转换为机器学习的特征矩阵。Feature Tools 使用一种称为深度特征合成（Deep Feature Synthesis，DFS）的算法，该算法遍历通过关系数据库的模式描述的关系路径，深度特征合成叠加多个转换和聚合操作，这在特征工具的词库中被称为特征基元，以便通过分布在多张表内的数据来构造新的特征。与机器学习中的大多数方法一样，这是建立在简单概念基础之上的复杂方法。深度特征只是叠加多个基元构造的一个特征，而 dfs 只是构造这些特征的过程的名称。深度特征的深度是构造这个特征所需的基元数量。

特征工具的前两个概念的是「实体」和「实体集」。一个实体就是一张表（或是 Pandas 中的一个 DataFrame（数据框））。一个实体集是一组表以及它们之间的关联。将一个实体集看成另一种 Python 数据结构，并带有自己的方法和属性。

## 1.4 特征选择

在实际应用中，经常会出现维度灾难问题。如果只选择所有特征中的部分特征构建模型，那么可以大大减少学习算法的运行时间，也可以增加模型的可解释性。
特征选择的原则: 尽可能小的特征子集，不显著降低分类精度、不影响分类分布以及特征子集应具有稳定、适应性强等特点。

有几点做特征选择的方法经验：

- 如果特征是分类变量，那么可以从 SelectKBest 开始，用卡方或者基于树的选择器来选择变量；
- 如果特征是定量变量，可以直接用线性模型和基于相关性的选择器来选择变量；
- 如果是二分类问题，可以考虑使用 SelectFromModel 和 SVC；
- 在进行特征选择前，还是需要做一下 EDA。

### a) 特征选择方法
  
#### 过滤法

先进行特征选择，然后去训练学习器，所以特征选择的过程与学习器无关。

- 主要思想: 对每一维特征“打分”，即给每一维的特征赋予权重，这样的权重就代表着该特征的重要性，然后依据权重排序。
- 优点：运行速度快，是一种非常流行的特征选择方法。
- 缺点：

  - 无法提供反馈，特征选择的标准/规范的制定是在特征搜索算法中完成，学习算法无法向特征搜索算法传递对特征的需求。
  - 可能处理某个特征时由于任意原因表示该特征不重要，但是该特征与其他特征结合起来则可能变得很重要。

- 主要方法:

  - 方差过滤

    - 介绍：一般用在特征选择前作为一个预处理的工作，即先去掉取值变化小的特征，然后再使用其他特征选择方法选择特征。  
      对于离散型变量，如果超过一定阈值的实例的特征值都是一样的，那么可以认为这个特征作用不大；对于连续型变量，需要将连续变量离散化后才能用
    - 实现：

      ```
      from sklearn.feature_selection import VarianceThreshold
      X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]
      sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
      sel.fit_transform(X)
      ＃array([[0, 1],
          [1, 0],
          [0, 0],
          [1, 1],
          [1, 0],
          [1, 1]])
      ```

  - 基于统计相关性的过滤

    - Chi-squared test（卡方检验）

      - 介绍：卡方检验是一种用途很广的计数资料的假设检验方法，由卡尔•皮尔逊提出。卡方值描述两个事件的独立性或者描述实际观察值与期望值的偏离程度。卡方值越大，表名实际观察值与期望值偏离越大，也说明两个事件的相互独立性越弱。
      - 原理: CHI 值(卡方值)用于衡量实际值与理论值的差异程度，除以 T 是为了避免不同观察值与不同期望之间产生的偏差因 T 的不同而差别太大
        $$CHI(x,y) = \chi^2(x,y) = \sum\frac{(A-T)^2}{T}$$
      - 实现流程：对于每个特征变量以及因变量，分别计算出 CHI(x1,y)、CHI(xn,y)，并按照 CHI 的值从大到小排序；选着合适的阈值，大于阈值的特征留下
      - 只适用于分类问题中离散型特征筛选，不能用于分类问题中连续型特征的筛选，也不能用于回归问题的特征筛选。
      - 代码实现
        ```
        from sklearn.feature_selection import SelectKBest ,chi2
        #选择相关性最高的前5个特征
        X_chi2 = SelectKBest(chi2, k=5).fit_transform(X, y)
        trainedforest = RandomForestClassifier(n_estimators=700).fit(X_chi2,y)
        X_chi2.shape
        输出：(27, 5)
        X_chi2。get_support()
        ```

    - ANOVA
      - 适用场景: 当目标变量是离散的，而自变量是连续变量时，可以使用 t 检验或 ANOVA 方差分析进行变量筛选
      - 代码实现:
        ```
        from sklearn.feature_selection import SelectKBest, f_classif
        X_anova = SelectKBest(f_classif, k=2).fit_transform(X, y)
        X_anova.get_support()
        ```
    - Information gain（信息增益）

      - 互信息(Mutual information)
        - 介绍: 如果变量不是独立的,那么我们可以通过考察联合概率分布与边缘概率分布乘积之间的 Kullback-Leibler 散度来判断它们是否“接近”于相互独立
        - 熵 H(Y)与条件熵 H(Y|X)之间的差称为互信息(ID3 决策树的特征选择规则):
          $$I(x,y) = H(x) - H(x|y) = H(y) - H(y|x)$$
        - 互信息可以评价定性自变量对定性因变量的相关性，但并不方便直接用于特征选择
          - 它不属于度量方式，也没有办法进行归一化，在不同的数据上的结果无法做比较
          - 只能用于离散型特征的选择，连续型特征需要先进行离散化才能用互信息进行特征选择，而互信息的结果对离散化的方式很敏感。
      - 最大信息系数方法(Maximal infromation coefficient)
        - 介绍: 最大信息数据首先寻找一种最优的离散方式，然后把互信息取值转换成一种度量方式，取值区间为[0,1]
        - 实现：
          ```
          x = np.random.normal(0,10,300)
          z = x *x
          pearsonr(x,z)
          # 输出-0.1
          from minepy import MINE
          m = MINE()
          m.compute_score(x, z)
          print(m.mic())
          # 输出1.0
          ```

    - Correlation coefficient scores（相关系数）

      - Pearson 相关系数

        - 原理：用两个变量的协方差除以各自的标准差，可以看成一种剔除了两个变量量纲影响、标准化后的特殊协方差
        - 适用场景：主要用于连续型特征的筛选，不适用于离散型特征的筛选
        - 优点：计算速度快，易于计算
        - 缺点：只对线性关系敏感
        - 实现：

          ```
          import numpy as np
          from scipy.stats import pearsonr

          np.random.seed(2019)
          size=1000
          x = np.random.normal(0, 1, size)
          # 计算两变量间的相关系数
          print("Lower noise {}".format(pearsonr(x, x + np.random.normal(0, 1, size))))
          print("Higher noise {}".format(pearsonr(x, x + np.random.normal(0, 10, size))))
          ```

      - 距离相关系数(Distance correlation)

        - 介绍：距离相关系数是为了克服 Pearson 相关系数的弱点而生的(Pearson 相关系数是 0，我们也不能断定这两个变量是独立的有可能是非线性相关)。

        $$
        \begin{cases}
        \hat{d}corr(u,v) = \frac{\hat{d}cov(u,v)}{\sqrt{\hat{d}cov(u,u)\hat{d}cov(v, v)}} \\\\
        \hat{d}cov^2(u,v) = \hat{S}_1 + \hat{S}_2 - 2\hat{S}_3 \\\\
        \hat{S_1} = \frac{1}{n^2}\sum_{i=1}^n\sum{j=1}^n{\mid\mid u_i - u_j \mid\mid}_{d_u}{\mid\mid v_i - v_j \mid\mid}_{d_v} \\\\
        \hat{S_2} = \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n{\mid\mid u_i-u_j \mid\mid}_{d_u} \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n{\mid\mid v_i-v_j \mid\mid}_{d_v} \\\\
        \hat{S_3} = \frac{1}{n^3}\sum_{i=1}^n\sum_{j=1}^n\sum_{l=1}^n{\mid\mid u_i - u_l \mid\mid}_{d_u}{\mid\mid v_j - v_l \mid\mid}_{d_v}
        \end{cases}
        $$

        - 代码实现

          ```
          from scipy.spatial.distance import pdist, squareform
          import numpy as np

          from numbapro import jit, float32

          def distcorr(X, Y):
              """ Compute the distance correlation function

              >>> a = [1,2,3,4,5]
              >>> b = np.array([1,2,9,4,4])
              >>> distcorr(a, b)
              0.762676242417
              """
              X = np.atleast_1d(X)
              Y = np.atleast_1d(Y)
              if np.prod(X.shape) == len(X):
                  X = X[:, None]
              if np.prod(Y.shape) == len(Y):
                  Y = Y[:, None]
              X = np.atleast_2d(X)
              Y = np.atleast_2d(Y)
              n = X.shape[0]
              if Y.shape[0] != X.shape[0]:
                  raise ValueError('Number of samples must match')
              a = squareform(pdist(X))
              b = squareform(pdist(Y))
              A = a - a.mean(axis=0)[None, :] - a.mean(axis=1)[:, None] + a.mean()
              B = b - b.mean(axis=0)[None, :] - b.mean(axis=1)[:, None] + b.mean()

              dcov2_xy = (A * B).sum()/float(n * n)
              dcov2_xx = (A * A).sum()/float(n * n)
              dcov2_yy = (B * B).sum()/float(n * n)
              dcor = np.sqrt(dcov2_xy)/np.sqrt(np.sqrt(dcov2_xx) * np.sqrt(dcov2_yy))
              return dcor
          ```

#### 封装法

遵循过滤方法的相同目标，但使用机器学习模型作为其评估标准（例如，向前/向后/双向/递归特征消除）。我们将一些特征输入机器学习模型，评估它们的性能，然后决定是否添加或删除特征以提高精度。

- 主要思想：将子集的选择看作是一个搜索寻优问题，生成不同的组合，对组合进行评价，再与其他的组合进行比较。将子集的选择看作是一个优化问题，可以用很多优化算法解决，特别是一些启发式的优化算法，比如 GA、PSO、DE 等。
- 优点：对特征选择的标准/规范是在学习算法的需求中展开的，能够考虑学习算法所属的任意学习偏差，从而确定最佳子特征，由于每次尝试针对特定子集时必须运行学习算法，所以能够关注到学习算法的学习偏差/归纳偏差，因此封装能够发挥巨大的作用。
- 缺点：运行速度远慢于过滤算法，实际应用用封装方法没有过滤方法流行。
- 主要方法：
  - 递归特征消除算法(Stability selection)
    - 稳定性选择常常是一种既能够有助于理解数据又能够挑出优质特征的这种选择。
    - 原理：
      - 稳定性选择是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM 或其他类似的方法。
      - 主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果。比如可以统计某个特征被认为是重要特征的频率
    - 代码实现：
      ```
      from sklearn.linear_model import RandomizedLasso
      from sklearn.datasets import load_boston
      boston = load_boston()
      #using the Boston housing data.
      #Data gets scaled automatically by sklearn's implementation
      X = boston["data"]
      Y = boston["target"]
      names = boston["feature_names"]
      rlasso = RandomizedLasso(alpha=0.025)
      rlasso.fit(X, Y)
      print("Features sorted by their score:")
      print(sorted(zip(map(lambda x: round(x, 4), rlasso.scores_), names),reverse=True))
      ```
  - 递归特征消除(Recursive feature elimination, RFE)
    - 原理:
      - 递归特征消除的主要思想是反复的构建模型（如 SVM 或者回归模型）然后选出最好的（或者最差的）的特征（可以根据系数来选），把选出来的特征放到一遍，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。
      - 这个过程中特征被消除的次序就是特征的排序。因此，这是一种寻找最优特征子集的贪心算法。
      - RFE 的稳定性很大程度上取决于在迭代的时候底层用哪种模型。
        - 假如 RFE 采用的普通的回归，没有经过正则化的回归是不稳定的，那么 RFE 就是不稳定的。
        - 假如 RFE 采用的是 Ridge，而用 Ridge 正则化的回归是稳定的，那么 RFE 就是稳定的。
    - 代码实现：
      ```
      from sklearn.feature_selection import RFE
      from sklearn.linear_model import LinearRegression
      boston = load_boston()
      X = boston["data"]
      Y = boston["target"]
      names = boston["feature_names"]
      #use linear regression as the model
      lr = LinearRegression()
      #rank all features, i.e continue the elimination until the last one
      rfe = RFE(lr, n_features_to_select=1)
      rfe.fit(X,Y)
      print("Features sorted by their rank:")
      print(sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), names)))
      ```

#### 嵌入法

与过滤方法一样，嵌入方法也使用机器学习模型。这两种方法的区别在于，嵌入的方法检查 ML 模型的不同训练迭代，然后根据每个特征对 ML 模型训练的贡献程度对每个特征的重要性进行排序。将特征选择嵌入到模型训练当中，其训练可能是相同的模型，但是特征选择完成后，还能给予特征选择完成的特征和模型训练出的超参数，再次训练优化。

- 主要思想：在模型既定的情况下学习出对提高模型准确性最好的特征。也就是在确定模型的过程中，挑选出那些对模型的训练有重要意义的特征。
- 优点：对特征进行搜索时围绕学习算法展开的，能够考虑学习算法所属的任意学习偏差。训练模型的次数小于 Wrapper 方法，比较节省时间。
- 缺点：运行速度慢。
- 主要方法：

  - 用带有 L1 正则化(Lasso regression)的项完成特征选择

    - 介绍：L1 正则化将系数 w 的 l1 范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成 0。因此 L1 正则化往往会使学到的模型很稀疏（系数 w 经常为 0），这个特性使得 L1 正则化成为一种很好的特征选择方法。
    - 代码：

      ```
      def pretty_print_linear(coefs, names = None, sort = False):
          if names == None:
              names = ["X%s" % x for x in range(len(coefs))]
          lst = zip(coefs, names)
          if sort:
              lst = sorted(lst,  key = lambda x:-np.abs(x[0]))
          return " + ".join("%s * %s" % (round(coef, 3), name)
                                      for coef, name in lst)

      from sklearn.linear_model import Lasso
      from sklearn.preprocessing import StandardScaler
      from sklearn.datasets import load_boston

      boston = load_boston()
      scaler = StandardScaler()
      X = scaler.fit_transform(boston["data"])
      Y = boston["target"]
      names = boston["feature_names"]

      lasso = Lasso(alpha=.3)
      lasso.fit(X, Y)

      print("Lasso model: {}".format(
          pretty_print_linear(lasso.coef_, names, sort = True)))
      ```

  - 结合 L2 惩罚项(Ridge regression)来优化

    - 介绍：L2 正则化对于特征选择来说一种稳定的模型，不像 L1 正则化那样，系数会因为细微的数据变化而波动。所以 L2 正则化和 L1 正则化提供的价值是不同的，L2 正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零。
    - 代码：

      ```
      from sklearn.linear_model import Ridge
      from sklearn.metrics import r2_score
      size = 100

      #We run the method 10 times with different random seeds
      for i in range(10):
          print("Random seed {}".format(i))
          np.random.seed(seed=i)
          X_seed = np.random.normal(0, 1, size)
          X1 = X_seed + np.random.normal(0, .1, size)
          X2 = X_seed + np.random.normal(0, .1, size)
          X3 = X_seed + np.random.normal(0, .1, size)
          Y = X1 + X2 + X3 + np.random.normal(0, 1, size)
          X = np.array([X1, X2, X3]).T

          lr = LinearRegression()
          lr.fit(X,Y)
          print("Linear model: {}".format(pretty_print_linear(lr.coef_)))

          ridge = Ridge(alpha=10)
          ridge.fit(X,Y)
          print("Ridge model: {}".format(pretty_print_linear(ridge.coef_)))
      ```

  - 随机森林选择

    - 平均不纯度减少法(mean decrease impurity)

      - 原理
        - 随机森林由多颗 CART 决策树构成，决策树中的每一个节点都是关于某个特征的条件，为的是将数据集按照不同的响应变量一分为二。
        - CART 利用不纯度可以确定节点（最优条件），对于分类问题，通常采用基尼不纯度，对于回归问题，通常采用的是方差或者最小二乘拟合。
        - 当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的标准。
        - 随机森林基于不纯度的排序结果非常鲜明，在得分最高的几个特征之后的特征，得分急剧的下降。
      - 代码实现

        ```
        from sklearn.datasets import load_boston
        from sklearn.ensemble import RandomForestRegressor
        import numpy as np

        #Load boston housing dataset as an example
        boston = load_boston()
        X = boston["data"]
        Y = boston["target"]
        names = boston["feature_names"]
        # 训练随机森林模型，并通过feature_importances_属性获取每个特征的重要性分数。
        rf = RandomForestRegressor()
        rf.fit(X, Y)
        print("Features sorted by their score:")
        print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), names),reverse=True))
        ```

    - 平均精确度减少法(mean decrease accuracy)
      - 原理介绍：
        - 通过直接度量每个特征对模型精确率的影响来进行特征选择。
        - 主要思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。
          - 对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大。
          - 对于重要的变量来说，打乱顺序就会降低模型的精确率。
      - 代码实现：
        ```
        from sklearn.cross_validation import ShuffleSplit
        from sklearn.metrics import r2_score
        from collections import defaultdict
        X = boston["data"]
        Y = boston["target"]
        rf = RandomForestRegressor()
        scores = defaultdict(list)
        #crossvalidate the scores on a number of different random splits of the data
        for train_idx, test_idx in ShuffleSplit(len(X), 100, .3):
            X_train, X_test = X[train_idx], X[test_idx]
            Y_train, Y_test = Y[train_idx], Y[test_idx]
            # 使用修改前的原始特征训练模型，其acc作为后续混洗特征值后的对比标准。r = rf.fit(X_train, Y_train)
            acc = r2_score(Y_test, rf.predict(X_test))
            # 遍历每一列特征
            for i in range(X.shape[1]):
                X_t = X_test.copy()
                # 对这一列特征进行混洗，交互了一列特征内部的值的顺序
                np.random.shuffle(X_t[:, i])
                shuff_acc = r2_score(Y_test, rf.predict(X_t))
                # 混洗某个特征值后，计算平均精确度减少程度。scores[names[i]].append((acc-shuff_acc)/acc)
        print("Features sorted by their score:")
        print(sorted([(round(np.mean(score), 4), feat) for feat, score in scores.items()], reverse=True))
        ```

#### 总结

1. 变量特征选择可以用于理解数据、数据的结构、特点，也可以用于排除不相关特征，但是它不能发现冗余特征。
2. 正则化的线性模型可用于特征理解和特征选择。相比起 L1 正则化，L2 正则化的表现更加稳定，L2 正则化对于数据的理解来说很合适。由于响应变量和特征之间往往是非线性关系，可以采用 basis expansion 的方式将特征转换到一个更加合适的空间当中，在此基础上再考虑运用简单的线性模型。
3. 随机森林是一种非常流行的特征选择方法，它易于使用。但它有两个主要问题：
   - 重要的特征有可能得分很低（关联特征问题）
   - 这种方法对特征变量类别多的特征越有利（偏向问题）
4. 特征选择在很多机器学习和数据挖掘场景中都是非常有用的。在使用的时候要弄清楚自己的目标是什么，然后找到哪种方法适用于自己的任务。
   - 当选择最优特征以提升模型性能的时候，可以采用交叉验证的方法来验证某种方法是否比其他方法要好。
   - 当用特征选择的方法来理解数据的时候要留心，特征选择模型的稳定性非常重要，稳定性差的模型很容易就会导致错误的结论。
   - 对数据进行二次采样然后在子集上运行特征选择算法能够有所帮助，如果在各个子集上的结果是一致的，那就可以说在这个数据集上得出来的结论是可信的，可以用这种特征选择模型的结果来理解数据。
5. 关于训练模型的特征筛选，实施流程：
   - 数据预处理后，先排除取值变化很小的特征。计算资源充足的话，尽可能的保留所有信息，可以把阈值设得比较高。
   - 如果数据量过大，计算资源不足（内存不足以使用所有数据进行训练、计算速度过慢），可以使用单特征选择法排除部分特征。这些被排除的特征并不一定完全被排除不再使用，在后续的特征构造时也可以作为原始特征使用。
   - 如果此时特征量依然非常大，或者是如果特征比较稀疏时，可以使用 PCA（主成分分析）和 LDA（线性判别）等方法进行特征降维。
   - 经过样本采样和特征预筛选后，训练样本可以用于训练模型。但是可能由于特征数量比较大而导致训练速度慢，或者想进一步筛选有效特征或排除无效特征（或噪音），我们可以使用正则化线性模型选择法、随机森林选择法或者顶层特征选择法进一步进行特征筛选。

# 如何提高数据敏感度

有人一眼看出你做的 PPT 里面的数据异常，随时能提出一个数据证明你的小结论有问题，然后以一个数据问题迅速推翻你整个报告的结论，这样的人逻辑性极强且对你的汇报有生杀大权，最重要的是他有极强的数据敏感度。

## 什么是数据敏感度

所谓的数据敏感度，其实就是在大脑内建立了数字和业务之间的联系，而优秀的数据敏感度，就是能够一眼看出数据的问题和背后可能的原因。
什么是一眼看出：

- 如果你是游戏行业的，我告诉你这款 MMORPG 的次留是 20%，你能知道我款产品在行业里处于什么样的水准，游戏前期可能存在什么样的问题等；
- 如果你是 O2O 行业的，我告诉你外卖订单量相比于昨天下跌了 10%，你能很快判断出问题的影响面和造成订单量下跌的可能原因；
- 如果你是电商行业的，我告诉你我这款产品的复购率是 40%，你能很快判断出我这款产品大概是什么类型的产品，在行业内是什么样的水准；

## 如何提升数据铭感度

### 熟悉业务

数据敏感度练成的基础是一定要对业务非常熟悉，无数次的推测及验证都是有用的宝贵经验。

1. 如何快速判断数据是高了低了还是错了：熟记关键指标的大数、观察趋势、紧盯异常值。
   - 电商行业：流量*转化率*客单价\*复购率
   - 游戏行业：Arppu、Arpu、次日留存、三日留存、七日留存、月留存、付费率
2. 知道所有指标是怎么来的，知道它们的意义以及相互的关系，进而判断数据异常的原因
   - 数据怎么来的？理解业务，分析溯源，同时也要判断数据来源的可靠性
   - 指标维度有哪些？不同业务有不同的关键业务指标，利用思维导图积累相关业务的指标体系，多总结多问为什么；指标体系经常用于数据细分找原因，知道数据构成才能更快地拆分数据，找到异常原因。
   - 数据如何说明业务？指标在业务中的应用，业务数据正常水平是怎么样的，受节假日或者活动营销的影响的数据又是怎么样的，要多对比，结合环比同比明白数据高低的意义等。
3. 拿到数据，能够根据分析目标很快理出分析框架，得出结论。
   - 先考虑全局指标：包括一定时间内新增用户量、总体付费率、总留存率、用户活跃度、各环节总转化率、搜索功能使用率、翻页率、崩溃率等。全局指标用于分析对全体用户产生影响的共性原因，绝大部分问题都会在全局指标上体现出来；
   - 再看分渠道指标：可以按不同用户属性（新老用户）、用户来源（下载渠道）、用户自然属性（地域、性别）网络环境（网络运营商、网络接入方式）等维度观察不同渠道数据是否存在异常。
   - 再考虑用户行为数据：重点观测用户在不同时间段、不同需求类型下的行为，从而定位到由于某一细分人群的定向变化产生的数据异常；
   - 考虑时间因素：外界环境的影响也可能对产品数据造成影响，因此观测环比和同比数据都很重要。如“月末效应”，“周一效应”，“寒暑假效应”
   - 其它产品线监控
   - 舆情监控：内部反馈渠道、竞争对手等
