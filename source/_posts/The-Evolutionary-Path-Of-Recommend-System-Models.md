---
title: The Evolutionary Path Of Recommend System Models
date: 2020-04-26 23:49:35
tags:
---

如果说存在这样一个应用，在构建这个应用的时候需要应用到大规模数据处理、数据挖掘、信息检索、预测理论、人工智能、认知科学、统计学、营销学、人机交互、AB 试验、微服务、高可用、高并发等相关技术(任何我见过的名词)，那么这个应用一定非推荐广告系统莫属。作为提供个性化服务能力的推荐广告系统已经渗透到我们生活的方方面面，比如当你打开某宝、某东、某信、某团、某条、某音、某乎、某 Hub 时，甚至你刚删掉的一条垃圾短信、拒掉的一个推销电话、忽略掉的一个垃圾推送。你看似不经意的一个操作带来对应交互呈现的背后，是几十个系统背后拼了老命计算协调后的结果，也是无数个心酸码农透支身体后留下的血与汗。

推荐系统这个名词想来大家都不陌生，大部分技术人员听到推荐系统这个名词脑海里的第一反应应该是“不就是协同过滤那一套算法模型么”，然而算法模型只是推荐系统中的冰山一角，虽然由其重要，但重要性却不及其它因素，如果一定要这些因素排个优先级的话， 那么我想它一定在 UI/UE、数据、领域知识之后。UI/UE 决定了用户对一个产品/系统的第一印象，只有当用户愿意留下来才有算法模型发挥的余地，特别是对看脸协会的高级会员来讲尤其重要，一个系统的“颜值”决定了我要不要停下来欣赏它的灵魂，所以 UI/UE 解决的是活下来的问题；其次得积累那么一些数据，巧妇难为无米之炊，有了米才能烹饪出美好的佳肴吸引更多的食客，所以数据解决的是让应用动起来的问题；接着是领域知识，算法模型朴素来讲是一个优化系统的工具，在优化系统之前得先理解优化的方向，定义量化评估优化的结果的方式，而不同的领域，优化方向和评估指标是不同的，比如富媒体新闻资讯类在意的是实时性，要快；电商类普通用户在意的是价格而不是兴趣，要准；流媒体类是想让用户停留时间更长，要久。领域知识能给算法模型快速指引一个方向，避免南辕北辙，做很多的无用功。

虽然算法模型的重要性不是那么高，但是现在普遍评估一个大厂推荐系统做的的好坏都是拿算法模型来说事，比如当系统恰好切中你的“要害”时，你的第一反应是，\*\*，为什么这么准，用的什么算法，而不是，诶，这个界面好好看，手感不错，这个系统好健壮好稳定，这是为什么呢？难道真的是好看的皮囊下真的要有一个有趣的灵魂么。其实主要原因还是现阶段推荐系统的其它通用型模块已经相对比较成熟，已经没有太多优化的空间了，各大有实力的厂商几乎都抄来抄去挖来挖去都一个样，即使系统的复杂度高又怎样，现代互联网的技术体系下什么高复杂性的问题解决不了？但是针对大量不同的业务场景、不同的人群，以及随时可能发生的兴趣变化这种不确定因素往往不是那种靠逻辑构建的应用系统能够捕捉的，所以算法模型站在前辈们的肩膀上在解决这些问题的时候自然而然的就显得格外亮眼。所以我们今天要聊的主题也是和推荐算法模型相关的个性化推荐技术，而不是工程技术，虽然其很有研究价值。好了，又习惯性的将了一大堆废话，下面介入正题。

作为个性化时代互联网的核心应用技术，推荐、搜索和广告一直是工业界技术研发和创新的主战场。这些领域由于非线性复杂度高，容易收集到大量数据的特性，天然适合于数据驱动创新的方法。在 2015 年左右，随深度学习的技术浪潮席卷而来，引爆整个领域的全面技术变革。深度学习除了在计算机视觉、语音、自然语言处理领域混的风声水起以外，其实很早的就已经被引入并应用到推荐、搜索、广告领域。只不过早期学术界研究、发表的大量复杂算法模型往往只是停留在实验室阶段，很难被工业界大规模应用，一方面是因为模型的假设太过苛刻，和实际情况相差较远，效果难以保证，另一方面，工业界计算规模巨大，除了准确性以外，人们越来越关注实时性，工程挑战大。所以随着深度学习技术的一步步沉淀，我们可以清晰的看到，现在在推荐、搜索、广告这类技术领域，核心模型算法的创新已经逐渐演变成由工业界主导、以工业实践和领域应用驱动为主的模型，这些领先的算法模型往往来自头部公司的顶尖团队，不再由学术界专门做研究的机器学习实验室基于假想的问题或者理论发展而创造。由此，接下来准备分享一些自己在推荐、广告领域关于深度学习在工业界应用场景的心得体会，与诸君共勉。当然在走进深度学习之前，我们先来看看传统推荐系统模型是如何进化的。两年前写过一篇介绍推荐系统传统模型的[文章](https://www.jianshu.com/p/b9ba5c84000b)，在逐渐了解到深度学习在推荐广告领域的应用之后，回来再看这些传统经典的模型，会有别样风味，下面一一道来。

## 传统推荐模型进化之路

推荐系统模型的发展可谓是一日千里，从 2010 年前千篇一律的协同过滤、逻辑回归进化到因子分解机、梯度提升树，再到 2015 年后深度学习在各个领域的百花齐放，各种模型架构层出不穷。推荐系统的主流模型经历了从单一模型到组合模型，从经典框架到深度学习、强化学习的发展过程。不得不说，深度学习推荐模型已经成为搜索、推荐、广告领域的主流。在工业界，当我们在享受深度学习技术红利的时候，认真的回顾前深度学习时代的推荐模型是很有必要的，因为即使在深度学习空前流行的今天，协同过滤、逻辑回归、因子分解机等传统模型仍然凭借其高度可解释性、环境要求低、易训练、易部署等不可替代的优势，拥有大量适用的场景。我们在选择应用模型时，不应该有新旧、高低贵贱之分，了解每个模型的优缺点、核心思想，能灵活运行和改进不同的算法模型是每一位从事推荐领域算法工程师、开发工程师的标配。其次传统推荐模型是深度学习推荐模型的基础，比如在工业界很有影响力的基于因子分解机的神经网络(FNN)、深度因子分解机(DeepFM)、神经网络因子分解机(NFM)等深度学习模型与传统的 FM 模型有着千丝万缕的关系，同时在传统推荐模型中广泛使用的梯度下降等训练方式，更是沿用到了深度学习时代，所以打好传统推荐模型的基本功，能让我们在推荐、广告领域走的越来越远，越来越稳。

传统推荐系统模型的演进主要由下图所示，传统推荐系统模型的发展脉络主要包含这几个部分：
![CTR](https://raw.githubusercontent.com/stayrascal/images/master/rs/CTR.png)

- 协同过滤算法簇
- 逻辑回归模型簇
- 因子分解机模型簇
- 组合模型

## 经典的协同过滤推荐算法

经典的协同过滤算法曾经是推荐系统的首选模型，协同过滤算法从物品相似度和用户相似度角度出发，衍生出基于物品的协同过滤(ItemCF)和基于用户的协同过滤(UserCF)两种算法，这两种算法的核心在于如何度量用户或者物品的相似度，在实施落地的时候，通常需要先构建用户物品共现矩阵，然后根据余弦相似度(Cosine Similarity)、修正的余弦相似度(Adjust Cosine Similarity:主要使用用户平均分或者物品平均分来减少用户、物品的偏置，因为每人评分标准不同)、皮尔逊相关系数(Person Coefficient)来计算用户或者物品的相似性，然后再根据用户的历史行为进行相似用户、相似物品的计算，最后加权输出排序结果进行推荐。
![CF](https://raw.githubusercontent.com/stayrascal/images/master/rs/cf.png)

基于用户的协同过滤和基于物品的协同过滤各自有不同的优缺点，适用于不同的业务场景，比如在大多数互联网应用场景下，用户数往往大于物品数，如果通过计算用户相似度方式的话，存储用户相似矩阵的开销会特别大，而且很难跟上用户快速增长的速度，所以大厂在早期的时候很少使用 UserCF 算法，而是选用 ItemCF 算法实现其最初的推荐系统，因为 Item 相对 User 来说比较稳定一些，那 UserCF 那不就没什么用啦？其实不然，UserCF 是基于用户的相似度进行推荐的，所以其本身具备很强的社交特性，用户能快速知道与自己兴趣相似的人最近都在关注些什么，这样的特点使其非常适用于新闻推荐场景，因为新闻本身的兴趣点往往是分散的，新闻的及时性、热点性往往是更重要的，所以 UserCF 适用于发现热点，以及跟踪热点的趋势。而 ItemCF 更适用于兴趣变化较为稳定的应用。

无论是 ItemCF，还是 UserCF，其实使用他们的条件是比较苛刻的，一方面这类算法主要适用于评分类型的数据，这类数据的收集代价其实是非常高的，因为评分、点赞行为处于用户行为轨迹的最后一个环节上，用户往往还没有走到这步就流失掉了；其次用户和物品之间的交互行为其实是特别少的，比如一个用户最多可能只会和成千上百万个物品中的几十个物品有交集，所以当我们构建用户物品共现矩阵时，会出现大量的稀疏矩阵问题，导致计算的相似度不准确。为了解决稀疏共现矩阵问题，增强模型的泛化能力，从协同过滤衍生出矩阵分解模型(Matrix Factorization，MF)，并发展出矩阵分解的各分支版本。

由于矩阵分解还是基于用户物品共现矩阵，所以矩阵分解还是属于协同过滤算法，业界有另外一个名字 Model Based CF，而之前的 UserCF、ItemCF 被统一称为 Memory Based CF。其实当推荐系统走到了矩阵分解模型这个阶段，才渐渐有了一点机器学习的味道，之前 Memory Based CF 相关的算法只是从统计的角度计算相似度而已，并不能算是入了机器学习的门槛，但是凭借其解释性高、算法简单、速度快等特性至今仍然被广泛的应用在现代推荐系统的召回阶段，而 Model Based CF 相关的算法在成百上千亿级数据量和实时性的要求下在工业界很少被使用，但其解决问题的思想已经影响了后续的很多模型，依然值得我们回味。

Memory Based CF 相关的算法还有一个缺陷，就是对热门的物品具有很强的头部效应，容易和大量物品产生相似性，而尾部物品由于特征向量稀疏，很少和其它物品产生相似性，所以很难被推荐，泛化能力比较弱，矩阵分解加入了隐向量的概念(这一点和深度学习中的 Embedding 向量是完全类似的)在一定程度上弥补了 Memory Based CF 模型处理稀疏矩阵不足的问题。在矩阵分解算法框架下，用户物品共现矩阵被分解为 k 维的用户、物品隐向量，我们之后可以将用户、物品隐向量进行內积得到用户对物品的感兴趣程度，从而进行推荐。k 的大小决定了隐向量表达能力的强弱，k 取值越小，隐向量包含的信息越少，模型的泛化程度越高，反之，k 越大，隐向量的表达能力越强，但方法能力程度相对降低。
![MF](https://raw.githubusercontent.com/stayrascal/images/master/rs/mf.jpg)

进行矩阵分解主要方法有几种：

- 特征值分解(Eigen Decomposition)
- 基于梯度下降(Gradient Descent)的奇异值分解(Singular Value Decomposition
- 基于最小二乘反复迭代的 ALS(Alternating Least Square)

特征值分解方法要求必须是方阵，只适用于数学老师留的那种一解就是两三页的课后作业，显然不适用于分解用户物品矩阵。
原始的奇异值分解方法要求原始的共现矩阵是稠密的，需要对缺失值进行填充，很依赖经验值，而且计算复杂度高到了 $O(mn^2)$的级别，不适用于解决大规模稀疏矩阵的矩阵分解问题。因此后面提出的 Funk-SVD 以及后续算法主要把梯度下降法作为进行矩阵分解的主要方法，先随机初始化用户矩阵 q 和物品矩阵 p，然后将两矩阵相乘并与真实共现矩阵作差得到损失函数，然后分别对 q 和 p 求偏导，再更新用户矩阵和物品矩阵直到收敛为止。
$$min\sum_{(u,i) \epsilon K}(r_{ui} - q_i^Tp_u)^2 + \lambda(||q_i|| + ||p_u||)^2$$
ALS 和梯度下降的方式类似，不过每次迭代只对 q 或者 p 求偏导进行更新，比如先固定 q 得到误差后对 p 求偏导进行更新，在下一次迭代的时候再固定 p 对 q 求偏导进行更新，从而达到“交替”的效果(想想这个和 GAN 的训练方式类似，你品，你细品)，比较适合稀疏的场景，所以与 SVD 相比，ALS 能有效的解决过拟合问题，而且可拓展性也高于 SVD，同时速度比梯度下降的方式快，并行度高，ALS-WR 还能处理用户对物品的隐式反馈，这也是为什么很多分布式机器学习框架比如 Spark Mllib 关于协同过滤的算法只有 ALS 的原因之一。
$$min\sum_{(u,i) \epsilon K}(r_{ui} - u - b_u - b_i - q_i^Tp_u)^2 + \lambda(||q_i||^2 + ||p_u||^2 + b_u^2 + b_i^2)$$
上面介绍到的矩阵分解方法只用到了用户对物品的评分信息，由于用户评分标准的差异性以及物品本身不同的特点，模型存在很多偏置信息，后续基于矩阵分解的改进模型也主要是关注在如何减少这部分偏置信息上，比如引进用户的平均分、物品平均分、整体平均分甚至增加历史行为、考虑时间因素等，比如 SVD++等系列的算法。

但无论怎么改进，矩阵分解的方法受到了很多限制因素导致至今很少被使用，除了前面提到的用于分解的数据获取代价大之外，矩阵分解只能使用这部分局部的信息，很难加入用户、物品、上下文等相关特征，导致大量信息的丢失，同时在缺乏用户历史行为时，无法进行有效的推荐，也就是冷启动问题。为了解决这些问题，逻辑回归模型及其后续发展的因子分解机模型，凭借其天然融合不同特征的能力，逐渐在推荐系统领域得到更广泛的应用。

在步入逻辑回归算法簇之前，需要额外补充一点，基于相似度推荐的方法除了协同过滤算法簇以外，还在基于内容的推荐算法上特别盛行，基于内容的推荐算法逻辑和 Memory Based CF 的逻辑几乎一致，主要的区别在于使用的数据不同，Memory Based CF 只使用用户和物品的交互数据，当企业发展到一定阶段之后自然而然的就积累了这些数据，使用起来也特别方便。而基于内容的推荐使用的是用户或者物品的描述性数据，大部分情况下直接依赖于用户画像或者物品画像，虽然基于内容的推荐在数据获取上非常方便，但从采集到的的原始数据到可以直接使用的画像数据这个过程需要大量的数据处理工作，比如为了构建用户或者物品画像，除了一些基本属性之外，我们需要利用自然语言处理技术甚至计算机视觉处理相关技术从评论、图片、视频内容中抽取中关键信息，为用户或者物品打上标签。这部分数据在后续的大部分模型中都可以被用到，甚至起了关键性的作用，毕竟数据是机器学习模型的天花板，模型只是无限逼近这个天花板而已。所以工业界的同行们花费了大量的精力在用户、物品内容提取上，从而构建成千上百万的标签体系，构造千亿维的训练数据。抛开生成的画像数据来讲，基于内容的推荐和 Memory Based CF 一样，目前被广泛的引用在推荐系统的召回阶段。

## 融合多种特征的逻辑回归模型

前面提到相比协同过滤模型仅仅利用用户物品共现矩阵进行推荐，逻辑回归可以综合利用用户、物品、上下文等多种不同的特征，训练更为全面的模型。在深度学习模型流行之前，能够进行多特征融合的逻辑回归模型曾在很长一段时间里是推荐系统、计算广告界的主要选择之一，凭借其强大的数学含义支撑、高度可解释性、模型简单、易于并行化、训练开销小等优势，至今被广泛用在离线排序和在线服务上。

逻辑回归不同于协同过滤预测用户对物品的喜好程度，而是将推荐问题看成一个分类问题，通过预测正样本的概率值，从而对物品排序，再进行推荐，预测用户是否会对某个物品进行点击，所以将推荐问题转换为一个点击率预估问题。

逻辑回归常用的训练方法主要有梯度下降法、牛顿法、拟牛顿法等，其中梯度下降法用的最广泛。逻辑回归作为广义线性模型的一种，它假设应变量 y 服从伯努利分布。

$$
\begin{cases}
P(y=1|x;w) = f_w(x) \\
P(y=0|x,w) = 1 - f_w(x)
\end{cases}
$$

$$P(y|x,w) = (f_w(x))^y(1 - f_w(x))^{1-y}$$
从而由似然估计原理得到逻辑归回的目标函数，再通过求对数将一个求最大值的问题转换为求极小值的问题。
$$L(w) = \Pi_{i=1}^{m}P(y|x;w)$$
$$J(w) = -\frac{1}{m}L(w) = -\frac{1}{m}(\sum_{i=1}^{m}(y^ilogf_w(x^i) + (1-y^i)log(1-f_w(x^i)))$$
接着通过对每个参数求偏导，得到梯度方向，从而进行模型参数的更新。可以看到整个计算过程体现了最优化理论最朴素的思想。
$$\frac{\delta}{\delta w_j}J(w) = \frac{1}{m}\sum_{i=1}^{m}(f_w(x^i) - y^i)x_j^i$$
$$w_j =  w_j - \gamma\frac{1}{m}\sum_{i=1}^{m}(f_w(x^i) - y^i)x_j^i $$

模型训练后的参数分别代表了各特征对最终点击与否结果所占的权重，从而使模型具有极强的可解释性，算法工程师可以轻易地根据权重的不同，解释哪些特征更重要，在预测结果有偏差时准确定位出哪些因素影响了最后的结果，特别是在和市场运营同时合作时，能大大降低沟通成本。

逻辑回归作为一个基础模型，有简单、直观、易用的优良特性，但是其表达能力不强，也没办法进行特征交叉、特征筛选等一系列高级的操作，从而导致某些有效信息的损失，甚至有时不只是信息损失的问题，还可能出现辛普森悖论的现象。早期解决这个问题的方法，主要还是通过算法工程师先手工组合特征，再通过各种分析手段筛选出重要特征的方法(类似咱们 Inception 中常用到的先发散再收敛的方法)来增加逻辑回归的拟合能力，这种方法的效率非常低，而且非常依赖工程师的经验，通常无法找到最优的特征组合。

## 自动特征交叉的探索

因此后面提出的 PLOY2 模型企图通过两两特征“暴力”组合的方式进行特征的组合。

$$POLY2(w,x) =w_0 + \sum_{i=0}^{n}w_ix_i + \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}w_{ij}x_ix_j$$

想象很美好，但现实很骨感。计算广告领域的数据和计算机视觉、语音、自然语言处理有一个很大的区别在于数据的稀疏性，因为在处理互联网数据时，我们通常会先采用 one-hot 的编码方式处理类别型的数据，PLOY2 的特征组合方式无疑近一步的增加了数据稀疏性，从而导致大量的权重由于样本缺失无法进行有效的训练，比如数据的稀疏导致大量的$x$为 0，所以$x_ix_j$等于 0 的概率会跟大，导致没有足够的样本来支持$w_{ij}$的更新，无法收敛。其次权重参数的数量也由 n 变成了$n^2$，增加了训练复杂度，种种缺陷表明 POLY2 这种方法就只能出现在学术论文上，在工业界根本无法使用。

为了解决 POLY2 模型的缺陷问题，Rendle 提出了 FM 模型。既然 POLY2 的参数太多了，模型太复杂了，那就降低模型的复杂性，将$n^2$级别的权重参数数量降低到$nk$级别，极大的降低了训练开销；既然数据会遇到稀疏性的问题，那么就换种方法来取代权重，比如使用两个向量內积$<v_i,v_j>$的方式替代权重$w_{ij}$；同时参考矩阵分解中为每个用户、物品学习一个隐因子向量的方式，FM 模型为每个特征学习了一个隐权重向量，在特征交叉的时候，直接使用两个特征向量的內积作为交叉特征的权重。
$$FM(w,x)=w_0 + \sum_{i=0}^{n}w_ix_i + \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}<v_i,v_j>x_ix_j$$

相比于 POLY2，虽然 FM 虽然参数减少了，学习能力减弱了，但是泛化能力大大提高了，也解决了之前需要人工组合特征的困扰。而且在工程方面，FM 模型同样可以用梯度下降法进行学习，不失实时性和灵活性，即使是在现在，也有很多将深度神经网络和 FM 结合起来作为线上排序模型的应用(比如 DeepFM，XDeepFM 等)，由此可以看到 FM 模型带来的影响有多遥远，可以说从 2012 到 2014 年前后，FM 模型代替逻辑回归成为了最主流的模型。

既然 FM 模型的学习能力有限，那为什么不想办法提升一下呢，于是 2015 年出厂的 FFM 模型在 FM 的基础上引入了特征域感知(field-aware)概念，加强模型的表达能力。
$$FFM(w,x)=w_0 + \sum_{i=0}^{n}w_ix_i + \sum_{i=1}^{n}\sum_{j=i+1}^{n}<v_{i,f_j},v_{j,f_i}>x_ix_j$$

FFM 和 FM 的区别在于特征的隐向量不再是唯一的一个隐向量，而是一组隐向量。比如，当$x_i$特征与$x_j$特征进行特征交叉时，FM 模型会直接对两个特征的隐向量进行內积，而 FFM 模型会从特征$x_i$的隐向量组中挑选出与特征$x_j$的域$f_j$对应的隐向量$v_{i,f_j}$进行交叉，同理特征$x_j$也会用与特征$x_i$的域$f_i$对应的隐向量$v_{j,f_i}$进行交叉。
这里提到的域代表特征域，域内的特征一般是采用 one-hot 编码方式形成的 one-hot 特征向量，比如说性别，品牌等。相比于 FM，FFM 引入了特征域的概念，因为引入更多有价值的信息，所以模型的表达能力更强，但并不能说 FFM 一定就比 FM 好，FFM 在提高学习能力的同时也增加了计算复杂度，所以在实际工程应用中，需要在模型效果和工程投入之间进行权衡，比如也有人结合 FM 和 FFM 的特点提出了双向 FFM(Bilinear-FFM)，计算复杂度和参数量介于 FM 和 FFM 之间，而核心思想也比较简单，既然 FFM 相对于 FM 来讲参数量上升了一个等级，那就降低参数量嘛，怎么降低呢，共享参数就是一个很好的思路(从全连接网络到卷积神经网络不也是这个套路么)，即将$<v_{i,f_j},v_{j,f_i}>$替换为$v_iWv_j$的方式达到降低参数减少复杂度的目的。

理论上，FM 模型簇可以利用特征交叉的思想进行三阶、四阶甚至更高阶的特征交叉，但是受到组合爆炸问题的限制，实际工程中很难实现更高阶的特征交叉，现代计算机很难大面积的支撑这么高的计算复杂度，所以 FM 的思想存在特征交叉的瓶颈。貌似 FM 的思想就已经走到尽头了，有没有什么其它方法可以解决这个问题？比如我们可不可以用多个模型组合达到高阶特征组合筛选的效果呢？答案是肯定的，Facebook 提出的利用 GBDT 自动进行特征筛选和组合，生成离散特征向量，作为 LR 模型输入的思想可谓是打开了计算广告领域的另一扇窗，我们可以看到当今很多主流模型都采用了这种思想，比如既然 GBDT 可以和 LR 组合，那我也可以把 DNN 和 LR 组合啊，于是家喻户晓的 Deep & Wide 就产生了；既然前面提供 FM 的效果比 LR 好，那我也可以将 DNN 和 FM/FFM 组合啊，于是 DeepFM 系列的算法就诞生了，而且组合的方式还被大家玩出了新花样，可以并行组合、串行组合、交叉组合，甚至把可以将树模型里面的每一个节点替换为 DNN，可以看到模型组合的思想一旦被打开了缺口就一发不可收拾，当然模型组合这门手艺也不是什么新技术，都是在其它领域玩烂的手法。

![GBDT + LR](https://raw.githubusercontent.com/stayrascal/images/master/rs/GBDT%2BLR.jpg)
GBDT 是由多棵决策树组成的树林，和随机森林中每棵树相互独立不同，GBDT 中的后一棵树以前面树林的结果与真实结果的残差为拟合目标，多层节点的结构对特征进行了有效的自动组合，非常高效地解决了过去棘手的特征选择和特征组合的问题。在特征转换过程中，一个训练样本在输入 GBDT 的某一颗子树后，会根据节点的规则落在某一个叶子节点上，那么这个叶子节点的输出值为 1，其它叶子节点的输出值为 0，所有子树叶子节点的输出值作为 LR 的输入向量。每棵树的深度决定了特征交叉的阶数，比如决策树的深度为 4，那么就会经过 3 次分裂，最终叶子节点的输出向量就是三阶特征组合的结果，这个组合能力不是 FM 系的模型所具备的，但也不能说 GBDT+LR 一定比 FM 好，因为 GBDT 容易过拟合，而且 GBDT 的组合方式实际上丢失了大量特征的数值信息，所以在模型选择调试上，需要综合多种因素考虑。

值得一提的是 GBDT+LR 组合模型的提出，意味着特征工程可以完全交给独立的模型来完成，不必在特征工程上投入过多的精力，实现真正的端到端训练，这个思想也是现在 AutoML 特征工程的一个主流实现思想，同时组合模型可以分阶段更新，比如大部分公司在引用 GBDT+LR 时，可以以天级别更新 GBDT，实时更新 LR。

在步入深度学习模型之前，除了上面提到的模型以外，还有一个由阿里提出的影响力比较大的传统的大规模分段线性模型(Large Scale Piece-wise Linear Model)值得一提，LS-PLM 又称 MLR(Mixed Logistic Regression)，它在逻辑回归的基础上采用分治的思路，先对样本进行分片，然后再对样本分片中应用逻辑回归进行 CTR 预估，也就是在逻辑回归的基础上加上了聚类的思想，也算是一种广义的组合模型。这个模型的灵感来自对广告推荐领域样本特点的观察，比如在预测女性是否点击女装广告时，男性用户点击体育类广告的样本数据和女性购买女装的广告场景毫无相关性，甚至还会在模型训练过程中干扰相关特征的权重。所以为了让 CTR 模型对不同用户群体、不同使用场景有针对性，可以先对全量样本进行聚类，再用逻辑回归对每个分类进行预估。
$$f(x)= \sum_{i=1}^{m}\pi(x)\eta_i(x)= \sum_{i=1}^{m} \frac{e^{u_ix}}{\sum_{j=1}^{m}e^{u_jx}\frac{1}{1+e^{-w_ix}}}$$
上述公式中超参数(分片数)m 可以较好的平衡模型的推广和拟合能力，当 m=1 时，其实就是标准的逻辑回归，m 越大，模型的拟合能力越强。在真实实践中，阿里根据自己的数据给 m 的推荐经验值为 12。

LS-PLM 虽然在 2017 年才被公之于众，但实际上早在 2012 年的时候就已经成为阿里用于推荐的主流模型了，也就是在深度学习才开始逐渐爆发的时候。值得一提的是 LS-PLM 的结构与三层神经网络极其相似，已经有一点深度学习的味道了，LS-PLR 通过样本分片的能力，能挖掘出数据中蕴藏的非线性模式，与神经网络通过非线性激活函数，SVM 通过核函数将一个非线性问题通过转换空间的形式达到线性可分的效果相似，大家的目的一致，只是各自的手段不同而已。同时 LS-PLM 在建模时引入了 L1 范数，从而使最终训练出来的模型具有较高的稀疏性，使模型的部署更加轻量级，在模型服务过程中只需要使用权重非零特征，大大的提升了模型推理的效率。

LS-PLM 的影响力如此之大的另外一个因素在于如果我们从深度学习的思路回头去看 LS-PLM 模型，可以把它看成是一个加了注意力(Attention)机制的三层神经网络模型，样本特征向量作为输入层，中间层是由 m 个神经元组成的隐藏层，以及用于 CTR 预估的单一神经元组成的输出层，在隐藏层和输出层之间，神经元的权重是由分片函数算出的注意力得分来确定的，即样本属于哪个分片的概率。当然在具体的实现上还是和经典的深度学习模型有有所区别，但这种注意力机制在阿里后续提出的模型中几乎成了标配，比如 DIN、DIEN、ESMM 等。

最后我们简单总结一下传统推荐模型的演进史，虽然他们被冠以传统之名，但仍然在现阶段的推荐系统架构中发光发热，比如现在推荐系统的召回阶段需要采用一些简单快速的模型从海量的候选集中召回一部分候选集，对模型的计算性能要求比较高，精度要求低，非常适合这些这些传统的推荐模型。
|模型名称|基本原理|特点|局限性|
|-------|------|---|-----|
|协同过滤|根据用户的历史行为构造用户-物品贡献矩阵，再计算以及利用用户、物品相似度进行推荐|原理简单、直接，应用广泛|泛化能力差，处理稀疏矩阵能力差，头部效应明显，容易越推越窄|
|矩阵分解|将协同过滤中的共现矩阵分解为用户矩阵和物品矩阵，利用用户和物品隐向量的內积进行排序推荐|相比协同过滤泛化能力和对稀疏性矩阵的处理能力有所加强|只能使用用户历史数据，比较难以利用用户、物品的特征属性数据和一些上下文特征信息|
|逻辑回归|将推荐问题转化为类似 CTR 预估的二分类问题，将用户、物品、上下文信息输入逻辑回归模型后得到 CTR，根据 CTR 预估进行排序推荐|模型简单、直观、易用、解释能力强|不具备自动特征组合能力，需要人工构造特征，代价大，表达能力比较差|
|FM|在逻辑回归的基础上加入了二阶特征交叉部分，为每一维特征训练一个隐向量，通过特征间隐向量的呢川籍运行得到交叉特征的权重|相比逻辑回归具备了二阶特征交叉组合能力，表达能力变强|受组合爆照问题的限制，没法做到三阶甚至更高阶的特征组合|
|FFM|在 FM 的基础上，加入特征域的概念，每个特征在于不同域特征交叉组合时采用不同的隐向量|相比 FM，进一步加强了特征交叉的能力|模型训练的开销达到了$O(n^2)$级别，训练开销大|
|GBDT+LR|利用 GBDT 自动化的特征组合，将原始特征向量进行多阶组合后的离散特征向量输入逻辑回归得到最终的 CTR 分数，再进行排序推荐|特征工程模块化，是模型具备了更高阶特征组合的能力|GBDT 没法进行完全的并行训练，训练时间比较长|
|LS-PLM|先对样本分片，然后在每个分片上构建逻辑回归模型，最后将样本每个分片的概率和逻辑回归的得分进行加权平均，再根据加权平均数排序推荐|模型结构类似三层神经网络，具备较强的表达能力|相对于深度学习模型还比较简单，还有很大的提升空间|

我们再回头来看看，在深度学习正式应用到推荐模型之前，传统推荐模型的演进旅程。矩阵分解的技术在 2006 年成功的运用在推荐系统领域，其隐向量的思想与深度学习中的 Embedding 技术的思路一脉相承；2010 年提出的 FM 将特征交叉引入到推荐系统中，其特征交叉的核心思想被深度学习模型发扬光大；阿里在 2012 大规模使用的 LS-PLM 在结构上已经非常接近三层神经网络了；Facebook 在 2014 年提出的 GBDT 自动化特征工程，揭开了特征工程模型化的篇章，这些概念延续到深度学习中，直到 2016 年，随着 Deep & Wide、FNN、Deep Crossing 等一大批优秀的深度学习推荐模型的提出，深度学习模型席卷推荐广告领域，成为新一代推荐模型的核心，我们会在后续的文章中带来更多的探讨。


参考阅读：
- [深入FFM原理与实践](https://tech.meituan.com/2016/03/03/deep-understanding-of-ffm-principles-and-practices.html)
- [推荐系统架构设计和算法](https://zhuanlan.zhihu.com/p/81752025)
- [深度学习推荐系统](https://zhuanlan.zhihu.com/p/119248677)
- [Recommendation System](https://developers.google.com/machine-learning/recommendation)
- [Factorization Machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)
- [Field-aware Factorization Machines for CTR Prediction](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf)
