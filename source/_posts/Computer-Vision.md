---
title: Computer Vision
date: 2018-06-10 18:22:15
tags:
---

## 计算机视觉领域的几重境界
> https://zhuanlan.zhihu.com/p/31727402
* 图像识别(Image Classification)：CNN
    - 经典网络结构：
        * LeNet-5: conv1(6)->pool1->conv2(16)->pool2->fc3(120)->fc4(84)->fc5(10)->softmax
        * AlexNet: use ReLU, Dropout, Data Augmentation
        * VGG-16/VGG-19: Simple Filter and Deep, use batch normalization
        * GoogleLeNet: multiple branch, 1x1convolution, conv1(64)->pool1->conv2^2(64,192)->pool2->inc3(256,480)->pool3->inc4^5(512,512,512,528,832)->pool4->inc5^2(832,1024)->pool5->fc(1000)
        * Inception v3/v4: divide 3x3 into 1x3 and 3x1
        * ResNet: short connection
        * preResNet: move Relu to short connection
        * ResNeXt: reuse incetpion block
        * 随机深度 ResNet: 以一定概率随机将residual模块失活。失活的模块直接由短路分支输出，而不经过有参数的分支。
        * DenseNet: dense模块中任意两层之间均有短路连接
        * SENet: 通过额外的分支(gap-fc-fc-sigm)来得到每个通道的[0, 1]权重，自适应地校正原各通道激活值响应。以提升有用的通道响应并抑制对当前任务用处不大的通道响应。
* 目标定位(Object Localization)：
    - 多任务学习，网络带有两个输出分支。一个分支用于做图像分类，即全连接+softmax判断目标类别，和单纯图像分类区别在于这里还另外需要一个“背景”类。另一个分支用于判断目标位置，即完成回归任务输出四个数字标记包围盒位置(例如中心点横纵坐标和包围盒长宽)，该分支输出结果只有在分类分支判断不为“背景”时才使用。
* 目标检测(Object Detection)：
    - 基于候选区域的目标检测算法
        - 基本思路: 使用不同大小的窗口在图像上滑动，在每个区域，对窗口内的区域进行目标定位。即将每个窗口内的区域前馈网络，其分类分支用于判断该区域的类别，回归分支用于输出包围盒。基于滑动窗的目标检测动机是，尽管原图中可能包含多个目标，但滑动窗对应的图像局部区域内通常只会有一个目标(或没有)。因此，我们可以沿用目标定位的思路对窗口内区域逐个进行处理。但是，由于该方法要把图像所有区域都滑动一遍，而且滑动窗大小不一，这会带来很大的计算开销。
        - R-CNN: 先利用一些非深度学习的类别无关的无监督方法，在图像中找到一些可能包含目标的候选区域。之后，对每个候选区域前馈网络，进行目标定位，即两分支(分类+回归)输出。模型本身存在的问题也很多，如需要训练三个不同的模型（proposal, classification, regression）、重复计算过多导致的性能问题等。
        - 候选区域(region proposal)候选区域生成算法通常基于图像的颜色、纹理、面积、位置等合并相似的像素，最终可以得到一系列的候选矩阵区域。这些算法，如selective search或EdgeBoxes，通常只需要几秒的CPU时间。
        - Fast R-CNN: R-CNN的弊端是需要多次前馈网络，这使得R-CNN的运行效率不高，预测一张图像需要47秒。Fast R-CNN同样基于候选区域进行目标检测，但受SPPNet启发，在Fast R-CNN中，不同候选区域的卷积特征提取部分是共享的。也就是说，我们先将整副图像前馈网络，并提取conv5卷积特征。之后，基于候选区域生成算法的结果在卷积特征上进行采样，这一步称为兴趣区域汇合。最后，对每个候选区域，进行目标定位，即两分支(分类+回归)输出。
        - Faster R-CNN: Fast R-CNN测试时每张图像前馈网络只需0.2秒，但瓶颈在于提取候选区域需要2秒。Faster R-CNN不再使用现有的无监督候选区域生成算法，而利用候选区域网络(RPN)从conv5特征中产生候选区域，并且将候选区域网络集成到整个网络中端到端训练。Faster R-CNN的测试时间是0.2秒，接近实时。后来有研究发现，通过使用更少的候选区域，可以在性能损失不大的条件下进一步提速。
        - R-FCN: Faster R-CNN在RoI pooling之后，需要对每个候选区域单独进行两分支预测。R-FCN旨在使几乎所有的计算共享，以进一步加快速度。由于图像分类任务不关心目标具体在图像的位置，网络具有平移不变性。但目标检测中由于要回归出目标的位置，所以网络输出应当受目标平移的影响。为了缓和这两者的矛盾，R-FCN显式地给予深度卷积特征各通道以位置关系。在RoI汇合时，先将候选区域划分成3×3的网格，之后将不同网格对应于候选卷积特征的不同通道，最后每个网格分别进行平均汇合。R-FCN同样采用了两分支(分类+回归)输出。
    - 基于直接回归的目标检测算法
        - 基本思路: 基于候选区域的方法由于有两步操作，虽然检测性能比较好，但速度上离实时仍有一些差距。基于直接回归的方法不需要候选区域，直接输出分类/回归结果。这类方法由于图像只需前馈网络一次，速度通常更快，可以达到实时。
        - YOLO: 将图像划分成7×7的网格，其中图像中的真实目标被其划分到目标中心所在的网格及其最接近的锚盒。
        - SSD: 相比YOLO，SSD在卷积特征后加了若干卷积层以减小特征空间大小，并通过综合多层卷积层的检测结果以检测不同大小的目标。
        - FPN: 之前的方法都是取高层卷积特征。但由于高层特征会损失一些细节信息，FPN融合多层特征，以综合高层、低分辨率、强语义信息和低层、高分辨率、弱语义信息来增强网络对小目标的处理能力。
        - RetinaNet: RetinaNet通过改进经典的交叉熵损失以降低对已经分的很好的样例的损失值，提出了焦点(focal)损失函数，以使模型训练时更加关注到困难的样例上。
    - 目标检测常用技巧
        - 非最大抑制(non-max suppression, NMS)
        - 在线困难样例挖掘(online hard example mining, OHEM)
        - 在对数空间回归
* 语义分割(Semantic Segmentation)：U-Net
    - 分割思路
        - 基本思路: 逐像素进行图像分类。我们将整张图像输入网络，使输出的空间大小和输入一致，通道数等于类别数，分别代表了各空间位置属于各类别的概率，即可以逐像素地进行分类。
        - 全卷积网络+反卷积网络:为使得输出具有三维结构，全卷积网络中没有全连接层，只有卷积层和汇合层。但是随着卷积和汇合的进行，图像通道数越来越大，而空间大小越来越小。要想使输出和输入有相同的空间大小，全卷积网络需要使用反卷积和反汇合来增大空间大小。
        - 反卷积(deconvolution)/转置卷积(transpose convolution)
        - 反最大汇合(max-unpooling)
    - 常用技巧
        - 扩张卷积(dilated convolution)
        - 条件随机场(conditional random field, CRF)
        - 利用低层信息
* 实例分割(Instance Segmentation):
        - 基本思路:目标检测+语义分割。先用目标检测方法将图像中的不同实例框出，再用语义分割方法在不同包围盒内进行逐像素标记。
        - Mask R-CNN: 用FPN进行目标检测，并通过添加额外分支进行语义分割(额外分割分支和原检测分支不共享参数)，即Mask R-CNN有三个输出分支(分类、坐标回归、和分割)。此外，Mask R-CNN的其他改进有：(1). 改进了RoI汇合，通过双线性差值使候选区域和卷积特征的对齐不因量化而损失信息。(2). 在分割时，Mask R-CNN将判断类别和输出模板(mask)这两个任务解耦合，用sigmoid配合对率(logistic)损失函数对每个类别的模板单独处理，取得了比经典分割方法用softmax让所有类别一起竞争更好的效果。


* 图像聚类：Auto-encoder-decoder
* 图像生成：GAN
* 迁移学习：Style transfer



- 图像分类(图像识别)：分类任务旨在判断该图像所属类别。
- 定位是在图像分类的基础上，进一步判断图像中的目标具体在图像的什么位置，通常是以包围盒的(bounding box)形式。
- 目标检测：在目标定位中，通常只有一个或固定数目的目标，而目标检测更一般化，其图像中出现的目标种类和数目都不定。
- 语意分割：语义分割是目标检测更进阶的任务，目标检测只需要框出每个目标的包围盒，语义分割需要进一步判断图像中哪些像素属于哪个目标。
- 实例分割：但是，语义分割不区分属于相同类别的不同实例。例如，当图像中有多只猫时，语义分割会将两只猫整体的所有像素预测为“猫”这个类别。与此不同的是，实例分割需要区分出哪些像素属于第一只猫、哪些像素属于第二只猫。
- 目标跟踪：目标跟踪通常是用于视频数据，和目标检测有密切的联系，同时要利用帧之间的时序关系。


### 机器视觉
如何从图像中解析出可供计算机理解的信息，是机器视觉的中心问题
- 分类（Classification）: 将图像结构化为某一类别的信息，用事先确定好的类别(string)或实例ID来描述图片。这一任务是最简单、最基础的图像理解任务
- 检测（Detection）: 分类任务关心整体，给出的是整张图片的内容描述，而检测则关注特定的物体目标，要求同时获得这一目标的类别信息和位置信息。相比分类，检测给出的是对图片前景和背景的理解，我们需要从背景中分离出感兴趣的目标，并确定这一目标的描述（类别和位置）
- 分割（Segmentation）: 分割包括语义分割（semantic segmentation）和实例分割（instance segmentation），前者是对前背景分离的拓展，要求分离开具有不同语义的图像部分，而后者是检测任务的拓展，要求描述出目标的轮廓（相比检测框更为精细）。分割是对图像的像素级描述，它赋予每个像素类别（实例）意义，适用于理解要求较高的场景，如无人驾驶中对道路和非道路的分割。

### 提升模型检测模型性能的技巧
- Data augmentation 数据增强：数据增强是增加深度模型鲁棒性和泛化性能的常用手段，随机翻转、随机裁剪、添加噪声等也被引入到检测任务的训练中来，其信念是通过数据的一般性来迫使模型学习到诸如对称不变性、旋转不变性等更一般的表示。通常需要注意标注的相应变换，并且会大幅增加训练的时间。
- Multi-scale Training/Testing 多尺度训练/测试: 输入图片的尺寸对检测模型的性能影响相当明显，事实上，多尺度是提升精度最明显的技巧之一。在基础网络部分常常会生成比原图小数十倍的特征图，导致小物体的特征描述不容易被检测网络捕捉。通过输入更大、更多尺寸的图片进行训练，能够在一定程度上提高检测模型对物体大小的鲁棒性，仅在测试阶段引入多尺度，也可享受大尺寸和多尺寸带来的增益。
- Global Context 全局语境: 是把整张图片作为一个RoI，对其进行RoI Pooling并将得到的feature vector拼接于每个RoI的feature vector上，作为一种辅助信息传入之后的R-CNN子网络。
- Box Refinement/Voting 预测框微调/投票法: 微调法最初是在SS算法得到的Region Proposal基础上用检测头部进行多次迭代得到一系列box，在ResNet的工作中，作者将输入R-CNN子网络的Region Proposal和R-CNN子网络得到的预测框共同进行NMS后处理，最后，把跟NMS筛选所得预测框的IoU超过一定阈值的预测框进行按其分数加权的平均，得到最后的预测结果。投票法可以理解为以顶尖筛选出一流，再用一流的结果进行加权投票决策。
- OHEM 在线难例挖掘: 两阶段检测模型中，提出的RoI Proposal在输入R-CNN子网络前，我们有机会对正负样本（背景类和前景类）的比例进行调整。通常，背景类的RoI Proposal个数要远远多于前景类，Fast R-CNN的处理方式是随机对两种样本进行上采样和下采样，以使每一batch的正负样本比例保持在1:3，这一做法缓解了类别比例不均衡的问题。实际操作中，维护两个完全相同的R-CNN子网络，其中一个只进行前向传播来为RoI Proposal的选择提供指导，另一个则为正常的R-CNN，参与损失的计算并更新权重，并且将权重复制到前者以使两个分支权重同步。
- Soft NMS 软化非极大抑制：NMS(Non-Maximum Suppression，非极大抑制）是检测模型的标准后处理操作，用于去除重合度（IoU）较高的预测框，只保留预测分数最高的预测框作为检测输出。
- RoIAlign RoI对齐：针对的问题是RoI在进行Pooling时有不同程度的取整，这影响了实例分割中mask损失的计算。文章采用双线性插值的方法将RoI的表示精细化，并带来了较为明显的性能提升
https://zhuanlan.zhihu.com/p/34142321
