---
title: Deep Learning:Neural Networrk
date: 2017-03-01 21:07:15
tags: Deep Learning
description: Neural Networrk
---


###  常见问题
------------

## 权重初始化
- 权重初始化并不等价于权重随机初始化
- 为什么需要矩阵权值化
	* 当使用高斯分布随机初始化权重的时候，可能导致线性计算的结果远小于-1或者远大于1的数，通过激活函数后所得到的输出会非常接近0或者1，也就是隐藏层神经元处于饱和的状态。所以当出现这样的情况时，在权重中进行微小的调整仅仅会给隐藏层神经元的激活值带来极其微弱的改变。而这种微弱的改变也会影响网络中剩下的神经元，然后会带来相应的代价函数的改变。结果就是，这些权重在我们进行梯度下降算法时会学习得非常缓慢
	* 我们可以通过改变权重w的分布，使|z|尽量接近于0
- 如何初始化
	* 使用标准正态分布、截断正太分布初始化权重矩阵
- 权值化的作用
	* 打破梯度对称性
- 忌全零初始化
	* 如果网络中的每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而进行同样的参数更新。换句话说，如果权重被初始化为同样的值，神经元之间就失去了不对称性的源头。
- 使用1/sqrt(n)校准方差
	* `w = np.random.randn(n) / sqrt(n)`。其中n是输入数据的数量。这样就保证了网络中所有神经元起始时有近似同样的输出分布。实践经验证明，这样做可以提高收敛的速度。
	* 网络中神经元的方差应该是2.0/n。代码为`w = np.random.randn(n) * sqrt(2.0/n)`。这个形式是神经网络算法使用ReLU神经元时的当前最佳推荐。
- 稀疏初始化（Sparse initialization）
	* 另一个处理非标定方差的方法是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接（其权重数值由一个小的高斯分布生成）。一个比较典型的连接数目是10个。
- 偏置（biases）的初始化
	* 通常还是使用0来初始化偏置参数
- 批量归一化（Batch Normalization）
	* 让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。
	* 神经网络中使用批量归一化已经变得非常常见。在实践中，使用了批量归一化的网络对于不好的初始值有更强的鲁棒性。批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起。
- 参考链接
	* http://neuralnetworksanddeeplearning.com/
	* http://www.jianshu.com/p/03009cfdf733

## 数据预处理
- 均值减法（Mean subtraction）：对数据中每个独立特征减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点
- 归一化（Normalization）：将数据的所有维度都归一化，使其数值范围都近似相等
	* 先对数据做零中心化（zero-centered）处理，然后每个维度都除以其标准差，实现代码为X /= np.std(X, axis=0)
	* 对每个维度都做归一化，使得每个维度的最大和最小值是1和-1
	* PCA和白化（Whitening）：先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构
- 参考链接：
	* https://zhuanlan.zhihu.com/p/21560667

## 梯度下降
- 深度学习的优化算法，说白了就是梯度下降。每次的参数更新有两种方式。
- 第一种，遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度，更新梯度。这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢，不支持在线学习，这称为Batch gradient descent，批梯度下降。
- 另一种，每看一个数据就算一下损失函数，然后求梯度更新参数，这个称为随机梯度下降，stochastic gradient descent。这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。
- 为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。


## 为什么要使用激活函数
 - 非线性：当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。但是，如果激活函数是恒等激活函数的时候，就不满足这个性质了，而且如果MLP使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。
 - 可微性：当优化方法是基于梯度的时候，这个性质是必须的。
 - 单调性：当激活函数是单调的时候，单层网络能够保证是凸函数。
 - f(x)≈x： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。
 - 输出值的范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate.
 - 激活函数的选择：
 	* ReLU: Rectified Linear Unit
 	* Leaky-ReLU
 	* PReLU: Parameteric Rectified Linear Unit
 	* ELU: Exponential Linear Unit
 	* Maxout
 - 参考链接：
 	* http://blog.csdn.net/cyh_24/article/details/50593400

## 逻辑回归的正则化
- 通过惩罚过大的参数来防止过拟合: ```J(w)=>J(w)+λ||w||p```
- L1 会趋向于产生少量的特征，而其他的特征都是0，而 L2 会选择更多的特征，这些特征都会接近于0。
- L1范数：是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）
	* 它能实现 特征的自动选择。
	* 一般来说，大部分特征 xi和输出 yi 之间并没有多大关系。在最小化目标函数的时候考虑到这些额外的特征 xi，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会干扰了对正确 yi 的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。
- L2范数：它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减”(weight decay)。
	*  它的强大之处就是它能 解决过拟合 问题。我们让 L2 范数的规则项 ||w||2 最小，可以使得 w 的每个元素都很小，都接近于0，但与 L1 范数不同，它不会让它等于0，而是接近于0，这里还是有很大区别的。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。

## 深度神经网络压缩
- Prunes the network：只保留一些重要的连接
- Quantize the weights：通过权值量化来共享一些weights
- Huffman coding：通过霍夫曼编码进一步压缩
- link：http://blog.csdn.net/cyh_24/article/details/51708469

## AlexNet
- 卷积运算一个重要的特点就是:通过卷积运算，可以使原信号特征增强，并且降低噪音
- 非线性激活函数：ReLU
	* 从原始图像（256,256）中，随机的crop出一些图像（224,224）【平移变换，crop】
	* 水平翻转图像。【反射变换，flip】
	* 给图像增加一些随机的光照。【光照、彩色变换，color jittering】
- 防止过拟合的方法：Dropout，Data augmentation
- 大数据训练：百万级ImageNet图像数据
- 其他：GPU实现，LRN归一化层的使用
- 测试的时候，对RGB空间做PCA，然后对主成分做一个(0, 0.1)的高斯扰动
- link：http://m.blog.csdn.net/article/details?id=51440344

## 深度信念网络
 - 自联想神经网络(自编码神经网络):三层BP网络，只不过它的输出等于输入, 输出是对输入的一种重构.
 - 使用层叠波尔兹曼机组成深度神经网络的方法，在深度学习里被称作深度信念网络DBN,通过层叠自编码网络的深度网络在深度学习里另外一个属于叫栈式自编码网络.
 -  DBN 在训练模型的过程中主要分为两步:
 	* 预训练：分别单独无监督地训练每一层 RBM 网络,确保特征向量映射到不同特征空间时,都尽可能多地保留特征信息;
 	* 微调：在 DBN 的最后一层设置 BP 网络,接收 RBM 的输出特征向量作为它的输入特征向量,有监督地训练实体关系分类器.而且每一层 RBM 网络只能确保自身层内的 权值对该层特征向量映射达到最优,并不是对整个 DBN 的特征向量映射达到最优,所以反向传播网络还将错误信息自顶向下传播至每一层 RBM,微调整个 DBN 网络.RBM 网络训练模型的过程可以看作对一个深层 BP 网络权值参数的初始化,使DBN 克服了 BP 网络因随机初始化权值参数而容易陷入局部最优和训练时间长的缺点.
 - link：
 	* http://blog.csdn.net/yangyangliangliang/article/details/20644853

## 如何训练深度神经网络
- 训练数据:
	* 获取越大的数据库越好。DNN 对数据很饥渴，越多越好
	* 去除所有包含损坏数据的训练样本，比如短文字，高度扭曲的图像，假输出标签，包含许多虚值（null values）的属性。
	* Data Augmentation（数据扩张）——生成新样例。以图像为例，重新调节，增加噪声等等
- 选择恰当的激活函数
	* Sigmoid 函数不可避免地存在两个缺陷：
		- 尾部sigmoids的饱和，进一步导致梯度消失
		- 不以 0 为中心（输出在 0 到 1 之间）
	* 参考上面的`为什么要使用激活函数`
- 隐藏单元和隐层（Hidden Units and Layers）的数量
	* 保留超出最优数量的隐藏单元，一般是比较保险的做法。
		- 任何正则化方法（ regularization method）都会处理好超出的单元，至少在某种程度上是这样
		- 保留比最优数量更少的隐藏单元，会导致更高的模型欠拟合（underfitting）几率。
		- 当采用无监督预训练的表示时（unsupervised pre-trained representations，下文会做进一步解释），隐藏单元的最优数目一般会变得更大。因此，预训练的表示可能会包含许多不相关信息（对于特定任务）。
		- 通过增加隐藏单元的数目，模型会得到所需的灵活性，以在预训练表示中过滤出最合适的信息。
- 权重初始化
	* 永远用小的随机数字初始化权重，以打破不同单元间的对称性（symmetry）.
		- 当使用 Sigmoid 激励函数时，如果权重初始化为很大的数字，那么 sigmoid 会饱和（尾部区域），导致死神经元（dead neurons）。如果权重特别小，梯度也会很小。因此，最好是在中间区域选择权重，比如说那些围绕平均值均衡分布的数值。
		- 对于 tanh 激励  `r=sqrt(6/(fan_in+fan_out))`
		- 对于 sigmoid 激励 `r=4*(sqrt(6/fan_in+fan_out))` 。fan_in 是上一层的大小， 而 fan_out 是下一层的。
- 学习率
	* 如果学习率设置得太小，你的模型很可能需要 n 年来收敛。设置得太大，再加上不多的初始训练样本，你的损失可能会极高。一般来说，0.01 的学习率比较保险。
	* 相比固定学习率，在每个周期、或每几千个样例后逐渐降低学习率是另一个选择。
	* 基于误差函数的曲率来调整学习率
	* 优化方法的研究，导致了自适应学习率：
		- 老式动能方法（ Momentum Method ）
		- Adagrad、Adam：能替我们省去人工选择初始学习率的麻烦；给定合适的时间，模型会开始平滑地收敛
		- RMSProp
- 超参数调参：扔掉网格搜索，拥抱随机搜索
	* 取决于经验，可以人工对部分常见超参数调参，比如学习率、隐层数目。
	* 采用随机搜索（random search），或者随机采样代替网格搜索，来选择最优超参数
- 权重的维度保持为 2 的幂
	* 内存管理在字节（byte）级别上进行。所以，把参数保持在 64, 128, 512, 1024 等 2 的次方也许能帮助分割矩阵和权重，导致学习效率的提升。当用 GPU 运算，这变得更明显。
- Mini-Batch（小批量） 对比随机学习（Stochastic Learning）
	* 训练中加入的噪音使得模型更不容易过拟合。
- 打乱训练样本
	* 把训练样例的顺序随机化（在不同周期，或者 mini-batch），会导致更快的收敛。如果模型看到的很多样例不在同一种顺序下，运算速度会有小幅提升。
- 使用 Dropout 正则化
	* 0.5 的默认值是一个不错的选择，当然，这取决于具体任务。如果模型不太复杂，0.2 的 Dropout 值或许就够了。
	* 在测试阶段，Dropout 应该被关闭，权重要调整到相应大小。只要对一个模型进行 Dropout 正则化，多一点训练时间，误差一定会降低。
- 周期 / 训练迭代次数
	* 继续按照一个固定的样例数或者周期训练模型，比如两万个样例或者一个周期。在每批样例之后，比较测试误差（test error）和训练误差（train error），如果它们的差距在缩小，那么继续训练。
	* 在每批训练之后，保存模型的参数，所以训练好之后你可以从多个模型中做选择。
- 可视化
	* 保存或打印损失值、训练误差、测试误差等项目的日志。

## 防止过拟合方法
- L2正则化
	* 对于网络中的每个权重w，向目标函数中增加一个`0.5λw^2`，其中λ是正则化强度
	* L2正则化可以直观理解为它对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。
	* 在梯度下降和参数更新的时候，使用L2正则化意味着所有的权重都以w += -lambda * W向着0线性下降
- L1正则化
	* 向目标函数增加一个`λ|w|`
	* Elastic net regularizaton(L1和L2正则化也可以进行组合):`λ|w| + 0.5λw^2`
	* L1正则化会让权重向量在最优化的过程中变得稀疏（即非常接近0）
	* 使用L1正则化的神经元最后使用的是它们最重要的输入数据的稀疏子集，同时对于噪音输入则几乎是不变的了
	* 相较L1正则化，L2正则化中的权重向量大多是分散的小数字。在实践中，如果不是特别关注某些明确的特征选择，一般说来L2正则化都会比L1正则化效果好
- 最大范式约束（Max norm constraints）
	* 给每个神经元中权重向量的量级设定上限，并使用投影梯度下降来确保这一约束
	* 在实践中，与之对应的是参数更新方式不变，然后要求神经元中的权重向量`w`必须满足`||w||2<c`这一条件，一般c值为3或者4。
	* 这种正则化还有一个良好的性质，即使在学习率设置过高的时候，网络中也不会出现数值“爆炸”
- 随机失活（Dropout）
	* 与L1正则化，L2正则化和最大范式约束等方法互为补充。在训练的时候，随机失活的实现方法是让神经元以超参数p的概率被激活或者被设置为0。
- 前向传播中的噪音
	* 它在前向传播的时候，一系列权重被随机设置为0。
- 偏置正则化
	* 对于偏置参数的正则化并不常见，因为它们在矩阵乘法中和输入数据并不产生互动，所以并不需要控制其在数据维度上的效果
	* 然而在实际应用中（使用了合理数据预处理的情况下），对偏置进行正则化也很少会导致算法性能变差
- 每层正则化
	* 对于不同的层进行不同强度的正则化很少见
- 实践
	* 通过交叉验证获得一个全局使用的L2正则化强度是比较常见的。在使用L2正则化的同时在所有层后面使用随机失活也很常见。p值一般默认设为0.5，也可能在验证集上调参

## 损失函数
- 损失函数的正则化损失部分，对模型复杂程度的某种惩罚
- 损失函数的第二个部分是数据损失，用于衡量分类算法的预测结果（即分类评分）和真实标签结果之间的一致性
- 在分类问题中，一个最常见的损失函数就是SVM（是Weston Watkins 公式）：

	<img src="http://chart.googleapis.com/chart?cht=tx&chl=\displaystyle L_i=\sum_{j\not=y_i}max(0,f_j-f_{y_i}+1)" style="border:none;">
- 第二个常用的损失函数是Softmax分类器，它使用交叉熵损失：
	
	<img src="http://chart.googleapis.com/chart?cht=tx&chl=\displaystyle L_i=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})" style="border:none;">
- 当类别数目巨大时，就需要使用分层Softmax（Hierarchical Softmax），分层softmax将标签分解成一个树。每个标签都表示成这个树上的一个路径，这个树的每个节点处都训练一个Softmax分类器来在左和右分枝之间做决策。树的结构对于算法的最终结果影响很大
- 属性（Attribute）分类：当每个样本含有多个正确的标签y时。
	* 在这种情况下，一个明智的方法是为每个属性创建一个独立的二分类的分类器。
	
		<img src="http://chart.googleapis.com/chart?cht=tx&chl=\displaystyle L_i=\sum_jmax(0,1-y_{ij}f_j)" style="border:none;">
		- 上式中，求和是对所有分类j，Yij的值为1或者-1，具体根据第i个样本是否被第j个属性打标签而定，当该类别被正确预测并展示的时候，分值向量Fj为正，其余情况为负。可以发现，当一个正样本的得分小于+1，或者一个负样本得分大于-1的时候，算法就会累计损失值。
	* 另一种方法是对每种属性训练一个独立的逻辑回归分类器
	
		<img src="http://chart.googleapis.com/chart?cht=tx&chl=\displaystyle P(y=1|x;w,b)=\frac{1}{1+e^{-(w^Tx+b)}}=\sigma(w^Tx+b)" style="border:none;">
- 回归问题
	* 对于这种问题，通常是计算预测值和真实值之间的损失。然后用L2平方范式或L1范式度量差异
	* L2损失比起较为稳定的Softmax损失来，其最优化过程要困难很多。
		* 它需要网络具备一个特别的性质，即对于每个输入（和增量）都要输出一个确切的正确值。
		* L2损失鲁棒性不好，因为异常值可以导致很大的梯度
		* 所以在面对一个回归问题时，先考虑将输出变成二值化是否真的不够用
		* 分类还有一个额外优点，就是能给出关于回归的输出的分布，而不是一个简单的毫无把握的输出值
- 结构化预测（structured prediction）
	* 结构化损失是指标签可以是任意的结构，例如图表、树或者其他复杂物体的情况。通常这种情况还会假设结构空间非常巨大，不容易进行遍历。
	* 结构化SVM背后的基本思想就是在正确的结构y_i和得分最高的非正确结构之间画出一个边界。
	* link: https://zhuanlan.zhihu.com/p/21560667

## 梯度检查
- 把解析梯度和数值计算梯度进行比较。
- 使用中心化公式计算数值梯度：
	
	<img src="http://chart.googleapis.com/chart?cht=tx&chl=\displaystyle \frac{df(x)}{dx}=\frac{f(x+h)-f(x-h)}{2h}" style="border:none;">
- 使用相对误差来比较：

	<img src="http://chart.googleapis.com/chart?cht=tx&chl=\displaystyle \frac{|f'_a-f'_n|}{max(|f'_a|,|f'_n|)}" style="border:none;">
	* 相对误差>1e-2：通常就意味着梯度可能出错。
	* 1e-2>相对误差>1e-4：要对这个值感到不舒服才行。
	* 1e-4>相对误差：这个值的相对误差对于有不可导点的目标函数是OK的。但如果目标函数中没有kink（使用tanh和softmax），那么相对误差值还是太高。
	* 1e-7或者更小：好结果，可以高兴一把了。
	* 相对误差的值要根据网络的深度来定义
- 使用双精度
	* 单精度会导致即使梯度实现正确，相对误差值也会很高（比如1e-2）
- 保持在浮点数的有效范围
- 目标函数的不可导点（kinks）
	* 解决不可导点问题的一个办法是使用更少的数据点
- 谨慎设置步长h
	* 有时候如果梯度检查无法进行，可以试试将h调到1e-4或者1e-6，然后突然梯度检查可能就恢复正常。
- 在操作的特性模式中梯度检查
	* 为了安全起见，最好让网络学习（“预热”）一小段时间，等到损失函数开始下降的之后再进行梯度检查
- 不要让正则化吞没数据
	* 正则化损失可能吞没掉数据损失，在这种情况下梯度主要来源于正则化部分
	* 推荐先关掉正则化对数据损失做单独检查，然后对正则化做单独检查
- 记得关闭随机失活（dropout）和数据扩张（augmentation）
- 检查少量的维度



## Optimizer
- SGD
	* 现在的SGD一般都指mini-batch gradient descent
	* 权重的更新完全依赖于当前batch的梯度
	* 缺点：
		* 选择合适的learning rate比较困难 - 对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更新快一些对于不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了
		* SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点
- Momentum
	* momentum是模拟物理里动量的概念，积累之前的动量来替代真正的梯度
	* M_t = U * M_t-1 + G_t
	* 特点：
		* 下降初期时，使用上一次参数更新，下降方向一致，乘上较大的动量因子能够进行很好的加速
		* 下降中后期时，在局部最小值来回震荡的时候，gradient->0，动量因子使得更新幅度增大，跳出陷阱
		* 在梯度改变方向的时候，动量因子能够减少更新 总而言之，momentum项能够在相关方向加速SGD，抑制振荡，从而加快收敛
- Nesterov
	* nesterov项在梯度更新时做一个校正，避免前进太快，同时提高灵敏度。
	* Nesterov基于momentum的改进就是让之前的动量直接影响当前的动量。
	* 所以，加上nesterov项后，梯度在大的跳跃后，进行计算对当前梯度进行校正。
	* momentum项和nesterov项都是为了使梯度更新更加灵活，对不同情况有针对性。
	* 缺点：
		* 人工设置一些学习率总还是有些生硬
- Adagrad
	* Adagrad其实是对学习率进行了一个约束
	* N_t = N_t-1 + Gt^2
	* 特点：
		* 前期g_t较小的时候， regularizer较大，能够放大梯度
		* 后期g_t较大的时候，regularizer较小，能够约束梯度
		* 适合处理稀疏梯度
	* 缺点：
		* 仍依赖于人工设置一个全局学习率
		* 步长设置过大的话，会使regularizer过于敏感，对梯度的调节太大
		* 中后期，分母上梯度平方的累加将会越来越大，使gradient->0，使得训练提前结束
- Adadelta
	* Adadelta是对Adagrad的扩展，最初方案依然是对学习率进行自适应约束，但是进行了计算上的简化。 Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。
	* N_t = v * N_t-1 + (1 - v) * Gt^2 
	* 经过近似牛顿迭代法处理之后，Adadelta已经不用依赖于全局学习率了:E|g^2|_t=p*E|g^2|_t-1+(1-p)*g_t^2
	* 特点：
		* 训练初中期，加速效果不错，很快
		* 训练后期，反复在局部最小值附近抖动
- RMSprop
	* RMSprop可以算作Adadelta的一个特例(p=0.5)
	* 特点：
		* 其实RMSprop依然依赖于全局学习率
		* RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间
		* 适合处理非平稳目标 - 对于RNN效果很好
- Adam
	* Adam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。
	* 特点：
		* 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点
		* 对内存需求较小
		* 为不同的参数计算不同的自适应学习率
		* 也适用于大多非凸优化 - 适用于大数据集和高维空间
- Adamax
	* Adamax是Adam的一种变体，此方法对学习率的上限提供了一个更简单的范围。
- Nadam
	* Nadam类似于带有Nesterov动量项的Adam。
	* Nadam对学习率有了更强的约束，同时对梯度的更新也有更直接的影响。
- 总结
	* 对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值
	* SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠
	* 如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。
	* Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。
	* 在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果
	* Link： https://zhuanlan.zhihu.com/p/22252270