---
title: Deep Learning
date: 2017-03-01 21:07:15
tags: Deep Learning
description: common question
---


###  常见问题
------------

## 权重初始化
- 权重初始化并不等价于权重随机初始化
- 为什么需要矩阵权值化
	* 当使用高斯分布随机初始化权重的时候，可能导致线性计算的结果远小于-1或者远大于1的数，通过激活函数后所得到的输出会非常接近0或者1，也就是隐藏层神经元处于饱和的状态。所以当出现这样的情况时，在权重中进行微小的调整仅仅会给隐藏层神经元的激活值带来极其微弱的改变。而这种微弱的改变也会影响网络中剩下的神经元，然后会带来相应的代价函数的改变。结果就是，这些权重在我们进行梯度下降算法时会学习得非常缓慢
	* 我们可以通过改变权重w的分布，使|z|尽量接近于0
- 如何初始化
	* 使用标准正态分布、截断正太分布初始化权重矩阵
- 权值化的作用
	* 打破梯度对称性
- 参考链接
	* http://neuralnetworksanddeeplearning.com/
	* http://www.jianshu.com/p/03009cfdf733

## 数据预处理
- 对每个维度都做归一化，使得每个维度的最大和最小值是1和-1
- 均值减法（Mean subtraction）：对数据中每个独立特征减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点
- 归一化（Normalization）：将数据的所有维度都归一化，使其数值范围都近似相等
	* 先对数据做零中心化（zero-centered）处理，然后每个维度都除以其标准差，实现代码为X /= np.std(X, axis=0)
	* 对每个维度都做归一化，使得每个维度的最大和最小值是1和-1
	* PCA和白化（Whitening）：先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构

## 梯度下降
- 深度学习的优化算法，说白了就是梯度下降。每次的参数更新有两种方式。
- 第一种，遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度，更新梯度。这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢，不支持在线学习，这称为Batch gradient descent，批梯度下降。
- 另一种，每看一个数据就算一下损失函数，然后求梯度更新参数，这个称为随机梯度下降，stochastic gradient descent。这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。
- 为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。



