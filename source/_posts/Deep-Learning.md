---
title: Deep Learning
date: 2017-03-01 21:07:15
tags: Deep Learning
description: common question
---


###  常见问题
------------

## 权重初始化
- 权重初始化并不等价于权重随机初始化
- 为什么需要矩阵权值化
	* 当使用高斯分布随机初始化权重的时候，可能导致线性计算的结果远小于-1或者远大于1的数，通过激活函数后所得到的输出会非常接近0或者1，也就是隐藏层神经元处于饱和的状态。所以当出现这样的情况时，在权重中进行微小的调整仅仅会给隐藏层神经元的激活值带来极其微弱的改变。而这种微弱的改变也会影响网络中剩下的神经元，然后会带来相应的代价函数的改变。结果就是，这些权重在我们进行梯度下降算法时会学习得非常缓慢
	* 我们可以通过改变权重w的分布，使|z|尽量接近于0
- 如何初始化
	* 使用标准正态分布、截断正太分布初始化权重矩阵
- 权值化的作用
	* 打破梯度对称性
- 参考链接
	* http://neuralnetworksanddeeplearning.com/
	* http://www.jianshu.com/p/03009cfdf733

## 数据预处理
- 对每个维度都做归一化，使得每个维度的最大和最小值是1和-1
- 均值减法（Mean subtraction）：对数据中每个独立特征减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点
- 归一化（Normalization）：将数据的所有维度都归一化，使其数值范围都近似相等
	* 先对数据做零中心化（zero-centered）处理，然后每个维度都除以其标准差，实现代码为X /= np.std(X, axis=0)
	* 对每个维度都做归一化，使得每个维度的最大和最小值是1和-1
	* PCA和白化（Whitening）：先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构
- 参考链接：
	* https://zhuanlan.zhihu.com/p/21560667

## 梯度下降
- 深度学习的优化算法，说白了就是梯度下降。每次的参数更新有两种方式。
- 第一种，遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度，更新梯度。这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢，不支持在线学习，这称为Batch gradient descent，批梯度下降。
- 另一种，每看一个数据就算一下损失函数，然后求梯度更新参数，这个称为随机梯度下降，stochastic gradient descent。这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。
- 为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。


## 为什么要使用激活函数
 - 非线性：当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。但是，如果激活函数是恒等激活函数的时候，就不满足这个性质了，而且如果MLP使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。
 - 可微性：当优化方法是基于梯度的时候，这个性质是必须的。
 - 单调性：当激活函数是单调的时候，单层网络能够保证是凸函数。
 - f(x)≈x： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。
 - 输出值的范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate.
 - 激活函数的选择：
 	* ReLU: Rectified Linear Unit
 	* Leaky-ReLU
 	* PReLU: Parameteric Rectified Linear Unit
 	* ELU: Exponential Linear Unit
 	* Maxout
 - 参考链接：
 	* http://blog.csdn.net/cyh_24/article/details/50593400

## 逻辑回归的正则化
- 通过惩罚过大的参数来防止过拟合: ```J(w)=>J(w)+λ||w||p```
- L1 会趋向于产生少量的特征，而其他的特征都是0，而 L2 会选择更多的特征，这些特征都会接近于0。
- L1范数：是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）
	* 它能实现 特征的自动选择。
	* 一般来说，大部分特征 xi和输出 yi 之间并没有多大关系。在最小化目标函数的时候考虑到这些额外的特征 xi，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会干扰了对正确 yi 的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。
- L2范数：它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减”(weight decay)。
	*  它的强大之处就是它能 解决过拟合 问题。我们让 L2 范数的规则项 ||w||2 最小，可以使得 w 的每个元素都很小，都接近于0，但与 L1 范数不同，它不会让它等于0，而是接近于0，这里还是有很大区别的。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。

## 深度神经网络压缩
- Prunes the network：只保留一些重要的连接
- Quantize the weights：通过权值量化来共享一些weights
- Huffman coding：通过霍夫曼编码进一步压缩
- link：http://blog.csdn.net/cyh_24/article/details/51708469

## AlexNet
- 卷积运算一个重要的特点就是:通过卷积运算，可以使原信号特征增强，并且降低噪音
- 非线性激活函数：ReLU
	* 从原始图像（256,256）中，随机的crop出一些图像（224,224）【平移变换，crop】
	* 水平翻转图像。【反射变换，flip】
	* 给图像增加一些随机的光照。【光照、彩色变换，color jittering】
- 防止过拟合的方法：Dropout，Data augmentation
- 大数据训练：百万级ImageNet图像数据
- 其他：GPU实现，LRN归一化层的使用
- 测试的时候，对RGB空间做PCA，然后对主成分做一个(0, 0.1)的高斯扰动
- link：http://m.blog.csdn.net/article/details?id=51440344

## 深度信念网络
 - 自联想神经网络(自编码神经网络):三层BP网络，只不过它的输出等于输入, 输出是对输入的一种重构.
 - 使用层叠波尔兹曼机组成深度神经网络的方法，在深度学习里被称作深度信念网络DBN,通过层叠自编码网络的深度网络在深度学习里另外一个属于叫栈式自编码网络.
 -  DBN 在训练模型的过程中主要分为两步:
 	* 预训练：分别单独无监督地训练每一层 RBM 网络,确保特征向量映射到不同特征空间时,都尽可能多地保留特征信息;
 	* 微调：在 DBN 的最后一层设置 BP 网络,接收 RBM 的输出特征向量作为它的输入特征向量,有监督地训练实体关系分类器.而且每一层 RBM 网络只能确保自身层内的 权值对该层特征向量映射达到最优,并不是对整个 DBN 的特征向量映射达到最优,所以反向传播网络还将错误信息自顶向下传播至每一层 RBM,微调整个 DBN 网络.RBM 网络训练模型的过程可以看作对一个深层 BP 网络权值参数的初始化,使DBN 克服了 BP 网络因随机初始化权值参数而容易陷入局部最优和训练时间长的缺点.
 - link：
 	* http://blog.csdn.net/yangyangliangliang/article/details/20644853

## 如何训练深度神经网络
- 训练数据:
	* 获取越大的数据库越好。DNN 对数据很饥渴，越多越好
	* 去除所有包含损坏数据的训练样本，比如短文字，高度扭曲的图像，假输出标签，包含许多虚值（null values）的属性。
	* Data Augmentation（数据扩张）——生成新样例。以图像为例，重新调节，增加噪声等等
- 选择恰当的激活函数
	* Sigmoid 函数不可避免地存在两个缺陷：
		- 尾部sigmoids的饱和，进一步导致梯度消失
		- 不以 0 为中心（输出在 0 到 1 之间）
	* 参考上面的`为什么要使用激活函数`
- 隐藏单元和隐层（Hidden Units and Layers）的数量
	* 保留超出最优数量的隐藏单元，一般是比较保险的做法。
		- 任何正则化方法（ regularization method）都会处理好超出的单元，至少在某种程度上是这样
		- 保留比最优数量更少的隐藏单元，会导致更高的模型欠拟合（underfitting）几率。
		- 当采用无监督预训练的表示时（unsupervised pre-trained representations，下文会做进一步解释），隐藏单元的最优数目一般会变得更大。因此，预训练的表示可能会包含许多不相关信息（对于特定任务）。
		- 通过增加隐藏单元的数目，模型会得到所需的灵活性，以在预训练表示中过滤出最合适的信息。
- 权重初始化
	* 永远用小的随机数字初始化权重，以打破不同单元间的对称性（symmetry）.
		- 当使用 Sigmoid 激励函数时，如果权重初始化为很大的数字，那么 sigmoid 会饱和（尾部区域），导致死神经元（dead neurons）。如果权重特别小，梯度也会很小。因此，最好是在中间区域选择权重，比如说那些围绕平均值均衡分布的数值。
		- 对于 tanh 激励  `r=sqrt(6/(fan_in+fan_out))`
		- 对于 sigmoid 激励 `r=4*(sqrt(6/fan_in+fan_out))` 。fan_in 是上一层的大小， 而 fan_out 是下一层的。
- 学习率
	* 如果学习率设置得太小，你的模型很可能需要 n 年来收敛。设置得太大，再加上不多的初始训练样本，你的损失可能会极高。一般来说，0.01 的学习率比较保险。
	* 相比固定学习率，在每个周期、或每几千个样例后逐渐降低学习率是另一个选择。
	* 基于误差函数的曲率来调整学习率
	* 优化方法的研究，导致了自适应学习率：
		- 老式动能方法（ Momentum Method ）
		- Adagrad、Adam：能替我们省去人工选择初始学习率的麻烦；给定合适的时间，模型会开始平滑地收敛
		- RMSProp
- 超参数调参：扔掉网格搜索，拥抱随机搜索
	* 取决于经验，可以人工对部分常见超参数调参，比如学习率、隐层数目。
	* 采用随机搜索（random search），或者随机采样代替网格搜索，来选择最优超参数
- 权重的维度保持为 2 的幂
	* 内存管理在字节（byte）级别上进行。所以，把参数保持在 64, 128, 512, 1024 等 2 的次方也许能帮助分割矩阵和权重，导致学习效率的提升。当用 GPU 运算，这变得更明显。
- Mini-Batch（小批量） 对比随机学习（Stochastic Learning）
	* 训练中加入的噪音使得模型更不容易过拟合。
- 打乱训练样本
	* 把训练样例的顺序随机化（在不同周期，或者 mini-batch），会导致更快的收敛。如果模型看到的很多样例不在同一种顺序下，运算速度会有小幅提升。
- 使用 Dropout 正则化
	* 0.5 的默认值是一个不错的选择，当然，这取决于具体任务。如果模型不太复杂，0.2 的 Dropout 值或许就够了。
	* 在测试阶段，Dropout 应该被关闭，权重要调整到相应大小。只要对一个模型进行 Dropout 正则化，多一点训练时间，误差一定会降低。
- 周期 / 训练迭代次数
	* 继续按照一个固定的样例数或者周期训练模型，比如两万个样例或者一个周期。在每批样例之后，比较测试误差（test error）和训练误差（train error），如果它们的差距在缩小，那么继续训练。
	* 在每批训练之后，保存模型的参数，所以训练好之后你可以从多个模型中做选择。
- 可视化
	* 保存或打印损失值、训练误差、测试误差等项目的日志。

